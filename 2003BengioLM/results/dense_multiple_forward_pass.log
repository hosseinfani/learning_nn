egative loglikelihood (-log p(nobel)): 8.083447456359863 or 8.083447456359863 or 8.083447456359863
('condemn', 'eu', 'bans', 'press', 'tv', 'officials')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (79, 'officials') has the max prob in order to be the next word. The prob is: 0.00033119425643235445
815200 The nn predicts (tensor([217]), 'pope') the next word with max prob: tensor([0.0228])
The negative loglikelihood (-log p(officials)): 8.012805938720703 or 8.012805938720703 or 8.012805938720703
('putin', 'signs', 'law', 'banning', 'foreign', 'banks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (545, 'banks') has the max prob in order to be the next word. The prob is: 0.00022966320102568716
815300 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.1060])
The negative loglikelihood (-log p(banks)): 8.378896713256836 or 8.378896713256836 or 8.378896713256836
('police', 'detain', 'british', 'reporter', 'unaware', 'broadcasting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4004, 'broadcasting') has the max prob in order to be the next word. The prob is: 0.00014906619617249817
815400 The nn predicts (tensor([183]), 'accused') the next word with max prob: tensor([0.0097])
The negative loglikelihood (-log p(broadcasting)): 8.81112003326416 or 8.81112003326416 or 8.81112003326416
('part', 'year', 'many', 'observers', 'predicting', 'environment')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1437, 'environment') has the max prob in order to be the next word. The prob is: 0.0003981732879765332
815500 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0143])
The negative loglikelihood (-log p(environment)): 7.828623294830322 or 7.828623294830322 or 7.828623294830322
('predator', 'arabian', 'gulf', 'chased', 'away', 'fighters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (340, 'fighters') has the max prob in order to be the next word. The prob is: 0.00016738020349293947
815600 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0281])
The negative loglikelihood (-log p(fighters)): 8.695242881774902 or 8.695242881774902 or 8.695242881774902
('historic', 'un', 'statement', 'women', 'final', 'text')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2384, 'text') has the max prob in order to be the next word. The prob is: 5.699524263036437e-05
815700 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0468])
The negative loglikelihood (-log p(text)): 9.772542953491211 or 9.772542953491211 or 9.772542953491211
('wrestle', 'austerity', 'thousands', 'protest', 'orlando', 'bisegna')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44016, 'bisegna') has the max prob in order to be the next word. The prob is: 1.4989201417847653e-06
815800 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0316])
The negative loglikelihood (-log p(bisegna)): 13.410765647888184 or 13.410765647888184 or 13.410765647888184
('gay', 'marriage', 'bill', 'demonstrate', 'citing', 'security')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (57, 'security') has the max prob in order to be the next word. The prob is: 0.0037442322354763746
815900 The nn predicts (tensor([217]), 'pope') the next word with max prob: tensor([0.0175])
The negative loglikelihood (-log p(security)): 5.587538719177246 or 5.587538719177246 or 5.587538719177246
('machines', 'emptied', 'across', 'cyprus', 'savers', 'learn')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3042, 'learn') has the max prob in order to be the next word. The prob is: 7.020236807875335e-05
816000 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0110])
The negative loglikelihood (-log p(learn)): 9.564128875732422 or 9.564128875732422 or 9.564128875732422
('mexico', 'deadly', 'fireworks', 'explosion', 'tlaxcala', 'state')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16, 'state') has the max prob in order to be the next word. The prob is: 0.03428996354341507
816100 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.1161])
The negative loglikelihood (-log p(state)): 3.3729026317596436 or 3.3729026317596436 or 3.3729026317596436
('time', 'decade', 'governing', 'coalition', 'contain', 'groups')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (327, 'groups') has the max prob in order to be the next word. The prob is: 3.883427052642219e-05
816200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0301])
The negative loglikelihood (-log p(groups)): 10.156207084655762 or 10.156207084655762 or 10.156207084655762
('referendum', 'voting', 'closed', 'zimbabwe', 'constitutional', 'referendum')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (605, 'referendum') has the max prob in order to be the next word. The prob is: 0.008340049535036087
816300 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0450])
The negative loglikelihood (-log p(referendum)): 4.786685943603516 or 4.786685943603516 or 4.786685943603516
('comes', 'stopping', 'sex', 'offenders', 'going', 'cuba')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (570, 'cuba') has the max prob in order to be the next word. The prob is: 9.656195470597595e-05
816400 The nn predicts (tensor([201]), 'get') the next word with max prob: tensor([0.0372])
The negative loglikelihood (-log p(cuba)): 9.245326042175293 or 9.245326042175293 or 9.245326042175293
('outgoing', 'pakistani', 'government', 'completing', 'full', 'term')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1395, 'term') has the max prob in order to be the next word. The prob is: 0.005447573494166136
816500 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.0254])
The negative loglikelihood (-log p(term)): 5.212584972381592 or 5.212584972381592 or 5.212584972381592
('territorial', 'dispute', 'japan', 'taking', 'measures', 'prevent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (964, 'prevent') has the max prob in order to be the next word. The prob is: 0.004798233974725008
816600 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0203])
The negative loglikelihood (-log p(prevent)): 5.339507579803467 or 5.339507579803467 or 5.339507579803467
('country', 'surprise', 'putting', 'least', 'tax', 'bank')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (87, 'bank') has the max prob in order to be the next word. The prob is: 0.001535935327410698
816700 The nn predicts (tensor([64]), 'deal') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(bank)): 6.478615760803223 or 6.478615760803223 or 6.478615760803223
('times', 'magazine', 'cover', 'story', 'story', 'palestinian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (99, 'palestinian') has the max prob in order to be the next word. The prob is: 0.00047655057278461754
816800 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0231])
The negative loglikelihood (-log p(palestinian)): 7.648936748504639 or 7.648936748504639 or 7.648936748504639
('fierce', 'horsemen', 'secured', 'empire', 'frontier', 'cossacks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14975, 'cossacks') has the max prob in order to be the next word. The prob is: 1.3818269053444965e-06
816900 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0168])
The negative loglikelihood (-log p(cossacks)): 13.492104530334473 or 13.492104530334473 or 13.492104530334473
('launches', 'indigenous', 'jamaran', 'destroyer', 'caspian', 'sea')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (126, 'sea') has the max prob in order to be the next word. The prob is: 0.15224561095237732
817000 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.1522])
The negative loglikelihood (-log p(sea)): 1.8822602033615112 or 1.8822602033615112 or 1.8822602033615112
('orban', 'constitutional', 'changes', 'thousands', 'hungarians', 'taken')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (638, 'taken') has the max prob in order to be the next word. The prob is: 0.00044235828681848943
817100 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0374])
The negative loglikelihood (-log p(taken)): 7.723390579223633 or 7.723390579223633 or 7.723390579223633
('savings', 'despite', 'promise', 'deposits', 'would', 'safe')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1178, 'safe') has the max prob in order to be the next word. The prob is: 0.0005106424796395004
817200 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0213])
The negative loglikelihood (-log p(safe)): 7.579840660095215 or 7.579840660095215 or 7.579840660095215
('<s>', 'iran', 'launches', 'destroyer', 'caspian', 'sea')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (126, 'sea') has the max prob in order to be the next word. The prob is: 0.08088286966085434
817300 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0809])
The negative loglikelihood (-log p(sea)): 2.5147533416748047 or 2.5147533416748047 or 2.5147533416748047
('refusing', 'food', 'conditions', 'detention', 'sparked', 'concern')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1531, 'concern') has the max prob in order to be the next word. The prob is: 0.0010526798432692885
817400 The nn predicts (tensor([141]), 'violence') the next word with max prob: tensor([0.0406])
The negative loglikelihood (-log p(concern)): 6.85641622543335 or 6.85641622543335 or 6.85641622543335
('casino', 'security', 'systems', 'netted', 'scammers', 'poker')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10168, 'poker') has the max prob in order to be the next word. The prob is: 1.212748156831367e-05
817500 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0156])
The negative loglikelihood (-log p(poker)): 11.320036888122559 or 11.320036888122559 or 11.320036888122559
('programmes', 'shown', 'tv', 'whether', 'using', 'tv')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (374, 'tv') has the max prob in order to be the next word. The prob is: 0.0034007548820227385
817600 The nn predicts (tensor([18]), 'military') the next word with max prob: tensor([0.0076])
The negative loglikelihood (-log p(tv)): 5.683757781982422 or 5.683757781982422 or 5.683757781982422
('press', 'regulation', 'deal', 'newspaper', 'groups', 'including')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (237, 'including') has the max prob in order to be the next word. The prob is: 0.005386171862483025
817700 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0312])
The negative loglikelihood (-log p(including)): 5.2239203453063965 or 5.2239203453063965 or 5.2239203453063965
('warns', 'us', 'missile', 'defence', 'programme', 'north')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (21, 'north') has the max prob in order to be the next word. The prob is: 0.025266535580158234
817800 The nn predicts (tensor([23]), 'south') the next word with max prob: tensor([0.0370])
The negative loglikelihood (-log p(north)): 3.678274393081665 or 3.678274393081665 or 3.678274393081665
('krugman', 'cyprus', 'bailout', 'push', 'europeans', 'stage')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1518, 'stage') has the max prob in order to be the next word. The prob is: 0.00011298973549855873
817900 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0279])
The negative loglikelihood (-log p(stage)): 9.088213920593262 or 9.088213920593262 or 9.088213920593262
('quebec', 'inmates', 'climbed', 'rope', 'hovering', 'helicopter')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1147, 'helicopter') has the max prob in order to be the next word. The prob is: 0.002545711351558566
818000 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0124])
The negative loglikelihood (-log p(helicopter)): 5.9733452796936035 or 5.9733452796936035 or 5.9733452796936035
('gujarat', 'cover', 'narmada', 'canals', 'solar', 'panels')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6023, 'panels') has the max prob in order to be the next word. The prob is: 6.259213841985911e-05
818100 The nn predicts (tensor([347]), 'plant') the next word with max prob: tensor([0.0397])
The negative loglikelihood (-log p(panels)): 9.678871154785156 or 9.678871154785156 or 9.678871154785156
('<s>', 'man', 'banned', 'libraries', 'face', 'earth')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (647, 'earth') has the max prob in order to be the next word. The prob is: 0.0006199845229275525
818200 The nn predicts (tensor([420]), 'rape') the next word with max prob: tensor([0.0331])
The negative loglikelihood (-log p(earth)): 7.3858160972595215 or 7.3858160972595215 or 7.3858160972595215
('bbc', 'news', 'arguments', 'supporting', 'cyprus', 'tax')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (274, 'tax') has the max prob in order to be the next word. The prob is: 0.0057164086028933525
818300 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0411])
The negative loglikelihood (-log p(tax)): 5.164414405822754 or 5.164414405822754 or 5.164414405822754
('region', 'vows', 'guarantee', 'future', 'jewish', 'people')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14, 'people') has the max prob in order to be the next word. The prob is: 0.009216143749654293
818400 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0455])
The negative loglikelihood (-log p(people)): 4.686798572540283 or 4.686798572540283 or 4.686798572540283
('militants', 'attack', 'court', 'pakistani', 'taliban', 'monday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (295, 'monday') has the max prob in order to be the next word. The prob is: 0.0009676741901785135
818500 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0309])
The negative loglikelihood (-log p(monday)): 6.940615177154541 or 6.940615177154541 or 6.940615177154541
('spanish', 'banking', 'system', 'comes', 'deposits', 'outside')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (548, 'outside') has the max prob in order to be the next word. The prob is: 0.00010890715202549472
818600 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0239])
The negative loglikelihood (-log p(outside)): 9.125015258789062 or 9.125015258789062 or 9.125015258789062
('name', 'illuminati', 'wanted', 'rap', 'career', 'murder')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (269, 'murder') has the max prob in order to be the next word. The prob is: 0.0025950984563678503
818700 The nn predicts (tensor([394]), 'old') the next word with max prob: tensor([0.0077])
The negative loglikelihood (-log p(murder)): 5.95413064956665 or 5.95413064956665 or 5.95413064956665
('cooling', 'water', 'several', 'hours', 'engineers', 'try')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1039, 'try') has the max prob in order to be the next word. The prob is: 0.0008163792663253844
818800 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0308])
The negative loglikelihood (-log p(try)): 7.110631465911865 or 7.110631465911865 or 7.110631465911865
('cyprus', 'considers', 'zero', 'tax', 'smaller', 'bank')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (87, 'bank') has the max prob in order to be the next word. The prob is: 0.08475112169981003
818900 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0848])
The negative loglikelihood (-log p(bank)): 2.46803617477417 or 2.468036413192749 or 2.468036413192749
('oil', 'militia', 'attacked', 'security', 'forces', 'guarding')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8919, 'guarding') has the max prob in order to be the next word. The prob is: 6.762109933333704e-06
819000 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0350])
The negative loglikelihood (-log p(guarding)): 11.904175758361816 or 11.904175758361816 or 11.904175758361816
('<s>', '<s>', '<s>', '<s>', '<s>', 'policy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (368, 'policy') has the max prob in order to be the next word. The prob is: 9.815625526243821e-05
819100 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.1672])
The negative loglikelihood (-log p(policy)): 9.228949546813965 or 9.228949546813965 or 9.228949546813965
('<s>', 'seven', 'marines', 'dead', 'training', 'exercise')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2015, 'exercise') has the max prob in order to be the next word. The prob is: 0.0001611111656529829
819200 The nn predicts (tensor([122]), 'afghanistan') the next word with max prob: tensor([0.0439])
The negative loglikelihood (-log p(exercise)): 8.733415603637695 or 8.733415603637695 or 8.733415603637695
('belgium', 'place', 'turkish', 'children', 'belgian', 'families')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (800, 'families') has the max prob in order to be the next word. The prob is: 0.00013183940609451383
819300 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0444])
The negative loglikelihood (-log p(families)): 8.93392562866211 or 8.93392562866211 or 8.93392562866211
('alleged', 'syrian', 'chemical', 'attack', 'killed', 'including')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (237, 'including') has the max prob in order to be the next word. The prob is: 0.0005626488127745688
819400 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0322])
The negative loglikelihood (-log p(including)): 7.482854843139648 or 7.482854843139648 or 7.482854843139648
('agency', 'europol', 'organized', 'crime', 'gangs', 'active')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3162, 'active') has the max prob in order to be the next word. The prob is: 6.40804719296284e-05
819500 The nn predicts (tensor([420]), 'rape') the next word with max prob: tensor([0.0308])
The negative loglikelihood (-log p(active)): 9.655370712280273 or 9.655370712280273 or 9.655370712280273
('accused', 'raping', 'committed', 'suicide', 'police', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.025769663974642754
819600 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0666])
The negative loglikelihood (-log p(said)): 3.658557415008545 or 3.658557176589966 or 3.658557176589966
('eu', 'blackmailing', 'cyprus', 'government', 'says', 'athanasios')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (61861, 'athanasios') has the max prob in order to be the next word. The prob is: 4.630100249869429e-07
819700 The nn predicts (tensor([1098]), 'bailout') the next word with max prob: tensor([0.0433])
The negative loglikelihood (-log p(athanasios)): 14.585516929626465 or 14.585516929626465 or 14.585516929626465
('calypso', 'nash', 'sacked', 'oxford', 'students', 'harlem')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12301, 'harlem') has the max prob in order to be the next word. The prob is: 5.2493201110337395e-06
819800 The nn predicts (tensor([91]), 'protest') the next word with max prob: tensor([0.0150])
The negative loglikelihood (-log p(harlem)): 12.157411575317383 or 12.157411575317383 or 12.157411575317383
('time', 'operating', 'inside', 'syria', 'eve', 'major')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (245, 'major') has the max prob in order to be the next word. The prob is: 0.00102450221311301
819900 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0324])
The negative loglikelihood (-log p(major)): 6.883548259735107 or 6.883548259735107 or 6.883548259735107
('geopolitical', 'adventure', 'tourism', 'dylan', 'harris', 'runs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2277, 'runs') has the max prob in order to be the next word. The prob is: 0.00026141759008169174
820000 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0086])
The negative loglikelihood (-log p(runs)): 8.249391555786133 or 8.249391555786133 or 8.249391555786133
('runs', 'ad', 'campaign', 'saying', 'hipster', 'real')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (737, 'real') has the max prob in order to be the next word. The prob is: 0.00029742205515503883
820100 The nn predicts (tensor([403]), 'church') the next word with max prob: tensor([0.0046])
The negative loglikelihood (-log p(real)): 8.12035846710205 or 8.12035846710205 or 8.12035846710205
('workers', 'trapped', 'mine', 'shaft', 'bringing', 'surface')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3613, 'surface') has the max prob in order to be the next word. The prob is: 0.00015826680464670062
820200 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0222])
The negative loglikelihood (-log p(surface)): 8.751228332519531 or 8.751228332519531 or 8.751228332519531
('yousafzai', 'shot', 'head', 'taliban', 'campaigning', 'girls')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (648, 'girls') has the max prob in order to be the next word. The prob is: 0.0017135763773694634
820300 The nn predicts (tensor([28]), 'first') the next word with max prob: tensor([0.0105])
The negative loglikelihood (-log p(girls)): 6.3691725730896 or 6.3691725730896 or 6.3691725730896
('justify', 'cruel', 'prison', 'system', 'death', 'penalty')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1750, 'penalty') has the max prob in order to be the next word. The prob is: 0.0035174270160496235
820400 The nn predicts (tensor([420]), 'rape') the next word with max prob: tensor([0.0169])
The negative loglikelihood (-log p(penalty)): 5.650025367736816 or 5.650025367736816 or 5.650025367736816
('sell', 'chinese', 'bowl', 'goes', 'million', 'auction')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3356, 'auction') has the max prob in order to be the next word. The prob is: 0.0008362066000699997
820500 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.0341])
The negative loglikelihood (-log p(auction)): 7.086634635925293 or 7.086634635925293 or 7.086634635925293
('men', 'rape', 'rob', 'way', 'parts', 'oruzgan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35283, 'oruzgan') has the max prob in order to be the next word. The prob is: 2.395949650235707e-06
820600 The nn predicts (tensor([1295]), 'escape') the next word with max prob: tensor([0.0251])
The negative loglikelihood (-log p(oruzgan)): 12.941730499267578 or 12.941730499267578 or 12.941730499267578
('korea', 'suspected', 'mass', 'south', 'korea', 'computer')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1736, 'computer') has the max prob in order to be the next word. The prob is: 0.0002450596366543323
820700 The nn predicts (tensor([23]), 'south') the next word with max prob: tensor([0.0292])
The negative loglikelihood (-log p(computer)): 8.314008712768555 or 8.314008712768555 or 8.314008712768555
('ordered', 'suspension', 'rating', 'hotel', 'agra', 'mahal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12839, 'mahal') has the max prob in order to be the next word. The prob is: 3.7198606150923297e-06
820800 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0295])
The negative loglikelihood (-log p(mahal)): 12.501824378967285 or 12.501824378967285 or 12.501824378967285
('caught', 'selling', 'crack', 'mexican', 'resort', 'city')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (71, 'city') has the max prob in order to be the next word. The prob is: 0.01678231544792652
820900 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0168])
The negative loglikelihood (-log p(city)): 4.087429523468018 or 4.087429523468018 or 4.087429523468018
('talks', 'southern', 'rebels', 'sudan', 'offers', 'hold')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (479, 'hold') has the max prob in order to be the next word. The prob is: 0.0018757432699203491
821000 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0364])
The negative loglikelihood (-log p(hold)): 6.278750419616699 or 6.278750419616699 or 6.278750419616699
('china', 'take', 'harder', 'position', 'india', 'russia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6, 'russia') has the max prob in order to be the next word. The prob is: 0.0022359113208949566
821100 The nn predicts (tensor([188]), 'arabia') the next word with max prob: tensor([0.0088])
The negative loglikelihood (-log p(russia)): 6.103106498718262 or 6.103106498718262 or 6.103106498718262
('north', 'korea', 'undertaken', 'military', 'provocation', 'within')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (563, 'within') has the max prob in order to be the next word. The prob is: 0.0028714952059090137
821200 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.0933])
The negative loglikelihood (-log p(within)): 5.852922439575195 or 5.852922439575195 or 5.852922439575195
('julia', 'gillard', 'apologises', 'australian', 'mothers', 'forced')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (381, 'forced') has the max prob in order to be the next word. The prob is: 0.0023764856159687042
821300 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(forced)): 6.042132377624512 or 6.042132377624512 or 6.042132377624512
('grapes', 'india', 'exceeds', 'grapes', 'south', 'africa')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (134, 'africa') has the max prob in order to be the next word. The prob is: 0.1363958716392517
821400 The nn predicts (tensor([22]), 'korea') the next word with max prob: tensor([0.2376])
The negative loglikelihood (-log p(africa)): 1.9921938180923462 or 1.9921938180923462 or 1.9921938180923462
('director', 'imf', 'connection', 'investigation', 'handling', 'financial')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (533, 'financial') has the max prob in order to be the next word. The prob is: 0.002084095263853669
821500 The nn predicts (tensor([436]), 'scandal') the next word with max prob: tensor([0.0153])
The negative loglikelihood (-log p(financial)): 6.173420429229736 or 6.173420429229736 or 6.173420429229736
('dutch', 'authorities', 'prosecute', 'japanese', 'whalers', 'allegedly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (815, 'allegedly') has the max prob in order to be the next word. The prob is: 0.0016701160930097103
821600 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0233])
The negative loglikelihood (-log p(allegedly)): 6.394862174987793 or 6.394862174987793 or 6.394862174987793
('control', 'could', 'impose', 'operational', 'limitations', 'human')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (90, 'human') has the max prob in order to be the next word. The prob is: 0.0012936509447172284
821700 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0124])
The negative loglikelihood (-log p(human)): 6.650286674499512 or 6.650286674499512 or 6.650286674499512
('korea', 'despite', 'discovery', 'attack', 'apparent', 'origin')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4741, 'origin') has the max prob in order to be the next word. The prob is: 4.384189014672302e-05
821800 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0853])
The negative loglikelihood (-log p(origin)): 10.034920692443848 or 10.034920692443848 or 10.034920692443848
('giving', 'babies', 'second', 'world', 'war', 'early')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (803, 'early') has the max prob in order to be the next word. The prob is: 0.000433389563113451
821900 The nn predicts (tensor([331]), 'crimes') the next word with max prob: tensor([0.0330])
The negative loglikelihood (-log p(early)): 7.743873596191406 or 7.743873596191406 or 7.743873596191406
('palestinian', 'child', 'grow', 'state', 'lives', 'presence')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1708, 'presence') has the max prob in order to be the next word. The prob is: 5.755861639045179e-05
822000 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0247])
The negative loglikelihood (-log p(presence)): 9.762706756591797 or 9.762706756591797 or 9.762706756591797
('strip', 'east', 'jerusalem', 'middle', 'east', 'war')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17, 'war') has the max prob in order to be the next word. The prob is: 0.002613646676763892
822100 The nn predicts (tensor([556]), 'jerusalem') the next word with max prob: tensor([0.0443])
The negative loglikelihood (-log p(war)): 5.947009086608887 or 5.9470086097717285 or 5.9470086097717285
('political', 'project', 'feels', 'like', 'loveless', 'marriage')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (606, 'marriage') has the max prob in order to be the next word. The prob is: 0.0013888946268707514
822200 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0304])
The negative loglikelihood (-log p(marriage)): 6.579246997833252 or 6.579246997833252 or 6.579246997833252
('north', 'korea', 'suspected', 'human', 'rights', 'abuses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1848, 'abuses') has the max prob in order to be the next word. The prob is: 0.017843401059508324
822300 The nn predicts (tensor([62]), 'group') the next word with max prob: tensor([0.1218])
The negative loglikelihood (-log p(abuses)): 4.026121616363525 or 4.026121616363525 or 4.026121616363525
('terraced', 'fields', 'yuanyang', 'county', 'yunnan', 'province')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (842, 'province') has the max prob in order to be the next word. The prob is: 0.006705071311444044
822400 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0239])
The negative loglikelihood (-log p(province)): 5.0048909187316895 or 5.0048909187316895 or 5.0048909187316895
('un', 'threatens', 'attack', 'us', 'bases', 'pacific')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1022, 'pacific') has the max prob in order to be the next word. The prob is: 0.00025905456277541816
822500 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1995])
The negative loglikelihood (-log p(pacific)): 8.258471488952637 or 8.258471488952637 or 8.258471488952637
('way', 'abuse', 'setting', 'age', 'consensual', 'sex')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (220, 'sex') has the max prob in order to be the next word. The prob is: 0.02171018347144127
822600 The nn predicts (tensor([220]), 'sex') the next word with max prob: tensor([0.0217])
The negative loglikelihood (-log p(sex)): 3.8299739360809326 or 3.8299739360809326 or 3.8299739360809326
('tv', 'suicide', 'bombing', 'mosque', 'kills', 'including')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (237, 'including') has the max prob in order to be the next word. The prob is: 0.0008393310126848519
822700 The nn predicts (tensor([84]), 'least') the next word with max prob: tensor([0.0475])
The negative loglikelihood (-log p(including)): 7.082905292510986 or 7.082905292510986 or 7.082905292510986
('united', 'nations', 'high', 'commissioner', 'human', 'rights')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (67, 'rights') has the max prob in order to be the next word. The prob is: 0.7187363505363464
822800 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.7187])
The negative loglikelihood (-log p(rights)): 0.33026066422462463 or 0.33026066422462463 or 0.33026066422462463
('government', 'technocrats', 'identity', 'future', 'pm', 'still')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (216, 'still') has the max prob in order to be the next word. The prob is: 0.0007483038352802396
822900 The nn predicts (tensor([1874]), 'hugo') the next word with max prob: tensor([0.0087])
The negative loglikelihood (-log p(still)): 7.197701454162598 or 7.197701454162598 or 7.197701454162598
('scale', 'foiled', 'every', 'year', 'uk', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.0024419077672064304
823000 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0528])
The negative loglikelihood (-log p(police)): 6.014975547790527 or 6.014975547790527 or 6.014975547790527
('zimbabwean', 'police', 'defied', 'high', 'court', 'ruling')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (481, 'ruling') has the max prob in order to be the next word. The prob is: 0.0015712038148194551
823100 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0182])
The negative loglikelihood (-log p(ruling)): 6.455913066864014 or 6.455913066864014 or 6.455913066864014
('invasion', 'south', 'korea', 'hostages', 'new', 'video')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (100, 'video') has the max prob in order to be the next word. The prob is: 0.0021447523031383753
823200 The nn predicts (tensor([23]), 'south') the next word with max prob: tensor([0.0206])
The negative loglikelihood (-log p(video)): 6.144731044769287 or 6.144731044769287 or 6.144731044769287
('bank', 'workers', 'rally', 'outside', 'parliament', 'cash')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (991, 'cash') has the max prob in order to be the next word. The prob is: 0.00012902947491966188
823300 The nn predicts (tensor([175]), 'vote') the next word with max prob: tensor([0.0205])
The negative loglikelihood (-log p(cash)): 8.955470085144043 or 8.955470085144043 or 8.955470085144043
('agency', 'says', 'prime', 'minister', 'najib', 'mikati')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (31742, 'mikati') has the max prob in order to be the next word. The prob is: 3.132969823127496e-06
823400 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0113])
The negative loglikelihood (-log p(mikati)): 12.673529624938965 or 12.673529624938965 or 12.673529624938965
('raid', 'flotilla', 'two', 'feuding', 'allies', 'agreed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1180, 'agreed') has the max prob in order to be the next word. The prob is: 0.0004652318893931806
823500 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0331])
The negative loglikelihood (-log p(agreed)): 7.672974586486816 or 7.672974586486816 or 7.672974586486816
('anonymous', 'releases', 'thousands', 'alleged', 'records', 'israeli')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (36, 'israeli') has the max prob in order to be the next word. The prob is: 0.0006593517027795315
823600 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0127])
The negative loglikelihood (-log p(israeli)): 7.324253559112549 or 7.324253559112549 or 7.324253559112549
('citizens', 'band', 'together', 'arrest', 'former', 'british')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (63, 'british') has the max prob in order to be the next word. The prob is: 0.0021029252093285322
823700 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0663])
The negative loglikelihood (-log p(british)): 6.164425849914551 or 6.164425849914551 or 6.164425849914551
('african', 'literary', 'icon', 'dies', 'novelist', 'chinua')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (20712, 'chinua') has the max prob in order to be the next word. The prob is: 2.7778660296462476e-06
823800 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.3056])
The negative loglikelihood (-log p(chinua)): 12.793827056884766 or 12.793828010559082 or 12.793828010559082
('sri', 'lanka', 'united', 'nations', 'human', 'rights')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (67, 'rights') has the max prob in order to be the next word. The prob is: 0.9183549880981445
823900 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.9184])
The negative loglikelihood (-log p(rights)): 0.08517126739025116 or 0.08517126739025116 or 0.08517126739025116
('worst', 'sectarian', 'bloodshed', 'hit', 'southeast', 'asian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1298, 'asian') has the max prob in order to be the next word. The prob is: 0.0022788785863667727
824000 The nn predicts (tensor([134]), 'africa') the next word with max prob: tensor([0.0395])
The negative loglikelihood (-log p(asian)): 6.084071636199951 or 6.084071636199951 or 6.084071636199951
('philippines', 'leading', 'probe', 'stuck', 'uss', 'guardian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1408, 'guardian') has the max prob in order to be the next word. The prob is: 0.00011049054592149332
824100 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0197])
The negative loglikelihood (-log p(guardian)): 9.110580444335938 or 9.110580444335938 or 9.110580444335938
('edge', 'arctic', 'chinese', 'plan', 'baffles', 'iceland')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1106, 'iceland') has the max prob in order to be the next word. The prob is: 0.0005074421060271561
824200 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0358])
The negative loglikelihood (-log p(iceland)): 7.586127758026123 or 7.586127758026123 or 7.586127758026123
('partners', 'seal', 'bailout', 'deal', 'avert', 'financial')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (533, 'financial') has the max prob in order to be the next word. The prob is: 0.0036986880004405975
824300 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.1455])
The negative loglikelihood (-log p(financial)): 5.5997772216796875 or 5.5997772216796875 or 5.5997772216796875
('african', 'warlord', 'abou', 'zeid', 'killed', 'fighting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (249, 'fighting') has the max prob in order to be the next word. The prob is: 0.002461021766066551
824400 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(fighting)): 6.007178783416748 or 6.007178783416748 or 6.007178783416748
('proceeds', 'game', 'would', 'go', 'david', 'suzuki')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17342, 'suzuki') has the max prob in order to be the next word. The prob is: 2.8887975531688426e-06
824500 The nn predicts (tensor([451]), 'cameron') the next word with max prob: tensor([0.0168])
The negative loglikelihood (-log p(suzuki)): 12.754670143127441 or 12.754670143127441 or 12.754670143127441
('assault', 'told', 'bbc', 'shouted', 'help', 'hour')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2092, 'hour') has the max prob in order to be the next word. The prob is: 3.423553062020801e-05
824600 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0148])
The negative loglikelihood (-log p(hour)): 10.282246589660645 or 10.282246589660645 or 10.282246589660645
('palestinian', 'authority', 'receive', 'aid', 'us', 'unblocks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17466, 'unblocks') has the max prob in order to be the next word. The prob is: 6.782818786632561e-07
824700 The nn predicts (tensor([18]), 'military') the next word with max prob: tensor([0.0228])
The negative loglikelihood (-log p(unblocks)): 14.203702926635742 or 14.203702926635742 or 14.203702926635742
('products', 'adobe', 'official', 'told', 'australian', 'parliamentary')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1638, 'parliamentary') has the max prob in order to be the next word. The prob is: 5.74566402065102e-05
824800 The nn predicts (tensor([132]), 'media') the next word with max prob: tensor([0.0401])
The negative loglikelihood (-log p(parliamentary)): 9.764479637145996 or 9.764479637145996 or 9.764479637145996
('<s>', 'meteor', 'delaware', 'crashes', 'ocean', 'video')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (100, 'video') has the max prob in order to be the next word. The prob is: 0.002086800755932927
824900 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0167])
The negative loglikelihood (-log p(video)): 6.172122955322266 or 6.172122955322266 or 6.172122955322266
('clashes', 'rock', 'central', 'myanmar', 'houses', 'mosque')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (778, 'mosque') has the max prob in order to be the next word. The prob is: 0.003320312825962901
825000 The nn predicts (tensor([191]), 'capital') the next word with max prob: tensor([0.0342])
The negative loglikelihood (-log p(mosque)): 5.707696437835693 or 5.707696437835693 or 5.707696437835693
('china', 'xi', 'arrives', 'africa', 'focus', 'trade')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (227, 'trade') has the max prob in order to be the next word. The prob is: 0.012340577319264412
825100 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0195])
The negative loglikelihood (-log p(trade)): 4.394862651824951 or 4.394862651824951 or 4.394862651824951
('commission', 'tons', 'marine', 'animals', 'thrown', 'back')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (115, 'back') has the max prob in order to be the next word. The prob is: 0.015462529845535755
825200 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0212])
The negative loglikelihood (-log p(back)): 4.169335842132568 or 4.169335842132568 or 4.169335842132568
('earth', 'hour', 'thousands', 'scots', 'switched', 'lights')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3769, 'lights') has the max prob in order to be the next word. The prob is: 5.8075718698091805e-05
825300 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0169])
The negative loglikelihood (-log p(lights)): 9.753763198852539 or 9.753763198852539 or 9.753763198852539
('balances', 'euros', 'remain', 'frozen', 'pending', 'conversion')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7594, 'conversion') has the max prob in order to be the next word. The prob is: 9.703125215310138e-06
825400 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0205])
The negative loglikelihood (-log p(conversion)): 11.543062210083008 or 11.543062210083008 or 11.543062210083008
('led', 'dozens', 'deaths', 'meikhtila', 'repeated', 'nearby')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3627, 'nearby') has the max prob in order to be the next word. The prob is: 0.00023057081853039563
825500 The nn predicts (tensor([141]), 'violence') the next word with max prob: tensor([0.0503])
The negative loglikelihood (-log p(nearby)): 8.37495231628418 or 8.37495231628418 or 8.37495231628418
('romania', 'bulgaria', 'moving', 'britain', 'eu', 'restrictions')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2086, 'restrictions') has the max prob in order to be the next word. The prob is: 0.0001870412816060707
825600 The nn predicts (tensor([1098]), 'bailout') the next word with max prob: tensor([0.1683])
The negative loglikelihood (-log p(restrictions)): 8.58418083190918 or 8.58418083190918 or 8.58418083190918
('hardline', 'buddhists', 'targeting', 'sri', 'lanka', 'muslims')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (407, 'muslims') has the max prob in order to be the next word. The prob is: 0.0056499638594686985
825700 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0242])
The negative loglikelihood (-log p(muslims)): 5.176105976104736 or 5.176105976104736 or 5.176105976104736
('dead', 'ducks', 'found', 'floating', 'chinese', 'river')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1400, 'river') has the max prob in order to be the next word. The prob is: 0.010220961645245552
825800 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0844])
The negative loglikelihood (-log p(river)): 4.583314418792725 or 4.583314418792725 or 4.583314418792725
('may', 'seize', 'deposits', 'cover', 'gambling', 'losses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2574, 'losses') has the max prob in order to be the next word. The prob is: 0.0010987820569425821
825900 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(losses)): 6.8135528564453125 or 6.8135528564453125 or 6.8135528564453125
('lenin', 'used', 'justify', 'confiscation', 'capitalists', 'property')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1935, 'property') has the max prob in order to be the next word. The prob is: 0.0005357956979423761
826000 The nn predicts (tensor([1547]), 'cyprus') the next word with max prob: tensor([0.0098])
The negative loglikelihood (-log p(property)): 7.531757831573486 or 7.531757831573486 or 7.531757831573486
('balances', 'could', 'lose', 'per', 'cent', 'money')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (358, 'money') has the max prob in order to be the next word. The prob is: 0.007747794967144728
826100 The nn predicts (tensor([1547]), 'cyprus') the next word with max prob: tensor([0.0168])
The negative loglikelihood (-log p(money)): 4.860346794128418 or 4.860346794128418 or 4.860346794128418
('northern', 'ireland', 'terrible', 'snow', 'storms', 'killed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13, 'killed') has the max prob in order to be the next word. The prob is: 0.022346580401062965
826200 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0496])
The negative loglikelihood (-log p(killed)): 3.801082134246826 or 3.801081895828247 or 3.801081895828247
('<s>', '<s>', 'musharraf', 'returns', 'pakistan', 'exile')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4163, 'exile') has the max prob in order to be the next word. The prob is: 0.00018653702863957733
826300 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0150])
The negative loglikelihood (-log p(exile)): 8.586880683898926 or 8.586880683898926 or 8.586880683898926
('central', 'african', 'republic', 'rebel', 'chief', 'name')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (959, 'name') has the max prob in order to be the next word. The prob is: 5.600702570518479e-05
826400 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0721])
The negative loglikelihood (-log p(name)): 9.790033340454102 or 9.790033340454102 or 9.790033340454102
('local', 'governments', 'cable', 'news', 'channel', 'ytn')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (41339, 'ytn') has the max prob in order to be the next word. The prob is: 1.0360663509345613e-06
826500 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.2508])
The negative loglikelihood (-log p(ytn)): 13.78007984161377 or 13.78007984161377 or 13.78007984161377
('assets', 'freeze', 'visa', 'ban', 'monday', 'zimbabwean')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5340, 'zimbabwean') has the max prob in order to be the next word. The prob is: 8.356591933988966e-06
826600 The nn predicts (tensor([1098]), 'bailout') the next word with max prob: tensor([0.0208])
The negative loglikelihood (-log p(zimbabwean)): 11.692460060119629 or 11.692460060119629 or 11.692460060119629
('korea', 'says', 'military', 'enter', 'combat', 'ready')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (468, 'ready') has the max prob in order to be the next word. The prob is: 8.290209370898083e-05
826700 The nn predicts (tensor([92]), 'troops') the next word with max prob: tensor([0.0331])
The negative loglikelihood (-log p(ready)): 9.397850036621094 or 9.397850036621094 or 9.397850036621094
('scottish', 'govt', 'approves', 'windfarm', 'opposed', 'donald')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3427, 'donald') has the max prob in order to be the next word. The prob is: 1.1999696653219871e-05
826800 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0542])
The negative loglikelihood (-log p(donald)): 11.330629348754883 or 11.330629348754883 or 11.330629348754883
('offer', 'different', 'picture', 'war', 'shiite', 'muslims')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (407, 'muslims') has the max prob in order to be the next word. The prob is: 0.01625676639378071
826900 The nn predicts (tensor([62]), 'group') the next word with max prob: tensor([0.0353])
The negative loglikelihood (-log p(muslims)): 4.119246006011963 or 4.119246006011963 or 4.119246006011963
('african', 'counterpart', 'jacob', 'zuma', 'ahead', 'brics')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4623, 'brics') has the max prob in order to be the next word. The prob is: 4.351943061919883e-06
827000 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0504])
The negative loglikelihood (-log p(brics)): 12.344887733459473 or 12.344887733459473 or 12.344887733459473
('clip', 'using', 'friend', 'alok', 'repeatedly', 'assaulted')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3654, 'assaulted') has the max prob in order to be the next word. The prob is: 0.00021088284847792238
827100 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0135])
The negative loglikelihood (-log p(assaulted)): 8.464207649230957 or 8.464207649230957 or 8.464207649230957
('amazon', 'jungle', 'region', 'says', 'affected', 'years')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (33, 'years') has the max prob in order to be the next word. The prob is: 0.0013444428332149982
827200 The nn predicts (tensor([227]), 'trade') the next word with max prob: tensor([0.0164])
The negative loglikelihood (-log p(years)): 6.6117753982543945 or 6.6117753982543945 or 6.6117753982543945
('look', 'reveals', 'work', 'plagued', 'rampant', 'copying')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13439, 'copying') has the max prob in order to be the next word. The prob is: 8.97422887646826e-06
827300 The nn predicts (tensor([507]), 'crime') the next word with max prob: tensor([0.0113])
The negative loglikelihood (-log p(copying)): 11.621153831481934 or 11.621153831481934 or 11.621153831481934
('red', 'tape', 'requiring', 'high', 'speed', 'links')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1089, 'links') has the max prob in order to be the next word. The prob is: 7.586221909150481e-05
827400 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0058])
The negative loglikelihood (-log p(links)): 9.486591339111328 or 9.486591339111328 or 9.486591339111328
('ben', 'zygier', 'found', 'hanged', 'cell', 'case')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (177, 'case') has the max prob in order to be the next word. The prob is: 0.0013199382228776813
827500 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0413])
The negative loglikelihood (-log p(case)): 6.6301703453063965 or 6.6301703453063965 or 6.6301703453063965
('gives', 'syrian', 'president', 'chair', 'chair', 'opposition')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (138, 'opposition') has the max prob in order to be the next word. The prob is: 0.0013340634759515524
827600 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0298])
The negative loglikelihood (-log p(opposition)): 6.619525909423828 or 6.619525909423828 or 6.619525909423828
('anonymous', 'hacks', 'mossad', 'website', 'gains', 'access')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (600, 'access') has the max prob in order to be the next word. The prob is: 0.0004383752529975027
827700 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0103])
The negative loglikelihood (-log p(access)): 7.73243522644043 or 7.73243522644043 or 7.73243522644043
('camera', 'helicopter', 'france', 'begins', 'using', 'roving')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (21486, 'roving') has the max prob in order to be the next word. The prob is: 2.3531808892585104e-06
827800 The nn predicts (tensor([204]), 'force') the next word with max prob: tensor([0.0112])
The negative loglikelihood (-log p(roving)): 12.959742546081543 or 12.959742546081543 or 12.959742546081543
('million', 'malware', 'bank', 'scam', 'wired', 'uk')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (20, 'uk') has the max prob in order to be the next word. The prob is: 0.008228940889239311
827900 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0237])
The negative loglikelihood (-log p(uk)): 4.800097942352295 or 4.800097942352295 or 4.800097942352295
('taiwan', 'supreme', 'court', 'says', 'porn', 'covered')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3768, 'covered') has the max prob in order to be the next word. The prob is: 3.1557781767332926e-05
828000 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(covered)): 10.363690376281738 or 10.363690376281738 or 10.363690376281738
('<s>', 'amanda', 'knox', 'fighting', 'clear', 'name')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (959, 'name') has the max prob in order to be the next word. The prob is: 0.000656946562230587
828100 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0070])
The negative loglikelihood (-log p(name)): 7.327908039093018 or 7.327908039093018 or 7.327908039093018
('deal', 'designed', 'safeguard', 'future', 'global', 'financial')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (533, 'financial') has the max prob in order to be the next word. The prob is: 0.005775203462690115
828200 The nn predicts (tensor([227]), 'trade') the next word with max prob: tensor([0.0456])
The negative loglikelihood (-log p(financial)): 5.154181957244873 or 5.154181957244873 or 5.154181957244873
('agencies', 'press', 'territorial', 'claims', 'asian', 'waters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1125, 'waters') has the max prob in order to be the next word. The prob is: 0.000490431790240109
828300 The nn predicts (tensor([156]), 'east') the next word with max prob: tensor([0.0708])
The negative loglikelihood (-log p(waters)): 7.620224475860596 or 7.620224475860596 or 7.620224475860596
('medicine', 'rides', 'distribution', 'remote', 'african', 'villages')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1701, 'villages') has the max prob in order to be the next word. The prob is: 0.0014750589616596699
828400 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0545])
The negative loglikelihood (-log p(villages)): 6.519057273864746 or 6.519057273864746 or 6.519057273864746
('africa', 'found', 'traces', 'human', 'tissue', 'meat')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1617, 'meat') has the max prob in order to be the next word. The prob is: 0.001384427072480321
828500 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0109])
The negative loglikelihood (-log p(meat)): 6.5824689865112305 or 6.5824689865112305 or 6.5824689865112305
('uk', 'government', 'loses', 'deportation', 'appeal', 'abu')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1450, 'abu') has the max prob in order to be the next word. The prob is: 6.770954496460035e-05
828600 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0255])
The negative loglikelihood (-log p(abu)): 9.6002836227417 or 9.6002836227417 or 9.6002836227417
('berlin', 'reverts', 'old', 'timidity', 'military', 'missions')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3729, 'missions') has the max prob in order to be the next word. The prob is: 0.00010911219578702003
828700 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(missions)): 9.123133659362793 or 9.123133659362793 or 9.123133659362793
('google', 'wants', 'trademark', 'proposed', 'swedish', 'word')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2686, 'word') has the max prob in order to be the next word. The prob is: 0.00019811054517049342
828800 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0223])
The negative loglikelihood (-log p(word)): 8.52668571472168 or 8.52668571472168 or 8.52668571472168
('seeks', 'billion', 'additional', 'funding', 'member', 'states')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (182, 'states') has the max prob in order to be the next word. The prob is: 0.32809117436408997
828900 The nn predicts (tensor([182]), 'states') the next word with max prob: tensor([0.3281])
The negative loglikelihood (-log p(states)): 1.1144636869430542 or 1.1144636869430542 or 1.1144636869430542
('street', 'view', 'ghost', 'town', 'left', 'fukushima')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (416, 'fukushima') has the max prob in order to be the next word. The prob is: 0.00010722911247285083
829000 The nn predicts (tensor([120]), 'thousands') the next word with max prob: tensor([0.0174])
The negative loglikelihood (-log p(fukushima)): 9.140542984008789 or 9.140542984008789 or 9.140542984008789
('egypt', 'naval', 'forces', 'capture', 'scuba', 'divers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6145, 'divers') has the max prob in order to be the next word. The prob is: 2.2338850612868555e-05
829100 The nn predicts (tensor([448]), 'base') the next word with max prob: tensor([0.0143])
The negative loglikelihood (-log p(divers)): 10.709183692932129 or 10.709182739257812 or 10.709182739257812
('residents', 'chance', 'win', 'car', 'plate', 'month')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (375, 'month') has the max prob in order to be the next word. The prob is: 0.0006553477724082768
829200 The nn predicts (tensor([101]), 'time') the next word with max prob: tensor([0.0073])
The negative loglikelihood (-log p(month)): 7.3303446769714355 or 7.3303446769714355 or 7.3303446769714355
('proud', 'first', 'canadian', 'operate', 'canadarm', 'orbit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3827, 'orbit') has the max prob in order to be the next word. The prob is: 0.0001430816191714257
829300 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0090])
The negative loglikelihood (-log p(orbit)): 8.852095603942871 or 8.852095603942871 or 8.852095603942871
('fights', 'droughts', 'africa', 'elsewhere', 'would', 'make')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (215, 'make') has the max prob in order to be the next word. The prob is: 0.012079949490725994
829400 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.0235])
The negative loglikelihood (-log p(make)): 4.416208267211914 or 4.416208267211914 or 4.416208267211914
('oscar', 'pistorius', 'appealing', 'bail', 'conditions', 'today')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (356, 'today') has the max prob in order to be the next word. The prob is: 0.00038126044091768563
829500 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(today)): 7.87202787399292 or 7.87202787399292 or 7.87202787399292
('cultural', 'revolution', 'son', 'guilt', 'mother', 'sent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (724, 'sent') has the max prob in order to be the next word. The prob is: 0.0009588600369170308
829600 The nn predicts (tensor([425]), 'died') the next word with max prob: tensor([0.0289])
The negative loglikelihood (-log p(sent)): 6.949765682220459 or 6.949765205383301 or 6.949765205383301
('says', 'sent', 'stealth', 'bombers', 'south', 'korea')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22, 'korea') has the max prob in order to be the next word. The prob is: 0.5241310000419617
829700 The nn predicts (tensor([22]), 'korea') the next word with max prob: tensor([0.5241])
The negative loglikelihood (-log p(korea)): 0.646013617515564 or 0.646013617515564 or 0.646013617515564
('want', 'deal', 'differ', 'particulars', 'even', 'treaty')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1300, 'treaty') has the max prob in order to be the next word. The prob is: 0.000170214640093036
829800 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0102])
The negative loglikelihood (-log p(treaty)): 8.678450584411621 or 8.678450584411621 or 8.678450584411621
('egypt', 'divers', 'caught', 'cutting', 'internet', 'cable')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2898, 'cable') has the max prob in order to be the next word. The prob is: 0.0007404029602184892
829900 The nn predicts (tensor([198]), 'internet') the next word with max prob: tensor([0.0337])
The negative loglikelihood (-log p(cable)): 7.208315849304199 or 7.208315849304199 or 7.208315849304199
('laureate', 'daw', 'aung', 'san', 'suu', 'kyi')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2305, 'kyi') has the max prob in order to be the next word. The prob is: 0.011247929185628891
830000 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0546])
The negative loglikelihood (-log p(kyi)): 4.4875712394714355 or 4.4875712394714355 or 4.4875712394714355
('feet', 'young', 'muslim', 'woman', 'prisoner', 'unprecedented')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1643, 'unprecedented') has the max prob in order to be the next word. The prob is: 4.604834975907579e-05
830100 The nn predicts (tensor([183]), 'accused') the next word with max prob: tensor([0.0240])
The negative loglikelihood (-log p(unprecedented)): 9.985818862915039 or 9.985818862915039 or 9.985818862915039
('time', 'united', 'states', 'europe', 'arab', 'states')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (182, 'states') has the max prob in order to be the next word. The prob is: 0.03856884315609932
830200 The nn predicts (tensor([248]), 'nations') the next word with max prob: tensor([0.0916])
The negative loglikelihood (-log p(states)): 3.255310535430908 or 3.255310535430908 or 3.255310535430908
('hollande', 'says', 'respect', 'judges', 'sacrosanct', 'credibility')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5682, 'credibility') has the max prob in order to be the next word. The prob is: 3.183125227224082e-05
830300 The nn predicts (tensor([127]), 'peace') the next word with max prob: tensor([0.0113])
The negative loglikelihood (-log p(credibility)): 10.355061531066895 or 10.355061531066895 or 10.355061531066895
('capture', 'divers', 'trying', 'cut', 'undersea', 'internet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (198, 'internet') has the max prob in order to be the next word. The prob is: 0.006931506097316742
830400 The nn predicts (tensor([416]), 'fukushima') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(internet)): 4.971678256988525 or 4.971678256988525 or 4.971678256988525
('man', 'sold', 'fate', 'investors', 'share', 'ipo')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9610, 'ipo') has the max prob in order to be the next word. The prob is: 2.576196948211873e-06
830500 The nn predicts (tensor([198]), 'internet') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(ipo)): 12.869195938110352 or 12.869195938110352 or 12.869195938110352
('homeland', 'mountainous', 'northwest', 'pakistani', 'taliban', 'fighters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (340, 'fighters') has the max prob in order to be the next word. The prob is: 0.01258932612836361
830600 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0441])
The negative loglikelihood (-log p(fighters)): 4.374906063079834 or 4.374906063079834 or 4.374906063079834
('reported', 'worth', 'nearly', 'million', 'publish', 'book')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1539, 'book') has the max prob in order to be the next word. The prob is: 0.0005891977343708277
830700 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0156])
The negative loglikelihood (-log p(book)): 7.436748504638672 or 7.436748504638672 or 7.436748504638672
('seek', 'another', 'solution', 'president', 'palace', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.01945808343589306
830800 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0195])
The negative loglikelihood (-log p(said)): 3.939492702484131 or 3.939492702484131 or 3.939492702484131
('korean', 'source', 'says', 'sharp', 'increase', 'personnel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2470, 'personnel') has the max prob in order to be the next word. The prob is: 8.569261990487576e-05
830900 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0303])
The negative loglikelihood (-log p(personnel)): 9.364744186401367 or 9.364744186401367 or 9.364744186401367
('differences', 'settled', 'physical', 'means', 'us', 'flew')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4352, 'flew') has the max prob in order to be the next word. The prob is: 1.7634125470067374e-05
831000 The nn predicts (tensor([314]), 'arms') the next word with max prob: tensor([0.0384])
The negative loglikelihood (-log p(flew)): 10.945674896240234 or 10.945674896240234 or 10.945674896240234
('queensland', 'keep', 'away', 'sanctuary', 'endangered', 'greater')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1904, 'greater') has the max prob in order to be the next word. The prob is: 0.00014583520533051342
831100 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0090])
The negative loglikelihood (-log p(greater)): 8.833033561706543 or 8.833033561706543 or 8.833033561706543
('new', 'one', 'study', 'geology', 'glaciers', 'geomagnetism')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (62015, 'geomagnetism') has the max prob in order to be the next word. The prob is: 3.083991032326594e-06
831200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(geomagnetism)): 12.689286231994629 or 12.689286231994629 or 12.689286231994629
('meps', 'european', 'parliamentary', 'group', 'urges', 'un')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (34, 'un') has the max prob in order to be the next word. The prob is: 0.002071790397167206
831300 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0164])
The negative loglikelihood (-log p(un)): 6.179342269897461 or 6.179342269897461 or 6.179342269897461
('giant', 'hedge', 'fund', 'ensnared', 'government', 'vast')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2601, 'vast') has the max prob in order to be the next word. The prob is: 0.00012075406266376376
831400 The nn predicts (tensor([114]), 'chief') the next word with max prob: tensor([0.0099])
The negative loglikelihood (-log p(vast)): 9.021754264831543 or 9.021754264831543 or 9.021754264831543
('three', 'days', 'saved', 'world', 'financial', 'system')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (333, 'system') has the max prob in order to be the next word. The prob is: 0.008455955423414707
831500 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.1477])
The negative loglikelihood (-log p(system)): 4.772884368896484 or 4.772884368896484 or 4.772884368896484
('news', 'tibet', 'landslip', 'buries', 'miners', 'huts')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19034, 'huts') has the max prob in order to be the next word. The prob is: 2.260931751152384e-06
831600 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0142])
The negative loglikelihood (-log p(huts)): 12.999733924865723 or 12.999733924865723 or 12.999733924865723
('prostitution', 'happening', 'north', 'korea', 'able', 'publicly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3110, 'publicly') has the max prob in order to be the next word. The prob is: 7.403907511616126e-05
831700 The nn predicts (tensor([215]), 'make') the next word with max prob: tensor([0.0241])
The negative loglikelihood (-log p(publicly)): 9.510917663574219 or 9.510917663574219 or 9.510917663574219
('<s>', '<s>', 'pope', 'final', 'straw', 'traditionalists')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26906, 'traditionalists') has the max prob in order to be the next word. The prob is: 2.356460299779428e-06
831800 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0199])
The negative loglikelihood (-log p(traditionalists)): 12.95835018157959 or 12.95835018157959 or 12.95835018157959
('xvi', 'visited', 'cuba', 'message', 'reconciliation', 'change')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (162, 'change') has the max prob in order to be the next word. The prob is: 0.001520634046755731
831900 The nn predicts (tensor([303]), 'visit') the next word with max prob: tensor([0.0223])
The negative loglikelihood (-log p(change)): 6.488627910614014 or 6.488627910614014 or 6.488627910614014
('lanka', 'economy', 'grows', 'commercial', 'disputes', 'heat')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2043, 'heat') has the max prob in order to be the next word. The prob is: 0.00020789052359759808
832000 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0285])
The negative loglikelihood (-log p(heat)): 8.478499412536621 or 8.478499412536621 or 8.478499412536621
('gold', 'mining', 'area', 'mountainous', 'tibet', 'burying')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9632, 'burying') has the max prob in order to be the next word. The prob is: 8.76641661307076e-06
832100 The nn predicts (tensor([23]), 'south') the next word with max prob: tensor([0.0200])
The negative loglikelihood (-log p(burying)): 11.644582748413086 or 11.644582748413086 or 11.644582748413086
('zone', 'operates', 'jointly', 'south', 'korea', 'perceived')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7717, 'perceived') has the max prob in order to be the next word. The prob is: 2.3675891497987323e-05
832200 The nn predicts (tensor([23]), 'south') the next word with max prob: tensor([0.0719])
The negative loglikelihood (-log p(perceived)): 10.651053428649902 or 10.651053428649902 or 10.651053428649902
('said', 'cameron', 'plan', 'legalize', 'gay', 'unions')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2568, 'unions') has the max prob in order to be the next word. The prob is: 0.0001686507894191891
832300 The nn predicts (tensor([606]), 'marriage') the next word with max prob: tensor([0.1178])
The negative loglikelihood (-log p(unions)): 8.6876802444458 or 8.6876802444458 or 8.6876802444458
('facing', 'doctors', 'without', 'borders', 'organization', 'concedes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7380, 'concedes') has the max prob in order to be the next word. The prob is: 1.19404612632934e-05
832400 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0659])
The negative loglikelihood (-log p(concedes)): 11.335577964782715 or 11.335577964782715 or 11.335577964782715
('secret', 'crypts', 'hidden', 'ruins', 'coventry', 'bombed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2876, 'bombed') has the max prob in order to be the next word. The prob is: 4.8847949074115604e-05
832500 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0130])
The negative loglikelihood (-log p(bombed)): 9.926797866821289 or 9.926797866821289 or 9.926797866821289
('fiasco', 'orlando', 'bisegna', 'index', 'showing', 'intensity')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10329, 'intensity') has the max prob in order to be the next word. The prob is: 1.2859831258538179e-05
832600 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0169])
The negative loglikelihood (-log p(intensity)): 11.261402130126953 or 11.261402130126953 or 11.261402130126953
('officers', 'escape', 'serious', 'injury', 'bin', 'bomb')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (131, 'bomb') has the max prob in order to be the next word. The prob is: 0.00343792955391109
832700 The nn predicts (tensor([910]), 'laden') the next word with max prob: tensor([0.1639])
The negative loglikelihood (-log p(bomb)): 5.672885894775391 or 5.672885894775391 or 5.672885894775391
('<s>', 'dead', 'pigs', 'found', 'china', 'rivers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4267, 'rivers') has the max prob in order to be the next word. The prob is: 0.0004599484382197261
832800 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0359])
The negative loglikelihood (-log p(rivers)): 7.684396266937256 or 7.684396266937256 or 7.684396266937256
('<s>', 'shooting', 'reported', 'toronto', 'yorkdale', 'mall')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2120, 'mall') has the max prob in order to be the next word. The prob is: 9.928592771757394e-05
832900 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0246])
The negative loglikelihood (-log p(mall)): 9.217506408691406 or 9.217506408691406 or 9.217506408691406
('rescue', 'services', 'latvia', 'rescued', 'people', 'stranded')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2314, 'stranded') has the max prob in order to be the next word. The prob is: 0.00016696713282726705
833000 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0268])
The negative loglikelihood (-log p(stranded)): 8.697713851928711 or 8.697713851928711 or 8.697713851928711
('suspected', 'eta', 'leader', 'dies', 'paris', 'hospital')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (415, 'hospital') has the max prob in order to be the next word. The prob is: 0.004344942048192024
833100 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0967])
The negative loglikelihood (-log p(hospital)): 5.438742637634277 or 5.438742637634277 or 5.438742637634277
('senior', 'surviving', 'member', 'regime', 'capable', 'defending')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4343, 'defending') has the max prob in order to be the next word. The prob is: 3.2491130696143955e-05
833200 The nn predicts (tensor([23]), 'south') the next word with max prob: tensor([0.0126])
The negative loglikelihood (-log p(defending)): 10.334543228149414 or 10.334543228149414 or 10.334543228149414
('survey', 'conducted', 'deep', 'south', 'watch', 'monitors')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3558, 'monitors') has the max prob in order to be the next word. The prob is: 1.2087662980775349e-05
833300 The nn predicts (tensor([23]), 'south') the next word with max prob: tensor([0.0882])
The negative loglikelihood (-log p(monitors)): 11.323325157165527 or 11.323325157165527 or 11.323325157165527
('francis', 'makes', 'pleas', 'peace', 'easter', 'sunday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (361, 'sunday') has the max prob in order to be the next word. The prob is: 0.0006971328984946012
833400 The nn predicts (tensor([127]), 'peace') the next word with max prob: tensor([0.0103])
The negative loglikelihood (-log p(sunday)): 7.2685346603393555 or 7.2685346603393555 or 7.2685346603393555
('chef', 'turned', 'author', 'ahmed', 'errachidi', 'five')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (225, 'five') has the max prob in order to be the next word. The prob is: 7.6013951911591e-05
833500 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0325])
The negative loglikelihood (-log p(five)): 9.484593391418457 or 9.484593391418457 or 9.484593391418457
('crowds', 'help', 'organise', 'protests', 'world', 'new')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4, 'new') has the max prob in order to be the next word. The prob is: 0.010457788594067097
833600 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.3422])
The negative loglikelihood (-log p(new)): 4.560408115386963 or 4.560408115386963 or 4.560408115386963
('business', 'despite', 'street', 'protests', 'complaints', 'local')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (466, 'local') has the max prob in order to be the next word. The prob is: 0.0020609882194548845
833700 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0146])
The negative loglikelihood (-log p(local)): 6.184569835662842 or 6.184569835662842 or 6.184569835662842
('phoenix', 'jihadist', 'dad', 'says', 'son', 'worked')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2866, 'worked') has the max prob in order to be the next word. The prob is: 0.00034887343645095825
833800 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0183])
The negative loglikelihood (-log p(worked)): 7.960801124572754 or 7.960801124572754 or 7.960801124572754
('<s>', '<s>', 'fighting', 'surges', 'northern', 'mali')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (732, 'mali') has the max prob in order to be the next word. The prob is: 0.08160124719142914
833900 The nn predicts (tensor([732]), 'mali') the next word with max prob: tensor([0.0816])
The negative loglikelihood (-log p(mali)): 2.505910873413086 or 2.505910634994507 or 2.505910634994507
('supreme', 'court', 'rejects', 'novartis', 'patent', 'petition')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2399, 'petition') has the max prob in order to be the next word. The prob is: 9.000234422273934e-05
834000 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0236])
The negative loglikelihood (-log p(petition)): 9.315674781799316 or 9.315674781799316 or 9.315674781799316
('farewell', 'nhs', 'dear', 'trusted', 'friend', 'finally')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1220, 'finally') has the max prob in order to be the next word. The prob is: 0.0009417862165719271
834100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0083])
The negative loglikelihood (-log p(finally)): 6.9677324295043945 or 6.9677324295043945 or 6.9677324295043945
('papers', 'may', 'parliamentary', 'polls', 'increasing', 'number')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (445, 'number') has the max prob in order to be the next word. The prob is: 0.001095653627999127
834200 The nn predicts (tensor([178]), 'political') the next word with max prob: tensor([0.0101])
The negative loglikelihood (-log p(number)): 6.816404342651367 or 6.816404342651367 or 6.816404342651367
('large', 'offshore', 'field', 'began', 'flowing', 'country')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35, 'country') has the max prob in order to be the next word. The prob is: 0.003990287892520428
834300 The nn predicts (tensor([66]), 'oil') the next word with max prob: tensor([0.0338])
The negative loglikelihood (-log p(country)): 5.523891925811768 or 5.523891925811768 or 5.523891925811768
('india', 'top', 'court', 'rejected', 'swiss', 'drug')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (184, 'drug') has the max prob in order to be the next word. The prob is: 0.01095011830329895
834400 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0241])
The negative loglikelihood (-log p(drug)): 4.514405250549316 or 4.514405250549316 or 4.514405250549316
('made', 'history', 'becoming', 'first', 'woman', 'run')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (587, 'run') has the max prob in order to be the next word. The prob is: 0.0011891042813658714
834500 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0922])
The negative loglikelihood (-log p(run)): 6.734554767608643 or 6.734554767608643 or 6.734554767608643
('steal', 'oil', 'pipeline', 'turn', 'like', 'hoped')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7354, 'hoped') has the max prob in order to be the next word. The prob is: 9.445609975955449e-06
834600 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0167])
The negative loglikelihood (-log p(hoped)): 11.569960594177246 or 11.569960594177246 or 11.569960594177246
('shifts', 'nuclear', 'war', 'rhetoric', 'talk', 'economic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (272, 'economic') has the max prob in order to be the next word. The prob is: 0.0021341200917959213
834700 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.0967])
The negative loglikelihood (-log p(economic)): 6.14970064163208 or 6.14970064163208 or 6.14970064163208
('landmark', 'india', 'cancer', 'drug', 'patent', 'case')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (177, 'case') has the max prob in order to be the next word. The prob is: 0.013520215637981892
834800 The nn predicts (tensor([184]), 'drug') the next word with max prob: tensor([0.0678])
The negative loglikelihood (-log p(case)): 4.303569316864014 or 4.303569316864014 or 4.303569316864014
('cool', 'planet', 'must', 'approved', 'world', 'governments')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1364, 'governments') has the max prob in order to be the next word. The prob is: 0.0008169172215275466
834900 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0547])
The negative loglikelihood (-log p(governments)): 7.109972953796387 or 7.109972953796387 or 7.109972953796387
('north', 'korean', 'border', 'us', 'moves', 'warships')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1881, 'warships') has the max prob in order to be the next word. The prob is: 7.778171129757538e-05
835000 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.1525])
The negative loglikelihood (-log p(warships)): 9.461604118347168 or 9.461604118347168 or 9.461604118347168
('things', 'exist', 'often', 'tragic', 'abundance', 'lost')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (584, 'lost') has the max prob in order to be the next word. The prob is: 0.0004031648568343371
835100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0150])
The negative loglikelihood (-log p(lost)): 7.816164970397949 or 7.816164970397949 or 7.816164970397949
('becomes', 'first', 'airline', 'introduce', 'pay', 'weigh')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5917, 'weigh') has the max prob in order to be the next word. The prob is: 6.443041911552427e-06
835200 The nn predicts (tensor([358]), 'money') the next word with max prob: tensor([0.0356])
The negative loglikelihood (-log p(weigh)): 11.952509880065918 or 11.952509880065918 or 11.952509880065918
('looking', 'best', 'modality', 'woo', 'tourists', 'asian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1298, 'asian') has the max prob in order to be the next word. The prob is: 0.0003497689322102815
835300 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0524])
The negative loglikelihood (-log p(asian)): 7.958237648010254 or 7.958237648010254 or 7.958237648010254
('lee', 'hsien', 'loong', 'first', 'oval', 'office')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (483, 'office') has the max prob in order to be the next word. The prob is: 0.003769826842471957
835400 The nn predicts (tensor([28]), 'first') the next word with max prob: tensor([0.0548])
The negative loglikelihood (-log p(office)): 5.580726146697998 or 5.580726146697998 or 5.580726146697998
('two', 'italian', 'marines', 'accused', 'killing', 'two')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (31, 'two') has the max prob in order to be the next word. The prob is: 0.025073368102312088
835500 The nn predicts (tensor([31]), 'two') the next word with max prob: tensor([0.0251])
The negative loglikelihood (-log p(two)): 3.6859490871429443 or 3.6859490871429443 or 3.6859490871429443
('nuclear', 'talks', 'javier', 'solana', 'former', 'nato')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (192, 'nato') has the max prob in order to be the next word. The prob is: 0.00020350066188257188
835600 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0773])
The negative loglikelihood (-log p(nato)): 8.499841690063477 or 8.499841690063477 or 8.499841690063477
('raising', 'limit', 'business', 'transactions', 'euros', 'allowing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1682, 'allowing') has the max prob in order to be the next word. The prob is: 0.00030041194986552
835700 The nn predicts (tensor([154]), 'billion') the next word with max prob: tensor([0.0362])
The negative loglikelihood (-log p(allowing)): 8.110356330871582 or 8.110356330871582 or 8.110356330871582
('cyprus', 'bailout', 'finance', 'minister', 'michael', 'sarris')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44100, 'sarris') has the max prob in order to be the next word. The prob is: 3.2322927836503368e-06
835800 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(sarris)): 12.642318725585938 or 12.642318725585938 or 12.642318725585938
('mairead', 'found', 'guilty', 'manslaughter', 'six', 'children')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (93, 'children') has the max prob in order to be the next word. The prob is: 0.002696083392947912
835900 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.3369])
The negative loglikelihood (-log p(children)): 5.915955066680908 or 5.915955066680908 or 5.915955066680908
('sangzi', 'hunan', 'province', 'forced', 'climb', 'tall')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7936, 'tall') has the max prob in order to be the next word. The prob is: 9.378006325277966e-06
836000 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0217])
The negative loglikelihood (-log p(tall)): 11.577143669128418 or 11.577143669128418 or 11.577143669128418
('jordan', 'press', 'questions', 'related', 'canadian', 'domestic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1536, 'domestic') has the max prob in order to be the next word. The prob is: 0.00018728763097897172
836100 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0289])
The negative loglikelihood (-log p(domestic)): 8.582864761352539 or 8.582864761352539 or 8.582864761352539
('british', 'couple', 'guilty', 'fire', 'deaths', 'six')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (351, 'six') has the max prob in order to be the next word. The prob is: 0.0031091030687093735
836200 The nn predicts (tensor([31]), 'two') the next word with max prob: tensor([0.0276])
The negative loglikelihood (-log p(six)): 5.773420810699463 or 5.773420810699463 or 5.773420810699463
('data', 'protection', 'authorities', 'conduct', 'formal', 'investigations')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3753, 'investigations') has the max prob in order to be the next word. The prob is: 0.00013999533257447183
836300 The nn predicts (tensor([1300]), 'treaty') the next word with max prob: tensor([0.0195])
The negative loglikelihood (-log p(investigations)): 8.8739013671875 or 8.8739013671875 or 8.8739013671875
('chavez', 'appears', 'president', 'candidate', 'little', 'bird')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1698, 'bird') has the max prob in order to be the next word. The prob is: 0.00011836984049296007
836400 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(bird)): 9.041696548461914 or 9.041696548461914 or 9.041696548461914
('students', 'angered', 'mass', 'outbreak', 'food', 'poisoning')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2965, 'poisoning') has the max prob in order to be the next word. The prob is: 0.0002525013987906277
836500 The nn predicts (tensor([209]), 'workers') the next word with max prob: tensor([0.0141])
The negative loglikelihood (-log p(poisoning)): 8.284093856811523 or 8.284093856811523 or 8.284093856811523
('said', 'france', 'cnil', 'data', 'protection', 'agency')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (304, 'agency') has the max prob in order to be the next word. The prob is: 0.0036730668507516384
836600 The nn predicts (tensor([316]), 'data') the next word with max prob: tensor([0.0229])
The negative loglikelihood (-log p(agency)): 5.606728553771973 or 5.6067280769348145 or 5.6067280769348145
('korea', 'bans', 'south', 'joint', 'industrial', 'zone')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (629, 'zone') has the max prob in order to be the next word. The prob is: 0.01248969417065382
836700 The nn predicts (tensor([1314]), 'reactor') the next word with max prob: tensor([0.0726])
The negative loglikelihood (-log p(zone)): 4.382851600646973 or 4.382851600646973 or 4.382851600646973
('inquiry', 'australia', 'opened', 'national', 'investigation', 'child')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (246, 'child') has the max prob in order to be the next word. The prob is: 0.0008582525188103318
836800 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0469])
The negative loglikelihood (-log p(child)): 7.060612201690674 or 7.060612201690674 or 7.060612201690674
('state', 'aid', 'investigation', 'ffp', 'implications', 'real')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (737, 'real') has the max prob in order to be the next word. The prob is: 0.00021835243387613446
836900 The nn predicts (tensor([430]), 'deaths') the next word with max prob: tensor([0.0187])
The negative loglikelihood (-log p(real)): 8.429400444030762 or 8.429400444030762 or 8.429400444030762
('worst', 'harder', 'decades', 'govt', 'refers', 'tax')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (274, 'tax') has the max prob in order to be the next word. The prob is: 0.0011688247323036194
837000 The nn predicts (tensor([178]), 'political') the next word with max prob: tensor([0.0134])
The negative loglikelihood (-log p(tax)): 6.75175666809082 or 6.75175666809082 or 6.75175666809082
('cables', 'vulnerable', 'think', 'saboteurs', 'would', 'dive')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8862, 'dive') has the max prob in order to be the next word. The prob is: 5.390526439441601e-06
837100 The nn predicts (tensor([201]), 'get') the next word with max prob: tensor([0.0298])
The negative loglikelihood (-log p(dive)): 12.130867958068848 or 12.130867958068848 or 12.130867958068848
('leg', 'span', 'inches', 'across', 'living', 'trees')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3250, 'trees') has the max prob in order to be the next word. The prob is: 0.00022351321240421385
837200 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0154])
The negative loglikelihood (-log p(trees)): 8.40604019165039 or 8.40604019165039 or 8.40604019165039
('new', 'human', 'cases', 'identified', 'late', 'tuesday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (324, 'tuesday') has the max prob in order to be the next word. The prob is: 0.015345847234129906
837300 The nn predicts (tensor([324]), 'tuesday') the next word with max prob: tensor([0.0153])
The negative loglikelihood (-log p(tuesday)): 4.176910400390625 or 4.176910400390625 or 4.176910400390625
('young', 'pakistanis', 'prefer', 'shariah', 'democracy', 'pessimistic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16434, 'pessimistic') has the max prob in order to be the next word. The prob is: 6.517270776384976e-06
837400 The nn predicts (tensor([118]), 'party') the next word with max prob: tensor([0.0112])
The negative loglikelihood (-log p(pessimistic)): 11.941055297851562 or 11.941055297851562 or 11.941055297851562
('new', 'bird', 'flu', 'claims', 'another', 'victim')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (921, 'victim') has the max prob in order to be the next word. The prob is: 0.0005057688686065376
837500 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0143])
The negative loglikelihood (-log p(victim)): 7.589430809020996 or 7.589430809020996 or 7.589430809020996
('person', 'compelled', 'heroism', 'real', 'story', 'syrian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (30, 'syrian') has the max prob in order to be the next word. The prob is: 8.693040581420064e-05
837600 The nn predicts (tensor([804]), 'story') the next word with max prob: tensor([0.0194])
The negative loglikelihood (-log p(syrian)): 9.35040283203125 or 9.35040283203125 or 9.35040283203125
('italian', 'court', 'orders', 'mafia', 'assets', 'seized')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (853, 'seized') has the max prob in order to be the next word. The prob is: 0.000545834715012461
837700 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0115])
The negative loglikelihood (-log p(seized)): 7.513194561004639 or 7.513194561004639 or 7.513194561004639
('could', 'harder', 'track', 'cousin', 'might', 'able')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1725, 'able') has the max prob in order to be the next word. The prob is: 0.004518216475844383
837800 The nn predicts (tensor([201]), 'get') the next word with max prob: tensor([0.0504])
The negative loglikelihood (-log p(able)): 5.3996381759643555 or 5.3996381759643555 or 5.3996381759643555
('models', 'ate', 'tissues', 'mask', 'hunger', 'revelations')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2700, 'revelations') has the max prob in order to be the next word. The prob is: 2.8485959774116054e-05
837900 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(revelations)): 10.46609878540039 or 10.46609878540039 or 10.46609878540039
('blog', 'del', 'narco', 'speaks', 'first', 'time')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (101, 'time') has the max prob in order to be the next word. The prob is: 0.07367216050624847
838000 The nn predicts (tensor([101]), 'time') the next word with max prob: tensor([0.0737])
The negative loglikelihood (-log p(time)): 2.6081302165985107 or 2.6081302165985107 or 2.6081302165985107
('german', 'painter', 'sculptor', 'emeritus', 'cornelius', 'richter')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19252, 'richter') has the max prob in order to be the next word. The prob is: 4.89600870423601e-06
838100 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0276])
The negative loglikelihood (-log p(richter)): 12.227089881896973 or 12.227089881896973 or 12.227089881896973
('graphic', 'iran', 'north', 'korea', 'respective', 'missile')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (250, 'missile') has the max prob in order to be the next word. The prob is: 0.009444953873753548
838200 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0232])
The negative loglikelihood (-log p(missile)): 4.6622748374938965 or 4.6622748374938965 or 4.6622748374938965
('north', 'korean', 'submarines', 'disappeared', 'chosun', 'tv')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (374, 'tv') has the max prob in order to be the next word. The prob is: 0.0007559585501439869
838300 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0794])
The negative loglikelihood (-log p(tv)): 7.18752384185791 or 7.18752384185791 or 7.18752384185791
('condemned', 'reported', 'saudi', 'court', 'ruling', 'sentencing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6473, 'sentencing') has the max prob in order to be the next word. The prob is: 2.548708835092839e-05
838400 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0337])
The negative loglikelihood (-log p(sentencing)): 10.577338218688965 or 10.577338218688965 or 10.577338218688965
('leaders', 'refuse', 'recognise', 'car', 'rebel', 'chief')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (114, 'chief') has the max prob in order to be the next word. The prob is: 0.0018917216220870614
838500 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.0307])
The negative loglikelihood (-log p(chief)): 6.270267963409424 or 6.270267963409424 or 6.270267963409424
('account', 'jon', 'stewart', 'link', 'mursi', 'office')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (483, 'office') has the max prob in order to be the next word. The prob is: 0.002598388120532036
838600 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0216])
The negative loglikelihood (-log p(office)): 5.952864170074463 or 5.952864170074463 or 5.952864170074463
('guinean', 'minister', 'trade', 'mohammed', 'dorval', 'doumbouya')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (62127, 'doumbouya') has the max prob in order to be the next word. The prob is: 2.398891638222267e-06
838700 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0184])
The negative loglikelihood (-log p(doumbouya)): 12.94050407409668 or 12.94050407409668 or 12.94050407409668
('muzzled', 'arrested', 'arab', 'gulf', 'states', 'kuwait')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2071, 'kuwait') has the max prob in order to be the next word. The prob is: 5.661438262904994e-05
838800 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0121])
The negative loglikelihood (-log p(kuwait)): 9.779247283935547 or 9.779247283935547 or 9.779247283935547
('<s>', 'israeli', 'troops', 'kill', 'palestinian', 'teens')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2559, 'teens') has the max prob in order to be the next word. The prob is: 1.2814786714443471e-05
838900 The nn predicts (tensor([103]), 'border') the next word with max prob: tensor([0.0399])
The negative loglikelihood (-log p(teens)): 11.264910697937012 or 11.264910697937012 or 11.264910697937012
('art', 'museums', 'city', 'canals', 'van', 'goghs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44134, 'goghs') has the max prob in order to be the next word. The prob is: 4.35745641880203e-06
839000 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(goghs)): 12.343622207641602 or 12.343622207641602 or 12.343622207641602
('bird', 'flu', 'killed', 'three', 'people', 'japan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (40, 'japan') has the max prob in order to be the next word. The prob is: 0.0001595055073266849
839100 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0792])
The negative loglikelihood (-log p(japan)): 8.74343204498291 or 8.74343204498291 or 8.74343204498291
('zimbabwe', 'country', 'chief', 'got', 'close', 'fired')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (731, 'fired') has the max prob in order to be the next word. The prob is: 2.6315057766623795e-05
839200 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0243])
The negative loglikelihood (-log p(fired)): 10.545369148254395 or 10.545369148254395 or 10.545369148254395
('police', 'officer', 'jailed', 'selling', 'seized', 'drugs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (645, 'drugs') has the max prob in order to be the next word. The prob is: 0.0005968516343273222
839300 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0111])
The negative loglikelihood (-log p(drugs)): 7.423841953277588 or 7.423841953277588 or 7.423841953277588
('mexican', 'drug', 'war', 'female', 'founder', 'mexico')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (145, 'mexico') has the max prob in order to be the next word. The prob is: 0.0009039011783897877
839400 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0096])
The negative loglikelihood (-log p(mexico)): 7.008790493011475 or 7.008790493011475 or 7.008790493011475
('athabasca', 'river', 'downriver', 'oil', 'sands', 'developments')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6700, 'developments') has the max prob in order to be the next word. The prob is: 1.8450980860507116e-05
839500 The nn predicts (tensor([66]), 'oil') the next word with max prob: tensor([0.0182])
The negative loglikelihood (-log p(developments)): 10.90039348602295 or 10.90039348602295 or 10.90039348602295
('thailand', 'main', 'index', 'rebounded', 'thursday', 'afternoon')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5882, 'afternoon') has the max prob in order to be the next word. The prob is: 7.096430636011064e-05
839600 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0192])
The negative loglikelihood (-log p(afternoon)): 9.553333282470703 or 9.553333282470703 or 9.553333282470703
('south', 'koreans', 'tough', 'kaesong', 'companies', 'run')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (587, 'run') has the max prob in order to be the next word. The prob is: 0.0018787237349897623
839700 The nn predicts (tensor([148]), 'take') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(run)): 6.277162551879883 or 6.277162551879883 or 6.277162551879883
('start', 'charging', 'visitors', 'entrance', 'fee', 'bid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (511, 'bid') has the max prob in order to be the next word. The prob is: 0.0004806828510481864
839800 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0206])
The negative loglikelihood (-log p(bid)): 7.640302658081055 or 7.640302658081055 or 7.640302658081055
('leaked', 'records', 'enormous', 'leak', 'financial', 'records')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1765, 'records') has the max prob in order to be the next word. The prob is: 0.0005051715997979045
839900 The nn predicts (tensor([333]), 'system') the next word with max prob: tensor([0.0268])
The negative loglikelihood (-log p(records)): 7.590612411499023 or 7.590612411499023 or 7.590612411499023
('wanted', 'washington', 'kingpin', 'international', 'drugs', 'trade')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (227, 'trade') has the max prob in order to be the next word. The prob is: 0.009458093903958797
840000 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0335])
The negative loglikelihood (-log p(trade)): 4.660884380340576 or 4.660884380340576 or 4.660884380340576
('un', 'says', 'suspending', 'food', 'distribution', 'centres')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4740, 'centres') has the max prob in order to be the next word. The prob is: 2.7678848709911108e-05
840100 The nn predicts (tensor([227]), 'trade') the next word with max prob: tensor([0.0266])
The negative loglikelihood (-log p(centres)): 10.494841575622559 or 10.494841575622559 or 10.494841575622559
('every', 'book', 'pamphlet', 'magazine', 'newspaper', 'published')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1476, 'published') has the max prob in order to be the next word. The prob is: 0.005055658519268036
840200 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0162])
The negative loglikelihood (-log p(published)): 5.287247180938721 or 5.287247180938721 or 5.287247180938721
('trade', 'minister', 'naftali', 'bennett', 'warned', 'obama')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (69, 'obama') has the max prob in order to be the next word. The prob is: 0.0003255182527936995
840300 The nn predicts (tensor([362]), 'wednesday') the next word with max prob: tensor([0.0154])
The negative loglikelihood (-log p(obama)): 8.030092239379883 or 8.030092239379883 or 8.030092239379883
('iran', 'friday', 'accept', 'offer', 'ease', 'economic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (272, 'economic') has the max prob in order to be the next word. The prob is: 0.0042417701333761215
840400 The nn predicts (tensor([32]), 'nuclear') the next word with max prob: tensor([0.0245])
The negative loglikelihood (-log p(economic)): 5.462774753570557 or 5.462774753570557 or 5.462774753570557
('undergoes', 'transition', 'democracy', 'president', 'thein', 'sein')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14730, 'sein') has the max prob in order to be the next word. The prob is: 3.126087540294975e-05
840500 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0257])
The negative loglikelihood (-log p(sein)): 10.373143196105957 or 10.373143196105957 or 10.373143196105957
('equipment', 'pakistan', 'likely', 'worth', 'billion', 'citing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2319, 'citing') has the max prob in order to be the next word. The prob is: 6.71846282784827e-05
840600 The nn predicts (tensor([154]), 'billion') the next word with max prob: tensor([0.0812])
The negative loglikelihood (-log p(citing)): 9.60806655883789 or 9.60806655883789 or 9.60806655883789
('north', 'korea', 'kim', 'jong', 'un', 'loves')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9757, 'loves') has the max prob in order to be the next word. The prob is: 3.28614555655804e-06
840700 The nn predicts (tensor([34]), 'un') the next word with max prob: tensor([0.1015])
The negative loglikelihood (-log p(loves)): 12.625795364379883 or 12.625795364379883 or 12.625795364379883
('stopped', 'reporting', 'cartels', 'govt', 'remained', 'silent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3097, 'silent') has the max prob in order to be the next word. The prob is: 9.015712566906586e-05
840800 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0070])
The negative loglikelihood (-log p(silent)): 9.313956260681152 or 9.313956260681152 or 9.313956260681152
('closed', 'culling', 'birds', 'curb', 'spread', 'flu')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1095, 'flu') has the max prob in order to be the next word. The prob is: 0.007383889518678188
840900 The nn predicts (tensor([266]), 'across') the next word with max prob: tensor([0.0420])
The negative loglikelihood (-log p(flu)): 4.908454895019531 or 4.908454895019531 or 4.908454895019531
('new', 'rules', 'cyberwar', 'wars', 'also', 'fought')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3154, 'fought') has the max prob in order to be the next word. The prob is: 9.20711609069258e-05
841000 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0158])
The negative loglikelihood (-log p(fought)): 9.292948722839355 or 9.292948722839355 or 9.292948722839355
('discover', 'fires', 'imaginations', 'enthusiasm', 'work', 'arts')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7830, 'arts') has the max prob in order to be the next word. The prob is: 1.89131260412978e-05
841100 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.0075])
The negative loglikelihood (-log p(arts)): 10.875654220581055 or 10.875654220581055 or 10.875654220581055
('pakistan', 'launches', 'offensive', 'restive', 'northwest', 'valley')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2987, 'valley') has the max prob in order to be the next word. The prob is: 0.00028200403903611004
841200 The nn predicts (tensor([103]), 'border') the next word with max prob: tensor([0.1060])
The negative loglikelihood (-log p(valley)): 8.173588752746582 or 8.173588752746582 or 8.173588752746582
('man', 'gone', 'daycare', 'gun', 'gatineau', 'quebec')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1920, 'quebec') has the max prob in order to be the next word. The prob is: 0.00012082151079084724
841300 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(quebec)): 9.021196365356445 or 9.021196365356445 or 9.021196365356445
('without', 'power', 'torrential', 'rain', 'buenos', 'aires')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5834, 'aires') has the max prob in order to be the next word. The prob is: 0.0014016609638929367
841400 The nn predicts (tensor([413]), 'coast') the next word with max prob: tensor([0.0164])
The negative loglikelihood (-log p(aires)): 6.57009744644165 or 6.57009744644165 or 6.57009744644165
('pakistani', 'judges', 'provoke', 'laughter', 'criticism', 'oddball')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (39467, 'oddball') has the max prob in order to be the next word. The prob is: 1.3726004226555233e-06
841500 The nn predicts (tensor([720]), 'islam') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(oddball)): 13.49880313873291 or 13.49880313873291 or 13.49880313873291
('case', 'syria', 'international', 'community', 'using', 'hands')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1134, 'hands') has the max prob in order to be the next word. The prob is: 0.00017661084712017328
841600 The nn predicts (tensor([75]), 'women') the next word with max prob: tensor([0.0071])
The negative loglikelihood (-log p(hands)): 8.641561508178711 or 8.641561508178711 or 8.641561508178711
('judge', 'ruling', 'friday', 'bp', 'means', 'company')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (353, 'company') has the max prob in order to be the next word. The prob is: 6.778930401196703e-05
841700 The nn predicts (tensor([137]), 'end') the next word with max prob: tensor([0.0105])
The negative loglikelihood (-log p(company)): 9.599105834960938 or 9.599105834960938 or 9.599105834960938
('transit', 'fare', 'increase', 'adds', 'fast', 'mexico')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (145, 'mexico') has the max prob in order to be the next word. The prob is: 0.0007503936067223549
841800 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0166])
The negative loglikelihood (-log p(mexico)): 7.194912910461426 or 7.194912910461426 or 7.194912910461426
('pope', 'francis', 'donates', 'flood', 'victims', 'argentina')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (757, 'argentina') has the max prob in order to be the next word. The prob is: 0.0003700657107401639
841900 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0130])
The negative loglikelihood (-log p(argentina)): 7.901830196380615 or 7.901830196380615 or 7.901830196380615
('<s>', '<s>', 'syria', 'ruins', 'focus', 'photos')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (728, 'photos') has the max prob in order to be the next word. The prob is: 0.00031958319596014917
842000 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0280])
The negative loglikelihood (-log p(photos)): 8.048493385314941 or 8.048493385314941 or 8.048493385314941
('lavrov', 'calls', 'security', 'measures', 'ahead', 'nato')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (192, 'nato') has the max prob in order to be the next word. The prob is: 0.00033930232166312635
842100 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0589])
The negative loglikelihood (-log p(nato)): 7.988618850708008 or 7.988618850708008 or 7.988618850708008
('haiti', 'aid', 'largely', 'went', 'us', 'groups')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (327, 'groups') has the max prob in order to be the next word. The prob is: 0.0002030550385825336
842200 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0270])
The negative loglikelihood (-log p(groups)): 8.502033233642578 or 8.502033233642578 or 8.502033233642578
('organisation', 'imo', 'rejected', 'motion', 'supporting', 'regulation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4208, 'regulation') has the max prob in order to be the next word. The prob is: 7.045415259199217e-05
842300 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0300])
The negative loglikelihood (-log p(regulation)): 9.560548782348633 or 9.560548782348633 or 9.560548782348633
('homes', 'due', 'atrocities', 'burmese', 'government', 'forces')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (54, 'forces') has the max prob in order to be the next word. The prob is: 0.02144191786646843
842400 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0322])
The negative loglikelihood (-log p(forces)): 3.842407464981079 or 3.842407464981079 or 3.842407464981079
('thoughts', 'fighting', 'afghanistan', 'news', 'peace', 'talks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (83, 'talks') has the max prob in order to be the next word. The prob is: 0.04688817262649536
842500 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0469])
The negative loglikelihood (-log p(talks)): 3.0599899291992188 or 3.0599899291992188 or 3.0599899291992188
('use', 'cards', 'pay', 'though', 'transactions', 'cost')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (869, 'cost') has the max prob in order to be the next word. The prob is: 0.0010017488384619355
842600 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0104])
The negative loglikelihood (-log p(cost)): 6.906007766723633 or 6.906007766723633 or 6.906007766723633
('reactions', 'cautious', 'media', 'published', 'list', 'alleged')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (390, 'alleged') has the max prob in order to be the next word. The prob is: 0.00029541520052589476
842700 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0219])
The negative loglikelihood (-log p(alleged)): 8.127128601074219 or 8.127128601074219 or 8.127128601074219
('british', 'woman', 'stabbed', 'death', 'india', 'named')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1275, 'named') has the max prob in order to be the next word. The prob is: 0.00014564351295121014
842800 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0459])
The negative loglikelihood (-log p(named)): 8.834348678588867 or 8.834348678588867 or 8.834348678588867
('australian', 'election', 'plans', 'fielding', 'candidates', 'least')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (84, 'least') has the max prob in order to be the next word. The prob is: 0.0004114460025448352
842900 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0332])
The negative loglikelihood (-log p(least)): 7.795832633972168 or 7.795832633972168 or 7.795832633972168
('committee', 'almost', 'every', 'aspect', 'every', 'project')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (802, 'project') has the max prob in order to be the next word. The prob is: 7.997168722795323e-05
843000 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.2669])
The negative loglikelihood (-log p(project)): 9.433837890625 or 9.433837890625 or 9.433837890625
('genuine', 'denominator', 'likely', 'due', 'one', 'three')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (97, 'three') has the max prob in order to be the next word. The prob is: 0.00426203990355134
843100 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0217])
The negative loglikelihood (-log p(three)): 5.458007335662842 or 5.458007335662842 or 5.458007335662842
('powers', 'iran', 'fail', 'end', 'nuclear', 'deadlock')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5870, 'deadlock') has the max prob in order to be the next word. The prob is: 9.480414155405015e-05
843200 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.3044])
The negative loglikelihood (-log p(deadlock)): 9.263697624206543 or 9.263697624206543 or 9.263697624206543
('carefully', 'sorted', 'waste', 'dumped', 'foreign', 'landfill')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12884, 'landfill') has the max prob in order to be the next word. The prob is: 1.938670720846858e-06
843300 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.1356])
The negative loglikelihood (-log p(landfill)): 13.153508186340332 or 13.153508186340332 or 13.153508186340332
('drone', 'strike', 'giving', 'tight', 'control', 'list')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (710, 'list') has the max prob in order to be the next word. The prob is: 0.00022761097352486104
843400 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0208])
The negative loglikelihood (-log p(list)): 8.387872695922852 or 8.387872695922852 or 8.387872695922852
('inquiry', 'british', 'intelligence', 'shows', 'former', 'pm')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (117, 'pm') has the max prob in order to be the next word. The prob is: 0.0021163029596209526
843500 The nn predicts (tensor([77]), 'former') the next word with max prob: tensor([0.0313])
The negative loglikelihood (-log p(pm)): 6.158084392547607 or 6.158084392547607 or 6.158084392547607
('venezuela', 'interim', 'president', 'maduro', 'addresses', 'topic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9210, 'topic') has the max prob in order to be the next word. The prob is: 1.1434044608904514e-05
843600 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0276])
The negative loglikelihood (-log p(topic)): 11.378915786743164 or 11.378914833068848 or 11.378914833068848
('israel', 'today', 'also', 'holocaust', 'memorial', 'day')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (121, 'day') has the max prob in order to be the next word. The prob is: 0.01790635846555233
843700 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0321])
The negative loglikelihood (-log p(day)): 4.022599220275879 or 4.022599220275879 or 4.022599220275879
('conference', 'development', 'reconstruction', 'sudan', 'darfur', 'region')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (357, 'region') has the max prob in order to be the next word. The prob is: 0.006345361936837435
843800 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0331])
The negative loglikelihood (-log p(region)): 5.060030937194824 or 5.060030937194824 or 5.060030937194824
('politicians', 'companies', 'worldwide', 'face', 'questions', 'offshore')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2070, 'offshore') has the max prob in order to be the next word. The prob is: 0.003055399749428034
843900 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0077])
The negative loglikelihood (-log p(offshore)): 5.790844917297363 or 5.790844917297363 or 5.790844917297363
('korea', 'says', 'north', 'missile', 'launch', 'may')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (47, 'may') has the max prob in order to be the next word. The prob is: 0.00026108179008588195
844000 The nn predicts (tensor([250]), 'missile') the next word with max prob: tensor([0.1406])
The negative loglikelihood (-log p(may)): 8.250677108764648 or 8.250677108764648 or 8.250677108764648
('thousands', 'killed', 'ongoing', 'drug', 'war', 'rejected')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1709, 'rejected') has the max prob in order to be the next word. The prob is: 9.213403245666996e-05
844100 The nn predicts (tensor([331]), 'crimes') the next word with max prob: tensor([0.0891])
The negative loglikelihood (-log p(rejected)): 9.292265892028809 or 9.292265892028809 or 9.292265892028809
('evil', 'city', 'traders', 'cause', 'banking', 'crash')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (253, 'crash') has the max prob in order to be the next word. The prob is: 0.000359761732397601
844200 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.0820])
The negative loglikelihood (-log p(crash)): 7.930068492889404 or 7.930068492889404 or 7.930068492889404
('future', 'afghans', 'writing', 'taliban', 'stronger', 'ever')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (509, 'ever') has the max prob in order to be the next word. The prob is: 0.0012085549533367157
844300 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0084])
The negative loglikelihood (-log p(ever)): 6.718329906463623 or 6.718329906463623 or 6.718329906463623
('soldiers', 'young', 'diplomat', 'saturday', 'american', 'civilian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1184, 'civilian') has the max prob in order to be the next word. The prob is: 0.00014130440831650048
844400 The nn predicts (tensor([172]), 'soldiers') the next word with max prob: tensor([0.0194])
The negative loglikelihood (-log p(civilian)): 8.864594459533691 or 8.864594459533691 or 8.864594459533691
('ends', 'violent', 'clashes', 'killing', 'one', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.0033142392057925463
844500 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0469])
The negative loglikelihood (-log p(police)): 5.709527015686035 or 5.709527015686035 or 5.709527015686035
('uruguay', 'one', 'laptop', 'per', 'child', 'program')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (596, 'program') has the max prob in order to be the next word. The prob is: 2.2957592591410503e-05
844600 The nn predicts (tensor([386]), 'abuse') the next word with max prob: tensor([0.0672])
The negative loglikelihood (-log p(program)): 10.681861877441406 or 10.681861877441406 or 10.681861877441406
('government', 'could', 'increase', 'pressure', 'public', 'political')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (178, 'political') has the max prob in order to be the next word. The prob is: 0.002239389345049858
844700 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0279])
The negative loglikelihood (-log p(political)): 6.1015520095825195 or 6.1015520095825195 or 6.1015520095825195
('kabul', 'killing', 'least', 'nine', 'civilians', 'wounding')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4149, 'wounding') has the max prob in order to be the next word. The prob is: 0.0021034947130829096
844800 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.1272])
The negative loglikelihood (-log p(wounding)): 6.164155006408691 or 6.164155006408691 or 6.164155006408691
('convicted', 'smuggling', 'million', 'worth', 'cocaine', 'resort')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2459, 'resort') has the max prob in order to be the next word. The prob is: 0.00025247823214158416
844900 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.0188])
The negative loglikelihood (-log p(resort)): 8.284185409545898 or 8.284185409545898 or 8.284185409545898
('new', 'radioactive', 'water', 'leak', 'reported', 'fukushima')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (416, 'fukushima') has the max prob in order to be the next word. The prob is: 0.017391497269272804
845000 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0326])
The negative loglikelihood (-log p(fukushima)): 4.051774024963379 or 4.051774024963379 or 4.051774024963379
('says', 'pulling', 'workers', 'joint', 'industrial', 'zone')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (629, 'zone') has the max prob in order to be the next word. The prob is: 0.07554139941930771
845100 The nn predicts (tensor([629]), 'zone') the next word with max prob: tensor([0.0755])
The negative loglikelihood (-log p(zone)): 2.5830745697021484 or 2.5830743312835693 or 2.5830743312835693
('north', 'korea', 'suspends', 'kaesong', 'operations', 'withdraws')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3388, 'withdraws') has the max prob in order to be the next word. The prob is: 9.519548257230781e-06
845200 The nn predicts (tensor([23]), 'south') the next word with max prob: tensor([0.1840])
The negative loglikelihood (-log p(withdraws)): 11.562163352966309 or 11.562163352966309 or 11.562163352966309
('korea', 'said', 'monday', 'recall', 'north', 'korean')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (195, 'korean') has the max prob in order to be the next word. The prob is: 0.2345900535583496
845300 The nn predicts (tensor([22]), 'korea') the next word with max prob: tensor([0.5834])
The negative loglikelihood (-log p(korean)): 1.4499157667160034 or 1.4499157667160034 or 1.4499157667160034
('harder', 'predict', 'cold', 'snap', 'fingers', 'crossed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4061, 'crossed') has the max prob in order to be the next word. The prob is: 1.6666821466060355e-05
845400 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0189])
The negative loglikelihood (-log p(crossed)): 11.002090454101562 or 11.002090454101562 or 11.002090454101562
('big', 'face', 'found', 'sri', 'lanka', 'also')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (464, 'also') has the max prob in order to be the next word. The prob is: 0.00017867617134470493
845500 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0527])
The negative loglikelihood (-log p(also)): 8.629935264587402 or 8.629935264587402 or 8.629935264587402
('pirates', 'foreign', 'journalists', 'paid', 'kenyans', 'pretending')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11683, 'pretending') has the max prob in order to be the next word. The prob is: 5.465324647957459e-06
845600 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0099])
The negative loglikelihood (-log p(pretending)): 12.117087364196777 or 12.117087364196777 or 12.117087364196777
('dam', 'upon', 'completion', 'displace', 'thousands', 'native')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4156, 'native') has the max prob in order to be the next word. The prob is: 6.425060564652085e-05
845700 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0878])
The negative loglikelihood (-log p(native)): 9.652719497680664 or 9.652719497680664 or 9.652719497680664
('costs', 'asking', 'government', 'withhold', 'percent', 'payments')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2674, 'payments') has the max prob in order to be the next word. The prob is: 0.0002622074098326266
845800 The nn predicts (tensor([1769]), 'euros') the next word with max prob: tensor([0.0267])
The negative loglikelihood (-log p(payments)): 8.24637508392334 or 8.24637508392334 or 8.24637508392334
('ontario', 'putting', 'end', 'power', 'plants', 'verge')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4013, 'verge') has the max prob in order to be the next word. The prob is: 2.7872665668837726e-05
845900 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0184])
The negative loglikelihood (-log p(verge)): 10.48786449432373 or 10.48786449432373 or 10.48786449432373
('falls', 'short', 'promised', 'havoc', 'hackers', 'vowed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3379, 'vowed') has the max prob in order to be the next word. The prob is: 6.220513751031831e-05
846000 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0155])
The negative loglikelihood (-log p(vowed)): 9.685072898864746 or 9.685072898864746 or 9.685072898864746
('north', 'korea', 'kim', 'aunt', 'kim', 'driving')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1715, 'driving') has the max prob in order to be the next word. The prob is: 2.3345091904047877e-05
846100 The nn predicts (tensor([1498]), 'jong') the next word with max prob: tensor([0.3154])
The negative loglikelihood (-log p(driving)): 10.66512393951416 or 10.66512393951416 or 10.66512393951416
('women', 'girls', 'willing', 'admit', 'raped', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.00034445608616806567
846200 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0545])
The negative loglikelihood (-log p(syria)): 7.973544120788574 or 7.973544120788574 or 7.973544120788574
('allegedly', 'executing', 'prisoners', 'officers', 'gone', 'trial')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (238, 'trial') has the max prob in order to be the next word. The prob is: 0.0034673085901886225
846300 The nn predicts (tensor([115]), 'back') the next word with max prob: tensor([0.0350])
The negative loglikelihood (-log p(trial)): 5.664376735687256 or 5.664376735687256 or 5.664376735687256
('korea', 'dispatch', 'destroyers', 'track', 'north', 'korean')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (195, 'korean') has the max prob in order to be the next word. The prob is: 0.2025952935218811
846400 The nn predicts (tensor([22]), 'korea') the next word with max prob: tensor([0.5948])
The negative loglikelihood (-log p(korean)): 1.5965449810028076 or 1.596544861793518 or 1.596544861793518
('interest', 'lawmakers', 'argued', 'banks', 'overcharged', 'kuwaitis')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17152, 'kuwaitis') has the max prob in order to be the next word. The prob is: 1.7611487237445544e-06
846500 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0310])
The negative loglikelihood (-log p(kuwaitis)): 13.249544143676758 or 13.249544143676758 or 13.249544143676758
('chinese', 'magazine', 'report', 'abuses', 'notorious', 'labor')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1091, 'labor') has the max prob in order to be the next word. The prob is: 0.00022689503384754062
846600 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0110])
The negative loglikelihood (-log p(labor)): 8.391022682189941 or 8.391022682189941 or 8.391022682189941
('<s>', 'japan', 'deploys', 'korean', 'missiles', 'tokyo')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1047, 'tokyo') has the max prob in order to be the next word. The prob is: 0.0016431512776762247
846700 The nn predicts (tensor([250]), 'missile') the next word with max prob: tensor([0.0543])
The negative loglikelihood (-log p(tokyo)): 6.411139488220215 or 6.411139488220215 or 6.411139488220215
('jack', 'tytell', 'two', 'life', 'sentences', 'additional')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3165, 'additional') has the max prob in order to be the next word. The prob is: 1.2975739991816226e-05
846800 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.2034])
The negative loglikelihood (-log p(additional)): 11.252429008483887 or 11.252429008483887 or 11.252429008483887
('dogs', 'shocking', 'footage', 'shows', 'dogs', 'kept')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2077, 'kept') has the max prob in order to be the next word. The prob is: 0.00016919754853006452
846900 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0154])
The negative loglikelihood (-log p(kept)): 8.684443473815918 or 8.684443473815918 or 8.684443473815918
('françois', 'hollande', 'thanks', 'helping', 'repel', 'islamist')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (447, 'islamist') has the max prob in order to be the next word. The prob is: 0.0022173882462084293
847000 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0282])
The negative loglikelihood (-log p(islamist)): 6.111425399780273 or 6.111425399780273 or 6.111425399780273
('set', 'previous', 'generations', 'communist', 'emperors', 'leading')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (821, 'leading') has the max prob in order to be the next word. The prob is: 0.0003433574747759849
847100 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0104])
The negative loglikelihood (-log p(leading)): 7.976738452911377 or 7.976738452911377 or 7.976738452911377
('<s>', '<s>', 'circus', 'elephant', 'injured', 'shooting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (567, 'shooting') has the max prob in order to be the next word. The prob is: 0.0007035828894004226
847200 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0813])
The negative loglikelihood (-log p(shooting)): 7.25932502746582 or 7.25932502746582 or 7.25932502746582
('cbc', 'court', 'tax', 'list', 'see', 'breaking')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1749, 'breaking') has the max prob in order to be the next word. The prob is: 0.00013221213885117322
847300 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.0106])
The negative loglikelihood (-log p(breaking)): 8.931102752685547 or 8.931102752685547 or 8.931102752685547
('thatcher', 'rule', 'good', 'britain', 'disagree', 'tackling')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5970, 'tackling') has the max prob in order to be the next word. The prob is: 7.938895578263327e-06
847400 The nn predicts (tensor([368]), 'policy') the next word with max prob: tensor([0.0110])
The negative loglikelihood (-log p(tackling)): 11.743736267089844 or 11.743736267089844 or 11.743736267089844
('safety', 'police', 'office', 'arms', 'store', 'april')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1530, 'april') has the max prob in order to be the next word. The prob is: 0.0004521082737483084
847500 The nn predicts (tensor([575]), 'station') the next word with max prob: tensor([0.0075])
The negative loglikelihood (-log p(april)): 7.701589107513428 or 7.7015886306762695 or 7.7015886306762695
('n', 'korean', 'tour', 'operator', 'cancels', 'tourist')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1362, 'tourist') has the max prob in order to be the next word. The prob is: 0.00014620939327869564
847600 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.0577])
The negative loglikelihood (-log p(tourist)): 8.83047103881836 or 8.83047103881836 or 8.83047103881836
('london', 'becoming', 'pompei', 'north', 'one', 'archaeological')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6531, 'archaeological') has the max prob in order to be the next word. The prob is: 1.0529882274568081e-05
847700 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0410])
The negative loglikelihood (-log p(archaeological)): 11.46129322052002 or 11.46129322052002 or 11.46129322052002
('term', 'insulting', 'religious', 'objects', 'punishable', 'roubles')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18409, 'roubles') has the max prob in order to be the next word. The prob is: 3.846217623504344e-06
847800 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0450])
The negative loglikelihood (-log p(roubles)): 12.468420028686523 or 12.468420028686523 or 12.468420028686523
('illegally', 'obtained', 'kept', 'thousands', 'iraq', 'cultural')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2223, 'cultural') has the max prob in order to be the next word. The prob is: 8.904784044716507e-05
847900 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0889])
The negative loglikelihood (-log p(cultural)): 9.326336860656738 or 9.326336860656738 or 9.326336860656738
('<s>', 'fewer', 'death', 'sentences', 'world', 'report')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (51, 'report') has the max prob in order to be the next word. The prob is: 0.006908286362886429
848000 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0675])
The negative loglikelihood (-log p(report)): 4.975033760070801 or 4.975033760070801 or 4.975033760070801
('rights', 'pregnancy', 'complaint', 'focuses', 'meeting', 'nova')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7733, 'nova') has the max prob in order to be the next word. The prob is: 2.482074023646419e-06
848100 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.0347])
The negative loglikelihood (-log p(nova)): 12.906415939331055 or 12.906415939331055 or 12.906415939331055
('city', 'mistakenly', 'tweets', 'north', 'korean', 'missile')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (250, 'missile') has the max prob in order to be the next word. The prob is: 0.06267620623111725
848200 The nn predicts (tensor([250]), 'missile') the next word with max prob: tensor([0.0627])
The negative loglikelihood (-log p(missile)): 2.769773483276367 or 2.769773483276367 or 2.769773483276367
('big', 'banks', 'canada', 'bringing', 'foreign', 'workers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (209, 'workers') has the max prob in order to be the next word. The prob is: 0.00946016889065504
848300 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0808])
The negative loglikelihood (-log p(workers)): 4.660665035247803 or 4.660665035247803 or 4.660665035247803
('syria', 'israeli', 'forces', 'may', 'operating', 'capacity')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2986, 'capacity') has the max prob in order to be the next word. The prob is: 3.6277757317293435e-05
848400 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0331])
The negative loglikelihood (-log p(capacity)): 10.224306106567383 or 10.224306106567383 or 10.224306106567383
('<s>', 'case', 'rehtaeh', 'parsons', 'canada', 'steubenville')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (29570, 'steubenville') has the max prob in order to be the next word. The prob is: 2.131979272235185e-06
848500 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0357])
The negative loglikelihood (-log p(steubenville)): 13.058460235595703 or 13.058460235595703 or 13.058460235595703
('sign', 'document', 'guarantee', 'would', 'recognise', 'results')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1207, 'results') has the max prob in order to be the next word. The prob is: 0.00018960692977998406
848600 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0243])
The negative loglikelihood (-log p(results)): 8.570557594299316 or 8.570557594299316 or 8.570557594299316
('legacy', 'chávism', 'april', 'venezuela', 'elect', 'successor')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4809, 'successor') has the max prob in order to be the next word. The prob is: 0.00024347497674170882
848700 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0602])
The negative loglikelihood (-log p(successor)): 8.320496559143066 or 8.320496559143066 or 8.320496559143066
('culture', 'ministry', 'sent', 'letter', 'local', 'media')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (132, 'media') has the max prob in order to be the next word. The prob is: 0.05476208031177521
848800 The nn predicts (tensor([132]), 'media') the next word with max prob: tensor([0.0548])
The negative loglikelihood (-log p(media)): 2.904757261276245 or 2.904757261276245 or 2.904757261276245
('<s>', '<s>', '<s>', 'margaret', 'thatcher', 'never')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (794, 'never') has the max prob in order to be the next word. The prob is: 8.749969128984958e-05
848900 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.1086])
The negative loglikelihood (-log p(never)): 9.34387493133545 or 9.34387493133545 or 9.34387493133545
('<s>', 'waging', 'war', 'court', 'sultan', 'sulu')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22760, 'sulu') has the max prob in order to be the next word. The prob is: 1.1393780368962325e-05
849000 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(sulu)): 11.382442474365234 or 11.382442474365234 or 11.382442474365234
('nine', 'experts', 'investigated', 'crisis', 'said', 'new')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4, 'new') has the max prob in order to be the next word. The prob is: 0.005275463219732046
849100 The nn predicts (tensor([295]), 'monday') the next word with max prob: tensor([0.0360])
The negative loglikelihood (-log p(new)): 5.244688987731934 or 5.244688987731934 or 5.244688987731934
('counts', 'plagiarism', 'misleading', 'public', 'academic', 'qualifications')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22634, 'qualifications') has the max prob in order to be the next word. The prob is: 4.9731215767678805e-06
849200 The nn predicts (tensor([51]), 'report') the next word with max prob: tensor([0.0094])
The negative loglikelihood (-log p(qualifications)): 12.21146297454834 or 12.21146297454834 or 12.21146297454834
('official', 'states', 'nk', 'missile', 'launcher', 'raised')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1889, 'raised') has the max prob in order to be the next word. The prob is: 0.0001327298377873376
849300 The nn predicts (tensor([367]), 'launch') the next word with max prob: tensor([0.0859])
The negative loglikelihood (-log p(raised)): 8.927194595336914 or 8.927194595336914 or 8.927194595336914
('prime', 'minister', 'carlos', 'gomes', 'júnior', 'exile')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4163, 'exile') has the max prob in order to be the next word. The prob is: 0.00024941860465332866
849400 The nn predicts (tensor([77]), 'former') the next word with max prob: tensor([0.0146])
The negative loglikelihood (-log p(exile)): 8.296378135681152 or 8.296378135681152 or 8.296378135681152
('closing', 'temporary', 'amid', 'missile', 'test', 'alert')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (931, 'alert') has the max prob in order to be the next word. The prob is: 0.0008756937459111214
849500 The nn predicts (tensor([367]), 'launch') the next word with max prob: tensor([0.0378])
The negative loglikelihood (-log p(alert)): 7.040493965148926 or 7.040493965148926 or 7.040493965148926
('despite', 'organ', 'donations', 'rising', 'uk', 'possible')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (453, 'possible') has the max prob in order to be the next word. The prob is: 0.00014105789887253195
849600 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0089])
The negative loglikelihood (-log p(possible)): 8.866339683532715 or 8.866339683532715 or 8.866339683532715
('economy', 'lawmaker', 'told', 'reuters', 'tuesday', 'april')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1530, 'april') has the max prob in order to be the next word. The prob is: 0.011359135620296001
849700 The nn predicts (tensor([324]), 'tuesday') the next word with max prob: tensor([0.0217])
The negative loglikelihood (-log p(april)): 4.477733135223389 or 4.477733135223389 or 4.477733135223389
('syrian', 'government', 'air', 'strikes', 'target', 'civilians')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (329, 'civilians') has the max prob in order to be the next word. The prob is: 0.014756816439330578
849800 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.0698])
The negative loglikelihood (-log p(civilians)): 4.216050148010254 or 4.216050148010254 or 4.216050148010254
('china', 'iran', 'iraq', 'saudi', 'arabia', 'us')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3, 'us') has the max prob in order to be the next word. The prob is: 0.0015333563787862659
849900 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0532])
The negative loglikelihood (-log p(us)): 6.4802961349487305 or 6.4802961349487305 or 6.4802961349487305
('israeli', 'police', 'thursday', 'detained', 'five', 'women')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (75, 'women') has the max prob in order to be the next word. The prob is: 0.010972284711897373
850000 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0508])
The negative loglikelihood (-log p(women)): 4.512382984161377 or 4.512382984161377 or 4.512382984161377
('agencies', 'western', 'world', 'going', 'south', 'korea')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22, 'korea') has the max prob in order to be the next word. The prob is: 0.41641560196876526
850100 The nn predicts (tensor([22]), 'korea') the next word with max prob: tensor([0.4164])
The negative loglikelihood (-log p(korea)): 0.8760714530944824 or 0.8760714530944824 or 0.8760714530944824
('project', 'organization', 'promotes', 'gay', 'lesbian', 'equality')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3174, 'equality') has the max prob in order to be the next word. The prob is: 0.0012839933624491096
850200 The nn predicts (tensor([606]), 'marriage') the next word with max prob: tensor([0.0561])
The negative loglikelihood (-log p(equality)): 6.657780170440674 or 6.657780170440674 or 6.657780170440674
('fluids', 'inject', 'deep', 'underground', 'extract', 'natural')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1526, 'natural') has the max prob in order to be the next word. The prob is: 0.001257173833437264
850300 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.0112])
The negative loglikelihood (-log p(natural)): 6.678889274597168 or 6.678889274597168 or 6.678889274597168
('pakistan', 'candidate', 'targeted', 'killed', 'official', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.05876714736223221
850400 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0588])
The negative loglikelihood (-log p(says)): 2.834172248840332 or 2.834172248840332 or 2.834172248840332
('concerns', 'buckingham', 'palace', 'questions', 'raised', 'appropriateness')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (62293, 'appropriateness') has the max prob in order to be the next word. The prob is: 1.0176112255066982e-06
850500 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.0122])
The negative loglikelihood (-log p(appropriateness)): 13.798052787780762 or 13.798052787780762 or 13.798052787780762
('budget', 'raise', 'taxes', 'jérome', 'cahuzac', 'admitted')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2528, 'admitted') has the max prob in order to be the next word. The prob is: 0.00021342127001844347
850600 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.2127])
The negative loglikelihood (-log p(admitted)): 8.452242851257324 or 8.452242851257324 or 8.452242851257324
('suggest', 'north', 'korean', 'regime', 'fully', 'tested')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2926, 'tested') has the max prob in order to be the next word. The prob is: 8.767603139858693e-05
850700 The nn predicts (tensor([250]), 'missile') the next word with max prob: tensor([0.0260])
The negative loglikelihood (-log p(tested)): 9.341861724853516 or 9.341861724853516 or 9.341861724853516
('learning', 'based', 'love', 'others', 'love', 'one')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (37, 'one') has the max prob in order to be the next word. The prob is: 0.0053887576796114445
850800 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0159])
The negative loglikelihood (-log p(one)): 5.223440170288086 or 5.223440170288086 or 5.223440170288086
('take', 'part', 'possible', 'amnesty', 'deal', 'offered')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1809, 'offered') has the max prob in order to be the next word. The prob is: 0.00010157904762309045
850900 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0170])
The negative loglikelihood (-log p(offered)): 9.194673538208008 or 9.194673538208008 or 9.194673538208008
('scrutiny', 'rejections', 'plummet', 'britain', 'financial', 'regulator')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3575, 'regulator') has the max prob in order to be the next word. The prob is: 3.694990664371289e-05
851000 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.1047])
The negative loglikelihood (-log p(regulator)): 10.205947875976562 or 10.205947875976562 or 10.205947875976562
('nicosia', 'said', 'cost', 'surged', 'billion', 'euros')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1769, 'euros') has the max prob in order to be the next word. The prob is: 0.04454689472913742
851100 The nn predicts (tensor([154]), 'billion') the next word with max prob: tensor([0.0580])
The negative loglikelihood (-log p(euros)): 3.111212968826294 or 3.111212730407715 or 3.111212730407715
('garbage', 'imported', 'mainly', 'w', 'asia', 'e')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6900, 'e') has the max prob in order to be the next word. The prob is: 1.7929560272023082e-05
851200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0231])
The negative loglikelihood (-log p(e)): 10.929059982299805 or 10.929059982299805 or 10.929059982299805
('report', 'mali', 'refugees', 'endure', 'appalling', 'conditions')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1279, 'conditions') has the max prob in order to be the next word. The prob is: 0.0012277596397325397
851300 The nn predicts (tensor([51]), 'report') the next word with max prob: tensor([0.0319])
The negative loglikelihood (-log p(conditions)): 6.702564239501953 or 6.702564239501953 or 6.702564239501953
('army', 'wipe', 'entire', 'murle', 'tribe', 'face')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (187, 'face') has the max prob in order to be the next word. The prob is: 0.0010115407640114427
851400 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0237])
The negative loglikelihood (-log p(face)): 6.896280765533447 or 6.896280765533447 or 6.896280765533447
('reports', 'people', 'liberation', 'army', 'pla', 'building')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (410, 'building') has the max prob in order to be the next word. The prob is: 0.000692289846483618
851500 The nn predicts (tensor([76]), 'army') the next word with max prob: tensor([0.0350])
The negative loglikelihood (-log p(building)): 7.275506019592285 or 7.275506019592285 or 7.275506019592285
('camps', 'likened', 'survivors', 'nazi', 'concentration', 'camps')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1503, 'camps') has the max prob in order to be the next word. The prob is: 0.0009078486473299563
851600 The nn predicts (tensor([532]), 'camp') the next word with max prob: tensor([0.0333])
The negative loglikelihood (-log p(camps)): 7.004432678222656 or 7.004432678222656 or 7.004432678222656
('dollars', 'danced', 'joy', 'presented', 'actual', 'lottery')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6936, 'lottery') has the max prob in order to be the next word. The prob is: 2.1672251023119316e-05
851700 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0109])
The negative loglikelihood (-log p(lottery)): 10.73947811126709 or 10.73947811126709 or 10.73947811126709
('cancer', 'patients', 'ontario', 'hospital', 'treated', 'chemotherapy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10542, 'chemotherapy') has the max prob in order to be the next word. The prob is: 4.116860145586543e-05
851800 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0203])
The negative loglikelihood (-log p(chemotherapy)): 10.097834587097168 or 10.097834587097168 or 10.097834587097168
('police', 'officer', 'gets', 'payout', 'falling', 'chair')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5157, 'chair') has the max prob in order to be the next word. The prob is: 5.940403298154706e-06
851900 The nn predicts (tensor([115]), 'back') the next word with max prob: tensor([0.0344])
The negative loglikelihood (-log p(chair)): 12.033733367919922 or 12.033733367919922 or 12.033733367919922
('marriage', 'bill', 'approved', 'french', 'senate', 'gay')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (251, 'gay') has the max prob in order to be the next word. The prob is: 0.005192833952605724
852000 The nn predicts (tensor([312]), 'bill') the next word with max prob: tensor([0.0837])
The negative loglikelihood (-log p(gay)): 5.2604756355285645 or 5.2604756355285645 or 5.2604756355285645
('islamic', 'state', 'iraq', 'levant', 'oh', 'fine')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1901, 'fine') has the max prob in order to be the next word. The prob is: 1.4257608199841343e-05
852100 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0162])
The negative loglikelihood (-log p(fine)): 11.158220291137695 or 11.158220291137695 or 11.158220291137695
('entering', 'sabah', 'charged', 'believed', 'political', 'group')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (62, 'group') has the max prob in order to be the next word. The prob is: 0.03539781644940376
852200 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0492])
The negative loglikelihood (-log p(group)): 3.3411052227020264 or 3.3411052227020264 or 3.3411052227020264
('john', 'kerry', 'china', 'amid', 'flaring', 'tensions')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (661, 'tensions') has the max prob in order to be the next word. The prob is: 0.054303620010614395
852300 The nn predicts (tensor([661]), 'tensions') the next word with max prob: tensor([0.0543])
The negative loglikelihood (-log p(tensions)): 2.9131643772125244 or 2.9131643772125244 or 2.9131643772125244
('mmr', 'scare', 'doctor', 'blames', 'uk', 'government')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11, 'government') has the max prob in order to be the next word. The prob is: 0.0031512051355093718
852400 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0170])
The negative loglikelihood (-log p(government)): 5.759970188140869 or 5.759970188140869 or 5.759970188140869
('published', 'list', 'us', 'officials', 'banned', 'russia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6, 'russia') has the max prob in order to be the next word. The prob is: 0.0011908217566087842
852500 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.0243])
The negative loglikelihood (-log p(russia)): 6.73311185836792 or 6.73311185836792 or 6.73311185836792
('time', 'completely', 'takes', 'away', 'border', 'protection')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1389, 'protection') has the max prob in order to be the next word. The prob is: 0.00013437676534522325
852600 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0260])
The negative loglikelihood (-log p(protection)): 8.914862632751465 or 8.914862632751465 or 8.914862632751465
('russia', 'bars', 'americans', 'response', 'magnitsky', 'list')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (710, 'list') has the max prob in order to be the next word. The prob is: 0.0035518722143024206
852700 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(list)): 5.640280246734619 or 5.640280246734619 or 5.640280246734619
('international', 'committee', 'red', 'cross', 'stopping', 'jail')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (397, 'jail') has the max prob in order to be the next word. The prob is: 0.00020566873718053102
852800 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0556])
The negative loglikelihood (-log p(jail)): 8.489243507385254 or 8.489243507385254 or 8.489243507385254
('mexico', 'state', 'puebla', 'ash', 'falling', 'ground')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (688, 'ground') has the max prob in order to be the next word. The prob is: 0.0021276127081364393
852900 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(ground)): 6.152754783630371 or 6.152754783630371 or 6.152754783630371
('maduro', 'accused', 'political', 'rivals', 'bringing', 'colombian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1782, 'colombian') has the max prob in order to be the next word. The prob is: 6.313875928753987e-05
853000 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0165])
The negative loglikelihood (-log p(colombian)): 9.670175552368164 or 9.670175552368164 or 9.670175552368164
('meets', 'june', 'uk', 'grove', 'hotel', 'north')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (21, 'north') has the max prob in order to be the next word. The prob is: 0.004000418353825808
853100 The nn predicts (tensor([207]), 'london') the next word with max prob: tensor([0.0083])
The negative loglikelihood (-log p(north)): 5.521356582641602 or 5.521356105804443 or 5.521356105804443
('voters', 'supported', 'fat', 'cat', 'initiative', 'referendum')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (605, 'referendum') has the max prob in order to be the next word. The prob is: 0.00014829248539172113
853200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0073])
The negative loglikelihood (-log p(referendum)): 8.816324234008789 or 8.816324234008789 or 8.816324234008789
('teenage', 'mother', 'kills', 'baby', 'pursue', 'studies')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3775, 'studies') has the max prob in order to be the next word. The prob is: 2.320416570000816e-05
853300 The nn predicts (tensor([420]), 'rape') the next word with max prob: tensor([0.0194])
The negative loglikelihood (-log p(studies)): 10.671178817749023 or 10.671178817749023 or 10.671178817749023
('would', 'use', 'second', 'term', 'reform', 'taxes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2107, 'taxes') has the max prob in order to be the next word. The prob is: 7.985124830156565e-05
853400 The nn predicts (tensor([312]), 'bill') the next word with max prob: tensor([0.0222])
The negative loglikelihood (-log p(taxes)): 9.435344696044922 or 9.435344696044922 or 9.435344696044922
('next', 'year', 'summer', 'olympic', 'games', 'brazil')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (196, 'brazil') has the max prob in order to be the next word. The prob is: 0.0006199172348715365
853500 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0326])
The negative loglikelihood (-log p(brazil)): 7.385924816131592 or 7.385924816131592 or 7.385924816131592
('investors', 'global', 'marketplace', 'still', 'struggling', 'make')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (215, 'make') has the max prob in order to be the next word. The prob is: 0.004760269541293383
853600 The nn predicts (tensor([285]), 'economy') the next word with max prob: tensor([0.0293])
The negative loglikelihood (-log p(make)): 5.347451210021973 or 5.347451210021973 or 5.347451210021973
('trial', 'killing', 'former', 'prime', 'minister', 'rafik')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16509, 'rafik') has the max prob in order to be the next word. The prob is: 2.885507456085179e-05
853700 The nn predicts (tensor([77]), 'former') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(rafik)): 10.453225135803223 or 10.453225135803223 or 10.453225135803223
('called', 'gandhi', 'old', 'witch', 'kissinger', 'met')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1718, 'met') has the max prob in order to be the next word. The prob is: 0.00012406791211105883
853800 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0222])
The negative loglikelihood (-log p(met)): 8.994681358337402 or 8.994681358337402 or 8.994681358337402
('<s>', 'anonymous', 'rehtaeh', 'parsons', 'full', 'statement')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1226, 'statement') has the max prob in order to be the next word. The prob is: 0.0004390077665448189
853900 The nn predicts (tensor([101]), 'time') the next word with max prob: tensor([0.0212])
The negative loglikelihood (-log p(statement)): 7.730993270874023 or 7.730993270874023 or 7.730993270874023
('countries', 'banks', 'least', 'trusted', 'banks', 'india')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (27, 'india') has the max prob in order to be the next word. The prob is: 0.00023445830447599292
854000 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0101])
The negative loglikelihood (-log p(india)): 8.358232498168945 or 8.358232498168945 or 8.358232498168945
('help', 'country', 'forest', 'department', 'protect', 'violent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (744, 'violent') has the max prob in order to be the next word. The prob is: 0.0001438565377611667
854100 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0229])
The negative loglikelihood (-log p(violent)): 8.846693992614746 or 8.846693992614746 or 8.846693992614746
('favour', 'state', 'responsibilities', 'individualism', 'finds', 'survey')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1714, 'survey') has the max prob in order to be the next word. The prob is: 0.0003101809124927968
854200 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0140])
The negative loglikelihood (-log p(survey)): 8.078354835510254 or 8.078354835510254 or 8.078354835510254
('worst', 'case', 'scenario', 'missile', 'could', 'hit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (171, 'hit') has the max prob in order to be the next word. The prob is: 0.001718558487482369
854300 The nn predicts (tensor([47]), 'may') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(hit)): 6.366269588470459 or 6.366269588470459 or 6.366269588470459
('morocco', 'powerful', 'king', 'pushed', 'reforms', 'two')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (31, 'two') has the max prob in order to be the next word. The prob is: 0.005326870828866959
854400 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0155])
The negative loglikelihood (-log p(two)): 5.234991073608398 or 5.234991073608398 or 5.234991073608398
('ukraine', 'president', 'tymoshenko', 'could', 'freed', 'post')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (723, 'post') has the max prob in order to be the next word. The prob is: 7.319833912333706e-06
854500 The nn predicts (tensor([77]), 'former') the next word with max prob: tensor([0.0369])
The negative loglikelihood (-log p(post)): 11.824922561645508 or 11.824922561645508 or 11.824922561645508
('government', 'trying', 'reduce', 'overfishing', 'popular', 'little')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1017, 'little') has the max prob in order to be the next word. The prob is: 0.0004099603393115103
854600 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0233])
The negative loglikelihood (-log p(little)): 7.799449920654297 or 7.799449920654297 or 7.799449920654297
('election', 'results', 'calls', 'foul', 'play', 'demands')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (730, 'demands') has the max prob in order to be the next word. The prob is: 0.0004883558140136302
854700 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.0125])
The negative loglikelihood (-log p(demands)): 7.624466419219971 or 7.624466419219971 or 7.624466419219971
('fire', 'hotel', 'internet', 'cafe', 'china', 'kills')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (108, 'kills') has the max prob in order to be the next word. The prob is: 0.004823663271963596
854800 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0233])
The negative loglikelihood (-log p(kills)): 5.334221839904785 or 5.334221839904785 or 5.334221839904785
('group', 'holds', 'segregated', 'lecture', 'leicester', 'student')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (642, 'student') has the max prob in order to be the next word. The prob is: 0.0022136301267892122
854900 The nn predicts (tensor([297]), 'school') the next word with max prob: tensor([0.0103])
The negative loglikelihood (-log p(student)): 6.113121509552002 or 6.113121509552002 or 6.113121509552002
('rising', 'demand', 'lax', 'laws', 'wiping', 'toothless')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19361, 'toothless') has the max prob in order to be the next word. The prob is: 2.5436904707021313e-06
855000 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0470])
The negative loglikelihood (-log p(toothless)): 12.881895065307617 or 12.881895065307617 or 12.881895065307617
('dead', 'hurt', 'wave', 'attacks', 'across', 'iraq')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44, 'iraq') has the max prob in order to be the next word. The prob is: 0.035860247910022736
855100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0641])
The negative loglikelihood (-log p(iraq)): 3.3281259536743164 or 3.3281259536743164 or 3.3281259536743164
('<s>', '<s>', '<s>', '<s>', 'horror', 'horrors')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7112, 'horrors') has the max prob in order to be the next word. The prob is: 1.5834613805054687e-05
855200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0267])
The negative loglikelihood (-log p(horrors)): 11.053312301635742 or 11.053312301635742 or 11.053312301635742
('attacks', 'canonisation', 'wicked', 'woman', 'laid', 'waste')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1690, 'waste') has the max prob in order to be the next word. The prob is: 1.7061776816262864e-05
855300 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(waste)): 10.978670120239258 or 10.978670120239258 or 10.978670120239258
('boost', 'oil', 'exports', 'japan', 'supporting', 'saudi')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (61, 'saudi') has the max prob in order to be the next word. The prob is: 0.0005475140060298145
855400 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0406])
The negative loglikelihood (-log p(saudi)): 7.510122299194336 or 7.510122299194336 or 7.510122299194336
('authority', 'fayyad', 'resignation', 'formally', 'accepted', 'remains')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (924, 'remains') has the max prob in order to be the next word. The prob is: 0.00014051729522179812
855500 The nn predicts (tensor([77]), 'former') the next word with max prob: tensor([0.0172])
The negative loglikelihood (-log p(remains)): 8.870180130004883 or 8.870180130004883 or 8.870180130004883
('korea', 'responds', 'ultimatum', 'apologizes', 'burning', 'portrait')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7168, 'portrait') has the max prob in order to be the next word. The prob is: 8.962937499745749e-06
855600 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(portrait)): 11.62241268157959 or 11.62241268157959 or 11.62241268157959
('australia', 'great', 'barrier', 'reef', 'unparalleled', 'beauty')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4705, 'beauty') has the max prob in order to be the next word. The prob is: 1.9886092559318058e-05
855700 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0133])
The negative loglikelihood (-log p(beauty)): 10.82548999786377 or 10.82548999786377 or 10.82548999786377
('stoking', 'tension', 'peninsula', 'new', 'ultimatum', 'south')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (23, 'south') has the max prob in order to be the next word. The prob is: 0.006697606761008501
855800 The nn predicts (tensor([661]), 'tensions') the next word with max prob: tensor([0.0175])
The negative loglikelihood (-log p(south)): 5.006004810333252 or 5.006004810333252 or 5.006004810333252
('least', 'boys', 'known', 'talibes', 'mostly', 'aged')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2175, 'aged') has the max prob in order to be the next word. The prob is: 0.0007793570403009653
855900 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0510])
The negative loglikelihood (-log p(aged)): 7.157041549682617 or 7.157041072845459 or 7.157041072845459
('recognise', 'climate', 'change', 'refugee', 'status', 'refugee')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (412, 'refugee') has the max prob in order to be the next word. The prob is: 8.520593837602064e-05
856000 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0728])
The negative loglikelihood (-log p(refugee)): 9.370439529418945 or 9.370439529418945 or 9.370439529418945
('hacktivism', 'behalf', 'rehtaeh', 'parsons', 'revolution', 'rape')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (420, 'rape') has the max prob in order to be the next word. The prob is: 0.0004763931210618466
856100 The nn predicts (tensor([47]), 'may') the next word with max prob: tensor([0.0139])
The negative loglikelihood (-log p(rape)): 7.649267196655273 or 7.649267196655273 or 7.649267196655273
('due', 'political', 'situation', 'senior', 'palestianian', 'churchmen')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44211, 'churchmen') has the max prob in order to be the next word. The prob is: 1.3805617982143303e-06
856200 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0285])
The negative loglikelihood (-log p(churchmen)): 13.493020057678223 or 13.493020057678223 or 13.493020057678223
('swansea', 'measles', 'outbreak', 'health', 'minister', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.21156907081604004
856300 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.2786])
The negative loglikelihood (-log p(says)): 1.553203821182251 or 1.5532037019729614 or 1.5532037019729614
('obama', 'putin', 'set', 'two', 'rounds', 'talks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (83, 'talks') has the max prob in order to be the next word. The prob is: 0.004566471092402935
856400 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0140])
The negative loglikelihood (-log p(talks)): 5.389014720916748 or 5.389014720916748 or 5.389014720916748
('death', 'penalty', 'war', 'crimes', 'decree', 'death')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (42, 'death') has the max prob in order to be the next word. The prob is: 0.0027428900357335806
856500 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0108])
The negative loglikelihood (-log p(death)): 5.898743152618408 or 5.898743152618408 or 5.898743152618408
('<s>', 'ghana', 'gold', 'mine', 'collapses', 'kills')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (108, 'kills') has the max prob in order to be the next word. The prob is: 0.0025173446629196405
856600 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0339])
The negative loglikelihood (-log p(kills)): 5.984550476074219 or 5.984550476074219 or 5.984550476074219
('united', 'states', 'engaged', 'practice', 'torture', 'nation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (480, 'nation') has the max prob in order to be the next word. The prob is: 0.00021484568424057215
856700 The nn predicts (tensor([577]), 'torture') the next word with max prob: tensor([0.0181])
The negative loglikelihood (-log p(nation)): 8.44559097290039 or 8.44559097290039 or 8.44559097290039
('boston', 'marathon', 'bombing', 'feds', 'raid', 'apartment')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3704, 'apartment') has the max prob in order to be the next word. The prob is: 0.00017862180538941175
856800 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0124])
The negative loglikelihood (-log p(apartment)): 8.630239486694336 or 8.630239486694336 or 8.630239486694336
('ap', 'coverage', 'syria', 'crisis', 'wins', 'pulitzer')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12195, 'pulitzer') has the max prob in order to be the next word. The prob is: 3.504692131173215e-06
856900 The nn predicts (tensor([127]), 'peace') the next word with max prob: tensor([0.0164])
The negative loglikelihood (-log p(pulitzer)): 12.561408042907715 or 12.561408042907715 or 12.561408042907715
('<s>', 'protests', 'disturbances', 'violence', 'continue', 'venezuela')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (176, 'venezuela') has the max prob in order to be the next word. The prob is: 0.0017579874256625772
857000 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0563])
The negative loglikelihood (-log p(venezuela)): 6.34358549118042 or 6.34358549118042 or 6.34358549118042
('venezuelan', 'nicolas', 'maduro', 'accused', 'opposition', 'tuesday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (324, 'tuesday') has the max prob in order to be the next word. The prob is: 0.0006975502474233508
857100 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0522])
The negative loglikelihood (-log p(tuesday)): 7.267935752868652 or 7.2679362297058105 or 7.2679362297058105
('siege', 'east', 'west', 'days', 'competition', 'china')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1, 'china') has the max prob in order to be the next word. The prob is: 0.0020744488574564457
857200 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0120])
The negative loglikelihood (-log p(china)): 6.1780595779418945 or 6.1780595779418945 or 6.1780595779418945
('national', 'third', 'person', 'killed', 'boston', 'marathon')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3474, 'marathon') has the max prob in order to be the next word. The prob is: 0.005037446040660143
857300 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0656])
The negative loglikelihood (-log p(marathon)): 5.290855884552002 or 5.290855884552002 or 5.290855884552002
('setback', 'since', 'taking', 'power', 'two', 'years')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (33, 'years') has the max prob in order to be the next word. The prob is: 0.35255035758018494
857400 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.3526])
The negative loglikelihood (-log p(years)): 1.0425617694854736 or 1.0425617694854736 or 1.0425617694854736
('mahmoud', 'ahmadinejad', 'visited', 'west', 'african', 'nation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (480, 'nation') has the max prob in order to be the next word. The prob is: 0.019819438457489014
857500 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.1399])
The negative loglikelihood (-log p(nation)): 3.9210920333862305 or 3.9210920333862305 or 3.9210920333862305
('racist', 'graffiti', 'targets', 'future', 'mosque', 'building')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (410, 'building') has the max prob in order to be the next word. The prob is: 0.0007850650581531227
857600 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0236])
The negative loglikelihood (-log p(building)): 7.149744033813477 or 7.149744033813477 or 7.149744033813477
('years', 'wicked', 'hotel', 'porter', 'raped', 'celebrity')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7346, 'celebrity') has the max prob in order to be the next word. The prob is: 1.5353680282714777e-05
857700 The nn predicts (tensor([2860]), 'boston') the next word with max prob: tensor([0.0123])
The negative loglikelihood (-log p(celebrity)): 11.084155082702637 or 11.084155082702637 or 11.084155082702637
('presidential', 'parliamentary', 'elections', 'national', 'treasury', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.0038572081830352545
857800 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0331])
The negative loglikelihood (-log p(said)): 5.557811737060547 or 5.557811737060547 or 5.557811737060547
('bangalore', 'improvised', 'explosive', 'device', 'ied', 'used')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (218, 'used') has the max prob in order to be the next word. The prob is: 0.004926779307425022
857900 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.1104])
The negative loglikelihood (-log p(used)): 5.313069820404053 or 5.313069820404053 or 5.313069820404053
('failed', 'emissions', 'trading', 'reform', 'end', 'european')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (113, 'european') has the max prob in order to be the next word. The prob is: 0.005212349817156792
858000 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.0446])
The negative loglikelihood (-log p(european)): 5.2567243576049805 or 5.2567243576049805 or 5.2567243576049805
('rocket', 'attack', 'kills', 'least', 'central', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.02182653732597828
858100 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0625])
The negative loglikelihood (-log p(syria)): 3.8246288299560547 or 3.8246288299560547 or 3.8246288299560547
('<s>', 'bomb', 'goes', 'indian', 'city', 'bengaluru')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15128, 'bengaluru') has the max prob in order to be the next word. The prob is: 5.935626745667832e-07
858200 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0441])
The negative loglikelihood (-log p(bengaluru)): 14.337122917175293 or 14.337122917175293 or 14.337122917175293
('camp', 'outside', 'li', 'headquarters', 'striking', 'dock')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5993, 'dock') has the max prob in order to be the next word. The prob is: 2.1038122213212773e-05
858300 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0318])
The negative loglikelihood (-log p(dock)): 10.769174575805664 or 10.769174575805664 or 10.769174575805664
('knifeman', 'ordered', 'bristol', 'women', 'take', 'hijabs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12963, 'hijabs') has the max prob in order to be the next word. The prob is: 5.847459760843776e-06
858400 The nn predicts (tensor([619]), 'streets') the next word with max prob: tensor([0.0170])
The negative loglikelihood (-log p(hijabs)): 12.049503326416016 or 12.049503326416016 or 12.049503326416016
('condemns', 'boston', 'marathon', 'attacks', 'takes', 'jab')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11454, 'jab') has the max prob in order to be the next word. The prob is: 3.861106506519718e-06
858500 The nn predicts (tensor([2860]), 'boston') the next word with max prob: tensor([0.0252])
The negative loglikelihood (-log p(jab)): 12.464556694030762 or 12.464556694030762 or 12.464556694030762
('got', 'man', 'despite', 'mistake', 'hostage', 'takers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (20485, 'takers') has the max prob in order to be the next word. The prob is: 2.5377485144417733e-06
858600 The nn predicts (tensor([1168]), 'bombings') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(takers)): 12.884233474731445 or 12.884233474731445 or 12.884233474731445
('months', 'far', 'jail', 'human', 'rights', 'watch')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (854, 'watch') has the max prob in order to be the next word. The prob is: 0.047268107533454895
858700 The nn predicts (tensor([854]), 'watch') the next word with max prob: tensor([0.0473])
The negative loglikelihood (-log p(watch)): 3.051919460296631 or 3.051919460296631 or 3.051919460296631
('fake', 'goods', 'drugs', 'human', 'trafficking', 'illicit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4866, 'illicit') has the max prob in order to be the next word. The prob is: 8.681849431013688e-05
858800 The nn predicts (tensor([668]), 'cases') the next word with max prob: tensor([0.0192])
The negative loglikelihood (-log p(illicit)): 9.351691246032715 or 9.351691246032715 or 9.351691246032715
('mounted', 'late', 'tony', 'nicklinson', 'help', 'doctor')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1313, 'doctor') has the max prob in order to be the next word. The prob is: 5.693625644198619e-05
858900 The nn predicts (tensor([151]), 'help') the next word with max prob: tensor([0.0255])
The negative loglikelihood (-log p(doctor)): 9.773578643798828 or 9.773578643798828 or 9.773578643798828
('turn', 'kenya', 'middle', 'income', 'nation', 'within')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (563, 'within') has the max prob in order to be the next word. The prob is: 0.0007742774323560297
859000 The nn predicts (tensor([276]), 'largest') the next word with max prob: tensor([0.0132])
The negative loglikelihood (-log p(within)): 7.163580417633057 or 7.163580417633057 or 7.163580417633057
('dogs', 'found', 'dead', 'china', 'prompting', 'chemical')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (313, 'chemical') has the max prob in order to be the next word. The prob is: 0.00027313397731631994
859100 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0171])
The negative loglikelihood (-log p(chemical)): 8.205548286437988 or 8.205548286437988 or 8.205548286437988
('instead', 'placing', 'bets', 'countries', 'even', 'afield')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (28710, 'afield') has the max prob in order to be the next word. The prob is: 1.9513299776008353e-06
859200 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0224])
The negative loglikelihood (-log p(afield)): 13.14699935913086 or 13.14699935913086 or 13.14699935913086
('companies', 'case', 'nigerians', 'royal', 'dutch', 'shell')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1707, 'shell') has the max prob in order to be the next word. The prob is: 3.5219469282310456e-05
859300 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.1920])
The negative loglikelihood (-log p(shell)): 10.253911972045898 or 10.253911972045898 or 10.253911972045898
('distributing', 'material', 'critical', 'country', 'autocratic', 'monarchy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5359, 'monarchy') has the max prob in order to be the next word. The prob is: 8.672483818372712e-05
859400 The nn predicts (tensor([170]), 'leaders') the next word with max prob: tensor([0.0252])
The negative loglikelihood (-log p(monarchy)): 9.35276985168457 or 9.35276985168457 or 9.35276985168457
('late', 'stage', 'cancer', 'saves', 'life', 'ambulance')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6426, 'ambulance') has the max prob in order to be the next word. The prob is: 8.656243153382093e-06
859500 The nn predicts (tensor([394]), 'old') the next word with max prob: tensor([0.0077])
The negative loglikelihood (-log p(ambulance)): 11.65722942352295 or 11.65722942352295 or 11.65722942352295
('ambassador', 'absent', 'united', 'nations', 'offer', 'silent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3097, 'silent') has the max prob in order to be the next word. The prob is: 9.29438101593405e-05
859600 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0254])
The negative loglikelihood (-log p(silent)): 9.283515930175781 or 9.283514976501465 or 9.283514976501465
('solihull', 'police', 'officers', 'arrest', 'man', 'dressed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4391, 'dressed') has the max prob in order to be the next word. The prob is: 0.00023959990357980132
859700 The nn predicts (tensor([815]), 'allegedly') the next word with max prob: tensor([0.0298])
The negative loglikelihood (-log p(dressed)): 8.336540222167969 or 8.336540222167969 or 8.336540222167969
('bullets', 'restaurant', 'eastern', 'kenyan', 'town', 'garissa')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10523, 'garissa') has the max prob in order to be the next word. The prob is: 1.1768615877372213e-06
859800 The nn predicts (tensor([112]), 'near') the next word with max prob: tensor([0.0272])
The negative loglikelihood (-log p(garissa)): 13.65265941619873 or 13.65265941619873 or 13.65265941619873
('arrest', 'answer', 'allegations', 'committed', 'treason', 'ignominious')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (62432, 'ignominious') has the max prob in order to be the next word. The prob is: 5.030881311540725e-07
859900 The nn predicts (tensor([296]), 'charges') the next word with max prob: tensor([0.1017])
The negative loglikelihood (-log p(ignominious)): 14.502500534057617 or 14.502500534057617 or 14.502500534057617
('cuts', 'subsidies', 'korea', 'groups', 'south', 'korea')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22, 'korea') has the max prob in order to be the next word. The prob is: 0.2462031990289688
860000 The nn predicts (tensor([22]), 'korea') the next word with max prob: tensor([0.2462])
The negative loglikelihood (-log p(korea)): 1.4015980958938599 or 1.4015980958938599 or 1.4015980958938599
('korea', 'tones', 'language', 'giving', 'hope', 'dialogue')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2493, 'dialogue') has the max prob in order to be the next word. The prob is: 0.0013153632171452045
860100 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.0324])
The negative loglikelihood (-log p(dialogue)): 6.633642673492432 or 6.633642673492432 or 6.633642673492432
('forgotten', 'mathematics', 'legend', 'vashishtha', 'narayan', 'singh')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5452, 'singh') has the max prob in order to be the next word. The prob is: 0.00012920817243866622
860200 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0524])
The negative loglikelihood (-log p(singh)): 8.954085350036621 or 8.954085350036621 or 8.954085350036621
('showing', 'male', 'circumcision', 'could', 'lower', 'rates')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1528, 'rates') has the max prob in order to be the next word. The prob is: 0.000288437120616436
860300 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0164])
The negative loglikelihood (-log p(rates)): 8.151033401489258 or 8.151033401489258 or 8.151033401489258
('rushes', 'back', 'maduro', 'casts', 'doubt', 'venezuela')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (176, 'venezuela') has the max prob in order to be the next word. The prob is: 0.007125642150640488
860400 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0220])
The negative loglikelihood (-log p(venezuela)): 4.944055557250977 or 4.944055557250977 or 4.944055557250977
('aid', 'end', 'year', 'un', 'high', 'commissioner')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3106, 'commissioner') has the max prob in order to be the next word. The prob is: 2.9899894798290916e-05
860500 The nn predicts (tensor([51]), 'report') the next word with max prob: tensor([0.0232])
The negative loglikelihood (-log p(commissioner)): 10.417655944824219 or 10.417655944824219 or 10.417655944824219
('recently', 'gays', 'lesbians', 'finding', 'novel', 'ways')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2618, 'ways') has the max prob in order to be the next word. The prob is: 0.00014068833843339235
860600 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0057])
The negative loglikelihood (-log p(ways)): 8.868963241577148 or 8.868963241577148 or 8.868963241577148
('doctors', 'find', 'bottle', 'candle', 'inside', 'rape')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (420, 'rape') has the max prob in order to be the next word. The prob is: 0.0009265582775697112
860700 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0082])
The negative loglikelihood (-log p(rape)): 6.984033584594727 or 6.984033584594727 or 6.984033584594727
('trial', 'former', 'dictator', 'ríos', 'montt', 'suspended')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1192, 'suspended') has the max prob in order to be the next word. The prob is: 0.00027634052094072104
860800 The nn predicts (tensor([238]), 'trial') the next word with max prob: tensor([0.0524])
The negative loglikelihood (-log p(suspended)): 8.193876266479492 or 8.193876266479492 or 8.193876266479492
('eu', 'plans', 'economic', 'help', 'syrian', 'rebels')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (86, 'rebels') has the max prob in order to be the next word. The prob is: 0.08471087366342545
860900 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0847])
The negative loglikelihood (-log p(rebels)): 2.4685113430023193 or 2.4685113430023193 or 2.4685113430023193
('maduro', 'sworn', 'venezuela', 'review', 'disputed', 'vote')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (175, 'vote') has the max prob in order to be the next word. The prob is: 0.0075761256739497185
861000 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0777])
The negative loglikelihood (-log p(vote)): 4.882753372192383 or 4.882753372192383 or 4.882753372192383
('office', 'asking', 'earlier', 'investigation', 'reopened', 'deal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (64, 'deal') has the max prob in order to be the next word. The prob is: 0.000866312999278307
861100 The nn predicts (tensor([385]), 'thursday') the next word with max prob: tensor([0.0308])
The negative loglikelihood (-log p(deal)): 7.05126428604126 or 7.05126428604126 or 7.05126428604126
('plant', 'rampal', 'bagerhat', 'district', 'joint', 'venture')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6840, 'venture') has the max prob in order to be the next word. The prob is: 2.7736565243685618e-05
861200 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(venture)): 10.492758750915527 or 10.492758750915527 or 10.492758750915527
('prostitutes', 'found', 'time', 'along', 'stretch', 'road')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (952, 'road') has the max prob in order to be the next word. The prob is: 0.0004793706757482141
861300 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0174])
The negative loglikelihood (-log p(road)): 7.643036365509033 or 7.643036365509033 or 7.643036365509033
('nz', 'maori', 'love', 'song', 'breaks', 'approval')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1964, 'approval') has the max prob in order to be the next word. The prob is: 6.890463555464521e-05
861400 The nn predicts (tensor([606]), 'marriage') the next word with max prob: tensor([0.0169])
The negative loglikelihood (-log p(approval)): 9.58278751373291 or 9.58278751373291 or 9.58278751373291
('injures', 'shallow', 'quake', 'strikes', 'sichuan', 'province')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (842, 'province') has the max prob in order to be the next word. The prob is: 0.02995876967906952
861500 The nn predicts (tensor([842]), 'province') the next word with max prob: tensor([0.0300])
The negative loglikelihood (-log p(province)): 3.5079331398010254 or 3.5079331398010254 or 3.5079331398010254
('eradicate', 'extreme', 'poverty', 'within', 'generation', 'raise')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1301, 'raise') has the max prob in order to be the next word. The prob is: 0.00011250204988755286
861600 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0210])
The negative loglikelihood (-log p(raise)): 9.092538833618164 or 9.092538833618164 or 9.092538833618164
('attacks', 'mortar', 'fire', 'failed', 'prevent', 'iraqis')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2276, 'iraqis') has the max prob in order to be the next word. The prob is: 3.888771607307717e-05
861700 The nn predicts (tensor([141]), 'violence') the next word with max prob: tensor([0.0436])
The negative loglikelihood (-log p(iraqis)): 10.154831886291504 or 10.154831886291504 or 10.154831886291504
('response', 'injuries', 'firefighters', 'attend', 'incident', 'blaze')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4050, 'blaze') has the max prob in order to be the next word. The prob is: 9.817970567382872e-05
861800 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(blaze)): 9.228711128234863 or 9.228711128234863 or 9.228711128234863
('great', 'news', 'everybody', 'amazon', 'setting', 'shop')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3362, 'shop') has the max prob in order to be the next word. The prob is: 6.28655543550849e-05
861900 The nn predicts (tensor([111]), 'fire') the next word with max prob: tensor([0.0143])
The negative loglikelihood (-log p(shop)): 9.674511909484863 or 9.674511909484863 or 9.674511909484863
('sharia', 'court', 'ruling', 'taliban', 'amputate', 'thieves')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3748, 'thieves') has the max prob in order to be the next word. The prob is: 6.194064917508513e-05
862000 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0188])
The negative loglikelihood (-log p(thieves)): 9.68933391571045 or 9.68933391571045 or 9.68933391571045
('syrian', 'rebels', 'receive', 'million', 'aid', 'united')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (140, 'united') has the max prob in order to be the next word. The prob is: 0.004279301501810551
862100 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0798])
The negative loglikelihood (-log p(united)): 5.453965663909912 or 5.453965663909912 or 5.453965663909912
('north', 'korea', 'moves', 'missile', 'launchers', 'east')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (156, 'east') has the max prob in order to be the next word. The prob is: 0.17587895691394806
862200 The nn predicts (tensor([156]), 'east') the next word with max prob: tensor([0.1759])
The negative loglikelihood (-log p(east)): 1.7379592657089233 or 1.7379592657089233 or 1.7379592657089233
('hamas', 'plead', 'eu', 'terror', 'list', 'removal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3271, 'removal') has the max prob in order to be the next word. The prob is: 3.174337325617671e-05
862300 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0223])
The negative loglikelihood (-log p(removal)): 10.357826232910156 or 10.357826232910156 or 10.357826232910156
('way', 'ets', 'well', 'problems', 'taking', 'green')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1049, 'green') has the max prob in order to be the next word. The prob is: 0.0001491381844971329
862400 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0149])
The negative loglikelihood (-log p(green)): 8.810637474060059 or 8.810637474060059 or 8.810637474060059
('names', 'merentes', 'finance', 'head', 'new', 'cabinet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1087, 'cabinet') has the max prob in order to be the next word. The prob is: 0.00109380844514817
862500 The nn predicts (tensor([583]), 'zealand') the next word with max prob: tensor([0.0304])
The negative loglikelihood (-log p(cabinet)): 6.818089485168457 or 6.818089485168457 or 6.818089485168457
('crimes', 'humanity', 'sparked', 'violence', 'elsewhere', 'country')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35, 'country') has the max prob in order to be the next word. The prob is: 0.019919976592063904
862600 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0558])
The negative loglikelihood (-log p(country)): 3.916032314300537 or 3.916032314300537 or 3.916032314300537
('dutt', 'willing', 'go', 'jail', 'files', 'review')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1405, 'review') has the max prob in order to be the next word. The prob is: 0.0004278295091353357
862700 The nn predicts (tensor([77]), 'former') the next word with max prob: tensor([0.0159])
The negative loglikelihood (-log p(review)): 7.756785869598389 or 7.756785869598389 or 7.756785869598389
('sidelined', 'govt', 'using', 'global', 'economic', 'crisis')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (65, 'crisis') has the max prob in order to be the next word. The prob is: 0.07868984341621399
862800 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.0787])
The negative loglikelihood (-log p(crisis)): 2.542241096496582 or 2.542241096496582 or 2.542241096496582
('<s>', 'horse', 'washington', 'riding', 'tunisia', 'ennahda')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16178, 'ennahda') has the max prob in order to be the next word. The prob is: 3.91330922866473e-06
862900 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0247])
The negative loglikelihood (-log p(ennahda)): 12.451127052307129 or 12.451127052307129 or 12.451127052307129
('korean', 'spy', 'blew', 'plane', 'married', 'intelligence')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (343, 'intelligence') has the max prob in order to be the next word. The prob is: 0.00015896650438662618
863000 The nn predicts (tensor([23]), 'south') the next word with max prob: tensor([0.0148])
The negative loglikelihood (-log p(intelligence)): 8.746816635131836 or 8.746816635131836 or 8.746816635131836
('syria', 'allow', 'purchase', 'crude', 'opposition', 'president')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12, 'president') has the max prob in order to be the next word. The prob is: 0.018968988209962845
863100 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0225])
The negative loglikelihood (-log p(president)): 3.9649498462677 or 3.9649498462677 or 3.9649498462677
('israeli', 'drones', 'en', 'route', 'monitor', 'situation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1136, 'situation') has the max prob in order to be the next word. The prob is: 0.00020406839030329138
863200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0526])
The negative loglikelihood (-log p(situation)): 8.497055053710938 or 8.497055053710938 or 8.497055053710938
('eu', 'near', 'austerity', 'limit', 'says', 'barroso')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9982, 'barroso') has the max prob in order to be the next word. The prob is: 2.1815026229887735e-06
863300 The nn predicts (tensor([157]), 'aid') the next word with max prob: tensor([0.0195])
The negative loglikelihood (-log p(barroso)): 13.035496711730957 or 13.035496711730957 or 13.035496711730957
('three', 'people', 'arrested', 'last', 'week', 'chinese')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (41, 'chinese') has the max prob in order to be the next word. The prob is: 0.0005693103885278106
863400 The nn predicts (tensor([194]), 'prison') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(chinese)): 7.4710845947265625 or 7.4710845947265625 or 7.4710845947265625
('<s>', 'europe', 'shrinking', 'military', 'spending', 'scrutiny')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3817, 'scrutiny') has the max prob in order to be the next word. The prob is: 9.396237874170765e-05
863500 The nn predicts (tensor([235]), 'high') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(scrutiny)): 9.272616386413574 or 9.272616386413574 or 9.272616386413574
('cede', 'control', 'certain', 'policy', 'domains', 'european')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (113, 'european') has the max prob in order to be the next word. The prob is: 0.0018333264160901308
863600 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0409])
The negative loglikelihood (-log p(european)): 6.301623344421387 or 6.301623344421387 or 6.301623344421387
('<s>', '<s>', 'china', 'confirms', 'cases', 'deaths')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (430, 'deaths') has the max prob in order to be the next word. The prob is: 0.007334022782742977
863700 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0462])
The negative loglikelihood (-log p(deaths)): 4.915231227874756 or 4.915231227874756 or 4.915231227874756
('suspects', 'planning', 'attack', 'commuter', 'train', 'arrests')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (474, 'arrests') has the max prob in order to be the next word. The prob is: 0.00010300977010047063
863800 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0502])
The negative loglikelihood (-log p(arrests)): 9.180686950683594 or 9.180686950683594 or 9.180686950683594
('year', 'year', 'finished', 'network', 'provide', 'gigabit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (27432, 'gigabit') has the max prob in order to be the next word. The prob is: 2.4001462861633627e-06
863900 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0110])
The negative loglikelihood (-log p(gigabit)): 12.939980506896973 or 12.939980506896973 or 12.939980506896973
('bombing', 'suspect', 'says', 'brother', 'acted', 'alone')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2097, 'alone') has the max prob in order to be the next word. The prob is: 0.00019564809917937964
864000 The nn predicts (tensor([745]), 'plot') the next word with max prob: tensor([0.0188])
The negative loglikelihood (-log p(alone)): 8.539193153381348 or 8.539193153381348 or 8.539193153381348
('fresh', 'boost', 'european', 'union', 'eased', 'oil')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (66, 'oil') has the max prob in order to be the next word. The prob is: 0.0036185753997415304
864100 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(oil)): 5.62167501449585 or 5.62167501449585 or 5.62167501449585
('secret', 'meeting', 'wikileaks', 'founder', 'google', 'chairman')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2420, 'chairman') has the max prob in order to be the next word. The prob is: 0.00011211395030841231
864200 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0622])
The negative loglikelihood (-log p(chairman)): 9.09599494934082 or 9.09599494934082 or 9.09599494934082
('denies', 'link', 'canada', 'calls', 'thwarted', 'train')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (494, 'train') has the max prob in order to be the next word. The prob is: 0.001311629661358893
864300 The nn predicts (tensor([745]), 'plot') the next word with max prob: tensor([0.0515])
The negative loglikelihood (-log p(train)): 6.6364850997924805 or 6.6364850997924805 or 6.6364850997924805
('least', 'injured', 'downtown', 'moscow', 'blast', 'reports')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (234, 'reports') has the max prob in order to be the next word. The prob is: 0.0021923230960965157
864400 The nn predicts (tensor([108]), 'kills') the next word with max prob: tensor([0.0330])
The negative loglikelihood (-log p(reports)): 6.122793674468994 or 6.122793674468994 or 6.122793674468994
('said', 'tuesday', 'syrian', 'government', 'repeatedly', 'used')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (218, 'used') has the max prob in order to be the next word. The prob is: 0.00182581995613873
864500 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0478])
The negative loglikelihood (-log p(used)): 6.305726051330566 or 6.305726051330566 or 6.305726051330566
('celebrated', 'earth', 'day', 'mexico', 'city', 'may')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (47, 'may') has the max prob in order to be the next word. The prob is: 0.001271065091714263
864600 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0091])
The negative loglikelihood (-log p(may)): 6.667900085449219 or 6.667900085449219 or 6.667900085449219
('austerity', 'early', 'based', 'someone', 'accidentally', 'updating')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13729, 'updating') has the max prob in order to be the next word. The prob is: 8.09605171525618e-06
864700 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0157])
The negative loglikelihood (-log p(updating)): 11.72413444519043 or 11.72413444519043 or 11.72413444519043
('kidnapped', 'syrian', 'bishops', 'released', 'church', 'official')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (107, 'official') has the max prob in order to be the next word. The prob is: 0.0003627568657975644
864800 The nn predicts (tensor([98]), 'arrested') the next word with max prob: tensor([0.0098])
The negative loglikelihood (-log p(official)): 7.921777725219727 or 7.921777725219727 or 7.921777725219727
('urges', 'prime', 'minister', 'make', 'parallels', 'abkhazia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8328, 'abkhazia') has the max prob in order to be the next word. The prob is: 5.688508281309623e-06
864900 The nn predicts (tensor([6]), 'russia') the next word with max prob: tensor([0.0220])
The negative loglikelihood (-log p(abkhazia)): 12.077062606811523 or 12.077062606811523 or 12.077062606811523
('package', 'found', 'edmonton', 'mall', 'bus', 'terminal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4648, 'terminal') has the max prob in order to be the next word. The prob is: 1.6572743334108964e-05
865000 The nn predicts (tensor([131]), 'bomb') the next word with max prob: tensor([0.0246])
The negative loglikelihood (-log p(terminal)): 11.00775146484375 or 11.00775146484375 or 11.00775146484375
('syrian', 'man', 'led', 'captors', 'humiliating', 'parade')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2166, 'parade') has the max prob in order to be the next word. The prob is: 7.534754695370793e-05
865100 The nn predicts (tensor([1168]), 'bombings') the next word with max prob: tensor([0.0181])
The negative loglikelihood (-log p(parade)): 9.493399620056152 or 9.493399620056152 or 9.493399620056152
('korea', 'halts', 'korea', 'humanitarian', 'aid', 'amid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (155, 'amid') has the max prob in order to be the next word. The prob is: 0.0018224878003820777
865200 The nn predicts (tensor([157]), 'aid') the next word with max prob: tensor([0.0428])
The negative loglikelihood (-log p(amid)): 6.307552814483643 or 6.307552814483643 or 6.307552814483643
('two', 'tribal', 'teenage', 'girls', 'die', 'within')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (563, 'within') has the max prob in order to be the next word. The prob is: 8.18561966298148e-05
865300 The nn predicts (tensor([328]), 'injured') the next word with max prob: tensor([0.0420])
The negative loglikelihood (-log p(within)): 9.41054630279541 or 9.41054630279541 or 9.41054630279541
('backlash', 'among', 'syrian', 'opposition', 'groups', 'rebels')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (86, 'rebels') has the max prob in order to be the next word. The prob is: 0.0025132757145911455
865400 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0302])
The negative loglikelihood (-log p(rebels)): 5.986168384552002 or 5.986168384552002 or 5.986168384552002
('banned', 'misleading', 'images', 'watchdog', 'also', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.004795328248292208
865500 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0286])
The negative loglikelihood (-log p(says)): 5.340113162994385 or 5.340113162994385 or 5.340113162994385
('grade', 'replica', 'gun', 'sentenced', 'months', 'jail')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (397, 'jail') has the max prob in order to be the next word. The prob is: 0.11618079990148544
865600 The nn predicts (tensor([397]), 'jail') the next word with max prob: tensor([0.1162])
The negative loglikelihood (-log p(jail)): 2.1526076793670654 or 2.1526076793670654 or 2.1526076793670654
('wave', 'iraq', 'violence', 'clashes', 'security', 'forces')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (54, 'forces') has the max prob in order to be the next word. The prob is: 0.22435468435287476
865700 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.2244])
The negative loglikelihood (-log p(forces)): 1.4945271015167236 or 1.4945271015167236 or 1.4945271015167236
('mining', 'logging', 'companies', 'leaving', 'chile', 'without')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (259, 'without') has the max prob in order to be the next word. The prob is: 0.0016073884908109903
865800 The nn predicts (tensor([66]), 'oil') the next word with max prob: tensor([0.0180])
The negative loglikelihood (-log p(without)): 6.433144569396973 or 6.433144569396973 or 6.433144569396973
('box', 'store', 'theft', 'culture', 'hurts', 'artists')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4996, 'artists') has the max prob in order to be the next word. The prob is: 3.42177772836294e-05
865900 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0142])
The negative loglikelihood (-log p(artists)): 10.28276538848877 or 10.28276538848877 or 10.28276538848877
('kuwait', 'top', 'list', 'highest', 'average', 'body')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (546, 'body') has the max prob in order to be the next word. The prob is: 6.024547474225983e-05
866000 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0234])
The negative loglikelihood (-log p(body)): 9.717082977294922 or 9.717082977294922 or 9.717082977294922
('transmits', 'easily', 'humans', 'another', 'strain', 'killed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13, 'killed') has the max prob in order to be the next word. The prob is: 0.017174893990159035
866100 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0554])
The negative loglikelihood (-log p(killed)): 4.064306735992432 or 4.064306735992432 or 4.064306735992432
('search', 'email', 'accounts', 'suspicious', 'travellers', 'despite')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (243, 'despite') has the max prob in order to be the next word. The prob is: 0.0006143520586192608
866200 The nn predicts (tensor([20]), 'uk') the next word with max prob: tensor([0.0244])
The negative loglikelihood (-log p(despite)): 7.394942283630371 or 7.394942283630371 or 7.394942283630371
('day', 'hours', 'day', 'employers', 'confiscate', 'workers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (209, 'workers') has the max prob in order to be the next word. The prob is: 0.004752147942781448
866300 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0262])
The negative loglikelihood (-log p(workers)): 5.349158763885498 or 5.349158763885498 or 5.349158763885498
('artist', 'confronts', 'british', 'prime', 'minister', 'david')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (498, 'david') has the max prob in order to be the next word. The prob is: 0.0023270295932888985
866400 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0273])
The negative loglikelihood (-log p(david)): 6.063162803649902 or 6.063162803649902 or 6.063162803649902
('monkey', 'cull', 'spurs', 'concerns', 'questions', 'raised')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1889, 'raised') has the max prob in order to be the next word. The prob is: 0.0006262577953748405
866500 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0311])
The negative loglikelihood (-log p(raised)): 7.375748634338379 or 7.375748634338379 or 7.375748634338379
('contracted', 'strain', 'bird', 'flu', 'travelling', 'china')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1, 'china') has the max prob in order to be the next word. The prob is: 0.1430852860212326
866600 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.1431])
The negative loglikelihood (-log p(china)): 1.9443144798278809 or 1.9443144798278809 or 1.9443144798278809
('education', 'system', 'including', 'buying', 'selling', 'teaching')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4588, 'teaching') has the max prob in order to be the next word. The prob is: 5.986854375805706e-05
866700 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0246])
The negative loglikelihood (-log p(teaching)): 9.723359107971191 or 9.723359107971191 or 9.723359107971191
('factory', 'least', 'people', 'killed', 'dhaka', 'building')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (410, 'building') has the max prob in order to be the next word. The prob is: 0.004413714166730642
866800 The nn predicts (tensor([79]), 'officials') the next word with max prob: tensor([0.0262])
The negative loglikelihood (-log p(building)): 5.423038959503174 or 5.423038959503174 or 5.423038959503174
('indictment', 'home', 'pelted', 'dead', 'chickens', 'bricks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13858, 'bricks') has the max prob in order to be the next word. The prob is: 5.04503213960561e-06
866900 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0091])
The negative loglikelihood (-log p(bricks)): 12.19710636138916 or 12.19710636138916 or 12.19710636138916
('raising', 'fundamental', 'questions', 'democratic', 'legitimacy', 'three')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (97, 'three') has the max prob in order to be the next word. The prob is: 0.00021130539244040847
867000 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0134])
The negative loglikelihood (-log p(three)): 8.46220588684082 or 8.46220588684082 or 8.46220588684082
('comet', 'impact', 'still', 'hanging', 'around', 'jupiter')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8144, 'jupiter') has the max prob in order to be the next word. The prob is: 2.1728849333157996e-06
867100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0753])
The negative loglikelihood (-log p(jupiter)): 13.039454460144043 or 13.039454460144043 or 13.039454460144043
('scour', 'ruins', 'bangladesh', 'building', 'collapse', 'kills')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (108, 'kills') has the max prob in order to be the next word. The prob is: 0.002279396168887615
867200 The nn predicts (tensor([514]), 'bangladesh') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(kills)): 6.0838446617126465 or 6.0838446617126465 or 6.0838446617126465
('appeals', 'court', 'convicted', 'gunman', 'spate', 'shootings')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3606, 'shootings') has the max prob in order to be the next word. The prob is: 0.00015502897440455854
867300 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0310])
The negative loglikelihood (-log p(shootings)): 8.77189826965332 or 8.77189826965332 or 8.77189826965332
('committed', 'serbs', 'yugoslavia', 'refused', 'call', 'killing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (150, 'killing') has the max prob in order to be the next word. The prob is: 0.0005706938682124019
867400 The nn predicts (tensor([137]), 'end') the next word with max prob: tensor([0.0142])
The negative loglikelihood (-log p(killing)): 7.468657493591309 or 7.468657493591309 or 7.468657493591309
('<s>', 'pkk', 'sets', 'date', 'withdrawal', 'turkey')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (49, 'turkey') has the max prob in order to be the next word. The prob is: 0.0054070851765573025
867500 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0491])
The negative loglikelihood (-log p(turkey)): 5.22004508972168 or 5.22004508972168 or 5.22004508972168
('access', 'app', 'store', 'iran', 'due', 'sanctions')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (200, 'sanctions') has the max prob in order to be the next word. The prob is: 0.0002673021226655692
867600 The nn predicts (tensor([32]), 'nuclear') the next word with max prob: tensor([0.0123])
The negative loglikelihood (-log p(sanctions)): 8.227130889892578 or 8.227130889892578 or 8.227130889892578
('<s>', 'bangladesh', 'collapse', 'factories', 'ignored', 'evacuation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2478, 'evacuation') has the max prob in order to be the next word. The prob is: 0.00011983574950136244
867700 The nn predicts (tensor([714]), 'collapse') the next word with max prob: tensor([0.0301])
The negative loglikelihood (-log p(evacuation)): 9.029388427734375 or 9.029388427734375 or 9.029388427734375
('allegedly', 'raped', 'woman', 'tent', 'outside', 'st')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4300, 'st') has the max prob in order to be the next word. The prob is: 7.342096796492115e-05
867800 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0305])
The negative loglikelihood (-log p(st)): 9.519301414489746 or 9.519301414489746 or 9.519301414489746
('banks', 'bought', 'record', 'amount', 'gold', 'collapse')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (714, 'collapse') has the max prob in order to be the next word. The prob is: 0.002665648702532053
867900 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.0157])
The negative loglikelihood (-log p(collapse)): 5.927308082580566 or 5.927307605743408 or 5.927307605743408
('look', 'buddhists', 'wreck', 'muslim', 'shop', 'video')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (100, 'video') has the max prob in order to be the next word. The prob is: 0.0010759177384898067
868000 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0115])
The negative loglikelihood (-log p(video)): 6.83458137512207 or 6.83458137512207 or 6.83458137512207
('factory', 'building', 'collapse', 'country', 'worst', 'ever')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (509, 'ever') has the max prob in order to be the next word. The prob is: 0.002225880278274417
868100 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.0678])
The negative loglikelihood (-log p(ever)): 6.107602596282959 or 6.107602596282959 or 6.107602596282959
('make', 'millionaire', 'oxford', 'comes', 'top', 'manchester')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4036, 'manchester') has the max prob in order to be the next word. The prob is: 1.156614780484233e-05
868200 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0144])
The negative loglikelihood (-log p(manchester)): 11.367427825927734 or 11.367427825927734 or 11.367427825927734
('bird', 'flu', 'outbreak', 'good', 'signs', 'bad')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (980, 'bad') has the max prob in order to be the next word. The prob is: 0.0008564646705053747
868300 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0536])
The negative loglikelihood (-log p(bad)): 7.062697410583496 or 7.062697410583496 or 7.062697410583496
('hours', 'ocean', 'us', 'vacationers', 'lucia', 'survive')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2888, 'survive') has the max prob in order to be the next word. The prob is: 3.2943520636763424e-05
868400 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.1171])
The negative loglikelihood (-log p(survive)): 10.32071590423584 or 10.32071590423584 or 10.32071590423584
('disgusting', 'bad', 'teeth', 'bad', 'bodies', 'prada')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (21953, 'prada') has the max prob in order to be the next word. The prob is: 2.1919133814662928e-06
868500 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0249])
The negative loglikelihood (-log p(prada)): 13.030735969543457 or 13.030735969543457 or 13.030735969543457
('cocaine', 'following', 'poisonous', 'milk', 'scandal', 'rocked')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3471, 'rocked') has the max prob in order to be the next word. The prob is: 7.996681961230934e-05
868600 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0144])
The negative loglikelihood (-log p(rocked)): 9.43389892578125 or 9.43389892578125 or 9.43389892578125
('russia', 'pussy', 'riot', 'activist', 'pleas', 'parole')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6395, 'parole') has the max prob in order to be the next word. The prob is: 1.4443933650909457e-05
868700 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0274])
The negative loglikelihood (-log p(parole)): 11.145236015319824 or 11.145236015319824 or 11.145236015319824
('formal', 'talks', 'resolve', 'standoff', 'led', 'suspension')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3952, 'suspension') has the max prob in order to be the next word. The prob is: 5.2188741392455995e-05
868800 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0275])
The negative loglikelihood (-log p(suspension)): 9.86064338684082 or 9.86064338684082 or 9.86064338684082
('chief', 'inspector', 'stephan', 'derrick', 'television', 'friday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (300, 'friday') has the max prob in order to be the next word. The prob is: 0.0015635074814781547
868900 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0482])
The negative loglikelihood (-log p(friday)): 6.4608235359191895 or 6.4608235359191895 or 6.4608235359191895
('chile', 'day', 'old', 'baby', 'girl', 'sacrificed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9754, 'sacrificed') has the max prob in order to be the next word. The prob is: 5.916188911214704e-06
869000 The nn predicts (tensor([1461]), 'raped') the next word with max prob: tensor([0.0184])
The negative loglikelihood (-log p(sacrificed)): 12.03781795501709 or 12.03781795501709 or 12.03781795501709
('police', 'four', 'members', 'group', 'arrested', 'chilean')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2662, 'chilean') has the max prob in order to be the next word. The prob is: 4.0959090256365016e-05
869100 The nn predicts (tensor([36]), 'israeli') the next word with max prob: tensor([0.0135])
The negative loglikelihood (-log p(chilean)): 10.102936744689941 or 10.102936744689941 or 10.102936744689941
('persuade', 'world', 'iran', 'cross', 'red', 'line')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (935, 'line') has the max prob in order to be the next word. The prob is: 0.0048324777744710445
869200 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0086])
The negative loglikelihood (-log p(line)): 5.332396030426025 or 5.332396030426025 or 5.332396030426025
('union', 'safety', 'plan', 'bangladesh', 'death', 'toll')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (438, 'toll') has the max prob in order to be the next word. The prob is: 0.009583929553627968
869300 The nn predicts (tensor([209]), 'workers') the next word with max prob: tensor([0.0105])
The negative loglikelihood (-log p(toll)): 4.647667407989502 or 4.647667407989502 or 4.647667407989502
('<s>', '<s>', 'china', 'jails', 'lending', 'crackdown')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (538, 'crackdown') has the max prob in order to be the next word. The prob is: 0.0002631611132528633
869400 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0192])
The negative loglikelihood (-log p(crackdown)): 8.242744445800781 or 8.242744445800781 or 8.242744445800781
('like', 'piece', 'meat', 'judge', 'fury', 'jails')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1808, 'jails') has the max prob in order to be the next word. The prob is: 5.3501913498621434e-05
869500 The nn predicts (tensor([20]), 'uk') the next word with max prob: tensor([0.0187])
The negative loglikelihood (-log p(jails)): 9.835793495178223 or 9.835793495178223 or 9.835793495178223
('poachers', 'diversify', 'hunt', 'total', 'ban', 'lead')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (665, 'lead') has the max prob in order to be the next word. The prob is: 7.784154149703681e-05
869600 The nn predicts (tensor([20]), 'uk') the next word with max prob: tensor([0.0210])
The negative loglikelihood (-log p(lead)): 9.460835456848145 or 9.460835456848145 or 9.460835456848145
('game', 'changer', 'must', 'considered', 'country', 'imports')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2095, 'imports') has the max prob in order to be the next word. The prob is: 4.3054151319665834e-05
869700 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0099])
The negative loglikelihood (-log p(imports)): 10.053051948547363 or 10.053051948547363 or 10.053051948547363
('shows', 'rush', 'act', 'syria', 'chemical', 'arms')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (314, 'arms') has the max prob in order to be the next word. The prob is: 0.006277414970099926
869800 The nn predicts (tensor([149]), 'weapons') the next word with max prob: tensor([0.6455])
The negative loglikelihood (-log p(arms)): 5.070796966552734 or 5.070796966552734 or 5.070796966552734
('arriving', 'friday', 'tunisia', 'ghriba', 'synagogue', 'oldest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1593, 'oldest') has the max prob in order to be the next word. The prob is: 6.0560847487067804e-05
869900 The nn predicts (tensor([300]), 'friday') the next word with max prob: tensor([0.0219])
The negative loglikelihood (-log p(oldest)): 9.711861610412598 or 9.711861610412598 or 9.711861610412598
('signals', 'taiwan', 'national', 'security', 'bureau', 'report')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (51, 'report') has the max prob in order to be the next word. The prob is: 0.001492341049015522
870000 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.1019])
The negative loglikelihood (-log p(report)): 6.50740909576416 or 6.50740909576416 or 6.50740909576416
('pussy', 'riot', 'whose', 'provocative', 'songs', 'prosecution')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2785, 'prosecution') has the max prob in order to be the next word. The prob is: 0.0001530525041744113
870100 The nn predicts (tensor([720]), 'islam') the next word with max prob: tensor([0.0101])
The negative loglikelihood (-log p(prosecution)): 8.784729957580566 or 8.784729957580566 or 8.784729957580566
('coma', 'family', 'gets', 'visa', 'visit', 'pakistan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (45, 'pakistan') has the max prob in order to be the next word. The prob is: 0.0017395159229636192
870200 The nn predicts (tensor([20]), 'uk') the next word with max prob: tensor([0.0161])
The negative loglikelihood (-log p(pakistan)): 6.3541483879089355 or 6.3541483879089355 or 6.3541483879089355
('iran', 'syria', 'within', 'two', 'months', 'say')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (53, 'say') has the max prob in order to be the next word. The prob is: 0.0001701273286016658
870300 The nn predicts (tensor([153]), 'since') the next word with max prob: tensor([0.0491])
The negative loglikelihood (-log p(say)): 8.678963661193848 or 8.678963661193848 or 8.678963661193848
('military', 'coalition', 'killed', 'crash', 'says', 'nato')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (192, 'nato') has the max prob in order to be the next word. The prob is: 0.004232359584420919
870400 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.0380])
The negative loglikelihood (-log p(nato)): 5.464995861053467 or 5.464995384216309 or 5.464995384216309
('assets', 'without', 'compensation', 'unfounded', 'according', 'kalaa')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (62590, 'kalaa') has the max prob in order to be the next word. The prob is: 6.912076173648529e-07
870500 The nn predicts (tensor([385]), 'thursday') the next word with max prob: tensor([0.0159])
The negative loglikelihood (-log p(kalaa)): 14.184825897216797 or 14.184825897216797 or 14.184825897216797
('russia', 'caught', 'bomb', 'suspect', 'wiretap', 'two')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (31, 'two') has the max prob in order to be the next word. The prob is: 0.0019529114942997694
870600 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0339])
The negative loglikelihood (-log p(two)): 6.238433837890625 or 6.238433837890625 or 6.238433837890625
('land', 'pharaohs', 'bury', 'dead', 'archaeologists', 'fear')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (602, 'fear') has the max prob in order to be the next word. The prob is: 0.00020330045663286
870700 The nn predicts (tensor([111]), 'fire') the next word with max prob: tensor([0.0152])
The negative loglikelihood (-log p(fear)): 8.500825881958008 or 8.500825881958008 or 8.500825881958008
('appalled', 'sri', 'lanka', 'hosting', 'government', 'summit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (630, 'summit') has the max prob in order to be the next word. The prob is: 0.000280772743280977
870800 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.0155])
The negative loglikelihood (-log p(summit)): 8.17796516418457 or 8.17796516418457 or 8.17796516418457
('nexus', 'politics', 'business', 'run', 'political', 'allies')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1038, 'allies') has the max prob in order to be the next word. The prob is: 0.00017860830121207982
870900 The nn predicts (tensor([118]), 'party') the next word with max prob: tensor([0.1363])
The negative loglikelihood (-log p(allies)): 8.630315780639648 or 8.630315780639648 or 8.630315780639648
('dhaka', 'killing', 'hundreds', 'people', 'arrested', 'government')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11, 'government') has the max prob in order to be the next word. The prob is: 0.0022107972763478756
871000 The nn predicts (tensor([84]), 'least') the next word with max prob: tensor([0.0526])
The negative loglikelihood (-log p(government)): 6.1144022941589355 or 6.1144022941589355 or 6.1144022941589355
('iceland', 'elects', 'parties', 'responsible', 'economic', 'collapse')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (714, 'collapse') has the max prob in order to be the next word. The prob is: 0.015772061422467232
871100 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.1661])
The negative loglikelihood (-log p(collapse)): 4.149515151977539 or 4.149515151977539 or 4.149515151977539
('<s>', '<s>', '<s>', 'jános', 'starker', 'died')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (425, 'died') has the max prob in order to be the next word. The prob is: 0.0003633081214502454
871200 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0253])
The negative loglikelihood (-log p(died)): 7.920259475708008 or 7.920259475708008 or 7.920259475708008
('neighbour', 'took', 'revenge', 'booming', 'iron', 'maiden')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9259, 'maiden') has the max prob in order to be the next word. The prob is: 3.5946093703387305e-05
871300 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0149])
The negative loglikelihood (-log p(maiden)): 10.233489990234375 or 10.233489990234375 or 10.233489990234375
('layoffs', 'taxes', 'part', 'bailout', 'deal', 'eu')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (43, 'eu') has the max prob in order to be the next word. The prob is: 0.00727187842130661
871400 The nn predicts (tensor([113]), 'european') the next word with max prob: tensor([0.0097])
The negative loglikelihood (-log p(eu)): 4.923740863800049 or 4.923740863800049 or 4.923740863800049
('slapped', 'leered', 'pawed', 'tied', 'raped', 'depiction')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13149, 'depiction') has the max prob in order to be the next word. The prob is: 5.77310402150033e-06
871500 The nn predicts (tensor([408]), 'girl') the next word with max prob: tensor([0.0140])
The negative loglikelihood (-log p(depiction)): 12.062300682067871 or 12.062300682067871 or 12.062300682067871
('rejected', 'dhaka', 'factory', 'collapse', 'feared', 'accepting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5735, 'accepting') has the max prob in order to be the next word. The prob is: 8.827302735880949e-06
871600 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.3213])
The negative loglikelihood (-log p(accepting)): 11.63766098022461 or 11.63766098022461 or 11.63766098022461
('lanka', 'appalling', 'record', 'authoritarianism', 'failure', 'tackle')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1886, 'tackle') has the max prob in order to be the next word. The prob is: 0.0005927435122430325
871700 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(tackle)): 7.43074893951416 or 7.43074893951416 or 7.43074893951416
('azam', 'tower', 'peshawar', 'killed', 'five', 'injured')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (328, 'injured') has the max prob in order to be the next word. The prob is: 0.010957262478768826
871800 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.2386])
The negative loglikelihood (-log p(injured)): 4.5137529373168945 or 4.5137529373168945 or 4.5137529373168945
('venezuela', 'maduro', 'pledges', 'continued', 'alliance', 'cuba')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (570, 'cuba') has the max prob in order to be the next word. The prob is: 0.0003798996622208506
871900 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0162])
The negative loglikelihood (-log p(cuba)): 7.875603199005127 or 7.875603199005127 or 7.875603199005127
('taxi', 'rates', 'par', 'us', 'everything', 'else')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2964, 'else') has the max prob in order to be the next word. The prob is: 0.0009837585967034101
872000 The nn predicts (tensor([216]), 'still') the next word with max prob: tensor([0.0133])
The negative loglikelihood (-log p(else)): 6.924129962921143 or 6.924129962921143 or 6.924129962921143
('organs', 'sparks', 'protests', 'australian', 'university', 'trained')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3463, 'trained') has the max prob in order to be the next word. The prob is: 5.967042670818046e-05
872100 The nn predicts (tensor([209]), 'workers') the next word with max prob: tensor([0.0183])
The negative loglikelihood (-log p(trained)): 9.72667407989502 or 9.72667407989502 or 9.72667407989502
('collapsed', 'building', 'hinders', 'search', 'hundreds', 'workers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (209, 'workers') has the max prob in order to be the next word. The prob is: 0.03342708572745323
872200 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0956])
The negative loglikelihood (-log p(workers)): 3.3983888626098633 or 3.3983888626098633 or 3.3983888626098633
('legal', 'challenge', 'fight', 'win', 'right', 'take')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (148, 'take') has the max prob in order to be the next word. The prob is: 0.0023545462172478437
872300 The nn predicts (tensor([175]), 'vote') the next word with max prob: tensor([0.0089])
The negative loglikelihood (-log p(take)): 6.051407337188721 or 6.051407337188721 or 6.051407337188721
('include', 'faked', 'resumes', 'visa', 'violations', 'discrimination')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2865, 'discrimination') has the max prob in order to be the next word. The prob is: 0.0003730767930392176
872400 The nn predicts (tensor([90]), 'human') the next word with max prob: tensor([0.0325])
The negative loglikelihood (-log p(discrimination)): 7.893726348876953 or 7.893726348876953 or 7.893726348876953
('terrifying', 'scene', 'two', 'famous', 'european', 'climbers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5309, 'climbers') has the max prob in order to be the next word. The prob is: 3.8016689813957782e-06
872500 The nn predicts (tensor([320]), 'union') the next word with max prob: tensor([0.0566])
The negative loglikelihood (-log p(climbers)): 12.480070114135742 or 12.480070114135742 or 12.480070114135742
('pesticides', 'linked', 'bee', 'decline', 'members', 'vote')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (175, 'vote') has the max prob in order to be the next word. The prob is: 9.701851377030835e-05
872600 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0302])
The negative loglikelihood (-log p(vote)): 9.240609169006348 or 9.240609169006348 or 9.240609169006348
('police', 'officer', 'dies', 'attempting', 'metre', 'high')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (235, 'high') has the max prob in order to be the next word. The prob is: 0.0007698885747231543
872700 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0154])
The negative loglikelihood (-log p(high)): 7.169264793395996 or 7.169264793395996 or 7.169264793395996
('air', 'cargo', 'crashes', 'bagram', 'afghanistan', 'shortly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4528, 'shortly') has the max prob in order to be the next word. The prob is: 7.497488695662469e-05
872800 The nn predicts (tensor([122]), 'afghanistan') the next word with max prob: tensor([0.0961])
The negative loglikelihood (-log p(shortly)): 9.498357772827148 or 9.498357772827148 or 9.498357772827148
('losing', 'millions', 'users', 'us', 'mature', 'markets')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1281, 'markets') has the max prob in order to be the next word. The prob is: 0.001677821041084826
872900 The nn predicts (tensor([168]), 'countries') the next word with max prob: tensor([0.0087])
The negative loglikelihood (-log p(markets)): 6.390259265899658 or 6.390259265899658 or 6.390259265899658
('also', 'supreme', 'bmw', 'dealer', 'saeed', 'ghasseminejad')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (62636, 'ghasseminejad') has the max prob in order to be the next word. The prob is: 3.1123072403715923e-06
873000 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0123])
The negative loglikelihood (-log p(ghasseminejad)): 12.680146217346191 or 12.680146217346191 or 12.680146217346191
('report', 'published', 'week', 'doubles', 'previous', 'death')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (42, 'death') has the max prob in order to be the next word. The prob is: 0.0004985256819054484
873100 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.0559])
The negative loglikelihood (-log p(death)): 7.603855609893799 or 7.603855609893799 or 7.603855609893799
('liquid', 'helium', 'cool', 'instruments', 'ending', 'mission')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (555, 'mission') has the max prob in order to be the next word. The prob is: 9.463727474212646e-05
873200 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0258])
The negative loglikelihood (-log p(mission)): 9.265459060668945 or 9.265459060668945 or 9.265459060668945
('good', 'life', 'goes', 'syrian', 'elite', 'sit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5201, 'sit') has the max prob in order to be the next word. The prob is: 6.898478750372306e-05
873300 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.0227])
The negative loglikelihood (-log p(sit)): 9.581624984741211 or 9.581624984741211 or 9.581624984741211
('prepared', 'enforce', 'islamic', 'law', 'aceh', 'requires')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4845, 'requires') has the max prob in order to be the next word. The prob is: 2.109523666149471e-05
873400 The nn predicts (tensor([75]), 'women') the next word with max prob: tensor([0.0330])
The negative loglikelihood (-log p(requires)): 10.766463279724121 or 10.766463279724121 or 10.766463279724121
('southwest', 'pakistan', 'latest', 'attack', 'ahead', 'election')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (158, 'election') has the max prob in order to be the next word. The prob is: 0.01575917936861515
873500 The nn predicts (tensor([366]), 'elections') the next word with max prob: tensor([0.0242])
The negative loglikelihood (-log p(election)): 4.150332450866699 or 4.150332450866699 or 4.150332450866699
('flags', 'inscribed', 'jihadi', 'slogans', 'extremist', 'religious')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (658, 'religious') has the max prob in order to be the next word. The prob is: 0.0034099696204066277
873600 The nn predicts (tensor([62]), 'group') the next word with max prob: tensor([0.0943])
The negative loglikelihood (-log p(religious)): 5.681051731109619 or 5.681051731109619 or 5.681051731109619
('victim', 'gun', 'opens', 'fire', 'border', 'guard')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1059, 'guard') has the max prob in order to be the next word. The prob is: 0.0004645551089197397
873700 The nn predicts (tensor([809]), 'damascus') the next word with max prob: tensor([0.0301])
The negative loglikelihood (-log p(guard)): 7.6744303703308105 or 7.6744303703308105 or 7.6744303703308105
('fear', 'planes', 'shoot', 'communities', 'grieve', 'lost')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (584, 'lost') has the max prob in order to be the next word. The prob is: 9.502707689534873e-05
873800 The nn predicts (tensor([141]), 'violence') the next word with max prob: tensor([0.0110])
The negative loglikelihood (-log p(lost)): 9.261348724365234 or 9.261348724365234 or 9.261348724365234
('doctor', 'accused', 'running', 'one', 'world', 'largest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (276, 'largest') has the max prob in order to be the next word. The prob is: 0.0425112321972847
873900 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.2089])
The negative loglikelihood (-log p(largest)): 3.157986879348755 or 3.157986879348755 or 3.157986879348755
('economies', 'region', 'germany', 'italy', 'reporting', 'little')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1017, 'little') has the max prob in order to be the next word. The prob is: 0.00013138068607077003
874000 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0110])
The negative loglikelihood (-log p(little)): 8.937411308288574 or 8.937411308288574 or 8.937411308288574
('obama', 'guantanamo', 'bay', 'must', 'close', 'bbc')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (82, 'bbc') has the max prob in order to be the next word. The prob is: 0.000533403770532459
874100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0297])
The negative loglikelihood (-log p(bbc)): 7.536231994628906 or 7.536231994628906 or 7.536231994628906
('day', 'herbie', 'hancock', 'john', 'beasley', 'wayne')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15859, 'wayne') has the max prob in order to be the next word. The prob is: 5.658753707393771e-06
874200 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0365])
The negative loglikelihood (-log p(wayne)): 12.082306861877441 or 12.082306861877441 or 12.082306861877441
('ranks', 'second', 'us', 'money', 'africa', 'afp')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2564, 'afp') has the max prob in order to be the next word. The prob is: 2.9652790544787422e-05
874300 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0266])
The negative loglikelihood (-log p(afp)): 10.425954818725586 or 10.42595386505127 or 10.42595386505127
('interior', 'minister', 'tsvetan', 'tsvetanov', 'allowed', 'employees')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1756, 'employees') has the max prob in order to be the next word. The prob is: 0.0007818291778676212
874400 The nn predicts (tensor([294]), 'pay') the next word with max prob: tensor([0.0442])
The negative loglikelihood (-log p(employees)): 7.153874397277832 or 7.153874397277832 or 7.153874397277832
('rose', 'billion', 'larger', 'incomes', 'national', 'governments')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1364, 'governments') has the max prob in order to be the next word. The prob is: 0.00329502928070724
874500 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0513])
The negative loglikelihood (-log p(governments)): 5.7153401374816895 or 5.7153401374816895 or 5.7153401374816895
('possible', 'hezbollah', 'intervention', 'syria', 'assad', 'side')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1533, 'side') has the max prob in order to be the next word. The prob is: 0.00032570576877333224
874600 The nn predicts (tensor([273]), 'assad') the next word with max prob: tensor([0.1885])
The negative loglikelihood (-log p(side)): 8.029516220092773 or 8.029516220092773 or 8.029516220092773
('region', 'pearl', 'tradition', 'thrived', 'since', 'early')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (803, 'early') has the max prob in order to be the next word. The prob is: 0.0006406098837032914
874700 The nn predicts (tensor([153]), 'since') the next word with max prob: tensor([0.0247])
The negative loglikelihood (-log p(early)): 7.353089809417725 or 7.353089809417725 or 7.353089809417725
('meeting', 'fails', 'china', 'wants', 'india', 'dismantle')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7710, 'dismantle') has the max prob in order to be the next word. The prob is: 6.258241228351835e-06
874800 The nn predicts (tensor([32]), 'nuclear') the next word with max prob: tensor([0.0242])
The negative loglikelihood (-log p(dismantle)): 11.981611251831055 or 11.981611251831055 or 11.981611251831055
('warning', 'risks', 'minorities', 'reached', 'crisis', 'level')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1129, 'level') has the max prob in order to be the next word. The prob is: 0.00036735396133735776
874900 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0276])
The negative loglikelihood (-log p(level)): 7.90918493270874 or 7.90918493270874 or 7.90918493270874
('uk', 'world', 'power', 'irreversible', 'decline', 'scotland')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (987, 'scotland') has the max prob in order to be the next word. The prob is: 6.131828558864072e-05
875000 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0243])
The negative loglikelihood (-log p(scotland)): 9.699432373046875 or 9.699432373046875 or 9.699432373046875
('gdp', 'cut', 'mass', 'unemployment', 'fueling', 'unrest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (940, 'unrest') has the max prob in order to be the next word. The prob is: 0.0030927688349038363
875100 The nn predicts (tensor([95]), 'global') the next word with max prob: tensor([0.0166])
The negative loglikelihood (-log p(unrest)): 5.778688430786133 or 5.778688430786133 or 5.778688430786133
('woman', 'given', 'death', 'penalty', 'accuses', 'uk')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (20, 'uk') has the max prob in order to be the next word. The prob is: 0.003245298285037279
875200 The nn predicts (tensor([331]), 'crimes') the next word with max prob: tensor([0.0196])
The negative loglikelihood (-log p(uk)): 5.730547904968262 or 5.730547904968262 or 5.730547904968262
('warns', 'get', 'involved', 'syria', 'foreign', 'threats')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (817, 'threats') has the max prob in order to be the next word. The prob is: 0.000616149278357625
875300 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.2027])
The negative loglikelihood (-log p(threats)): 7.392021179199219 or 7.392021179199219 or 7.392021179199219
('reform', 'attempt', 'make', 'country', 'telecommunications', 'industry')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (695, 'industry') has the max prob in order to be the next word. The prob is: 0.006976368837058544
875400 The nn predicts (tensor([368]), 'policy') the next word with max prob: tensor([0.0157])
The negative loglikelihood (-log p(industry)): 4.965226650238037 or 4.965226650238037 or 4.965226650238037
('death', 'toll', 'hits', 'calls', 'grow', 'grant')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3500, 'grant') has the max prob in order to be the next word. The prob is: 9.810785741137806e-06
875500 The nn predicts (tensor([714]), 'collapse') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(grant)): 11.532028198242188 or 11.532028198242188 or 11.532028198242188
('state', 'se', 'asian', 'leaders', 'becoming', 'increasingly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1742, 'increasingly') has the max prob in order to be the next word. The prob is: 0.001851305365562439
875600 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0380])
The negative loglikelihood (-log p(increasingly)): 6.291864395141602 or 6.291864395141602 or 6.291864395141602
('testing', 'blood', 'samples', 'taken', 'syrian', 'casualties')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2029, 'casualties') has the max prob in order to be the next word. The prob is: 0.00015189162513706833
875700 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0300])
The negative loglikelihood (-log p(casualties)): 8.792343139648438 or 8.792343139648438 or 8.792343139648438
('former', 'us', 'president', 'jimmy', 'carter', 'seeking')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1132, 'seeking') has the max prob in order to be the next word. The prob is: 0.0008898225496523082
875800 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0483])
The negative loglikelihood (-log p(seeking)): 7.02448844909668 or 7.02448844909668 or 7.02448844909668
('commission', 'brazil', 'puts', 'project', 'gay', 'cure')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3631, 'cure') has the max prob in order to be the next word. The prob is: 0.00011592502414714545
875900 The nn predicts (tensor([606]), 'marriage') the next word with max prob: tensor([0.1522])
The negative loglikelihood (-log p(cure)): 9.062566757202148 or 9.062566757202148 or 9.062566757202148
('pollinators', 'represents', 'world', 'food', 'supply', 'highlighted')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10893, 'highlighted') has the max prob in order to be the next word. The prob is: 1.8096960047842003e-05
876000 The nn predicts (tensor([180]), 'gas') the next word with max prob: tensor([0.0205])
The negative loglikelihood (-log p(highlighted)): 10.919766426086426 or 10.919766426086426 or 10.919766426086426
('events', 'syria', 'poll', 'found', 'percent', 'neither')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5575, 'neither') has the max prob in order to be the next word. The prob is: 3.713500336743891e-05
876100 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0601])
The negative loglikelihood (-log p(neither)): 10.200950622558594 or 10.200950622558594 or 10.200950622558594
('group', 'offers', 'million', 'sajjan', 'kumar', 'conviction')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3019, 'conviction') has the max prob in order to be the next word. The prob is: 0.00015222975343931466
876200 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0419])
The negative loglikelihood (-log p(conviction)): 8.790120124816895 or 8.790119171142578 or 8.790119171142578
('border', 'gate', 'deadly', 'gunfire', 'exchange', 'pakistan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (45, 'pakistan') has the max prob in order to be the next word. The prob is: 0.002139497082680464
876300 The nn predicts (tensor([103]), 'border') the next word with max prob: tensor([0.0318])
The negative loglikelihood (-log p(pakistan)): 6.147184371948242 or 6.147184371948242 or 6.147184371948242
('firing', 'squad', 'part', 'stringent', 'new', 'measures')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1026, 'measures') has the max prob in order to be the next word. The prob is: 0.001292262226343155
876400 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0681])
The negative loglikelihood (-log p(measures)): 6.651360988616943 or 6.651360988616943 or 6.651360988616943
('attacked', 'buddhist', 'sherpas', 'week', 'final', 'straw')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9312, 'straw') has the max prob in order to be the next word. The prob is: 9.655262147134636e-06
876500 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0919])
The negative loglikelihood (-log p(straw)): 11.54800796508789 or 11.54800796508789 or 11.54800796508789
('temple', 'feathered', 'serpent', 'near', 'mexico', 'city')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (71, 'city') has the max prob in order to be the next word. The prob is: 0.08845294266939163
876600 The nn predicts (tensor([103]), 'border') the next word with max prob: tensor([0.0948])
The negative loglikelihood (-log p(city)): 2.4252846240997314 or 2.4252846240997314 or 2.4252846240997314
('eastern', 'nangarhar', 'province', 'according', 'interior', 'ministry')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (477, 'ministry') has the max prob in order to be the next word. The prob is: 0.16460810601711273
876700 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.3798])
The negative loglikelihood (-log p(ministry)): 1.8041877746582031 or 1.8041877746582031 or 1.8041877746582031
('house', 'drone', 'policy', 'says', 'us', 'would')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (104, 'would') has the max prob in order to be the next word. The prob is: 0.0015234367456287146
876800 The nn predicts (tensor([365]), 'drone') the next word with max prob: tensor([0.0221])
The negative loglikelihood (-log p(would)): 6.486786365509033 or 6.486786365509033 or 6.486786365509033
('hits', 'girl', 'attack', 'athens', 'mayor', 'giorgos')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (27409, 'giorgos') has the max prob in order to be the next word. The prob is: 1.3056404668532196e-06
876900 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0429])
The negative loglikelihood (-log p(giorgos)): 13.548816680908203 or 13.548816680908203 or 13.548816680908203
('nine', 'pet', 'cats', 'western', 'sweden', 'arrested')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (98, 'arrested') has the max prob in order to be the next word. The prob is: 0.001438202685676515
877000 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0202])
The negative loglikelihood (-log p(arrested)): 6.544361114501953 or 6.544361114501953 or 6.544361114501953
('civilians', 'including', 'women', 'children', 'feared', 'dead')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (48, 'dead') has the max prob in order to be the next word. The prob is: 0.08738186955451965
877100 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0874])
The negative loglikelihood (-log p(dead)): 2.437467575073242 or 2.437467575073242 or 2.437467575073242
('second', 'biggest', 'front', 'american', 'drone', 'warfare')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3661, 'warfare') has the max prob in order to be the next word. The prob is: 0.00010139514779439196
877200 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.1383])
The negative loglikelihood (-log p(warfare)): 9.19648551940918 or 9.19648551940918 or 9.19648551940918
('releases', 'photos', 'three', 'men', 'benghazi', 'attack')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15, 'attack') has the max prob in order to be the next word. The prob is: 0.08176206797361374
877300 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0818])
The negative loglikelihood (-log p(attack)): 2.503941774368286 or 2.503941774368286 or 2.503941774368286
('attack', 'targets', 'without', 'human', 'input', 'power')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (106, 'power') has the max prob in order to be the next word. The prob is: 0.0010695421369746327
877400 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.0241])
The negative loglikelihood (-log p(power)): 6.840524673461914 or 6.840524673461914 or 6.840524673461914
('buy', 'new', 'attack', 'planes', 'equipped', 'electronic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2791, 'electronic') has the max prob in order to be the next word. The prob is: 9.34014969971031e-05
877500 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0231])
The negative loglikelihood (-log p(electronic)): 9.278603553771973 or 9.278603553771973 or 9.278603553771973
('depsang', 'bulge', 'area', 'indian', 'spy', 'drones')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1118, 'drones') has the max prob in order to be the next word. The prob is: 0.0007123174145817757
877600 The nn predicts (tensor([304]), 'agency') the next word with max prob: tensor([0.0252])
The negative loglikelihood (-log p(drones)): 7.2469868659973145 or 7.2469868659973145 or 7.2469868659973145
('successful', 'somali', 'pirate', 'hijackings', 'nearly', 'year')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (58, 'year') has the max prob in order to be the next word. The prob is: 0.023548604920506477
877700 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0820])
The negative loglikelihood (-log p(year)): 3.7486886978149414 or 3.7486886978149414 or 3.7486886978149414
('abroad', 'might', 'return', 'enhanced', 'capabilities', 'launch')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (367, 'launch') has the max prob in order to be the next word. The prob is: 0.0068415203131735325
877800 The nn predicts (tensor([32]), 'nuclear') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(launch)): 4.984745502471924 or 4.984745502471924 or 4.984745502471924
('caused', 'global', 'warming', 'says', 'world', 'meteorological')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10775, 'meteorological') has the max prob in order to be the next word. The prob is: 5.136203071742784e-06
877900 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0592])
The negative loglikelihood (-log p(meteorological)): 12.17919635772705 or 12.17919635772705 or 12.17919635772705
('interview', 'become', 'latest', 'politician', 'sent', 'death')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (42, 'death') has the max prob in order to be the next word. The prob is: 0.007084313780069351
878000 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0126])
The negative loglikelihood (-log p(death)): 4.9498724937438965 or 4.9498724937438965 or 4.9498724937438965
('golden', 'dawn', 'party', 'food', 'handout', 'greeks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3052, 'greeks') has the max prob in order to be the next word. The prob is: 5.881931429030374e-05
878100 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0124])
The negative loglikelihood (-log p(greeks)): 9.741040229797363 or 9.741040229797363 or 9.741040229797363
('change', 'uk', 'forever', 'white', 'britons', 'minority')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2207, 'minority') has the max prob in order to be the next word. The prob is: 0.00019050795526709408
878200 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0259])
The negative loglikelihood (-log p(minority)): 8.565816879272461 or 8.565816879272461 or 8.565816879272461
('bans', 'passenger', 'flights', 'syria', 'missiles', 'reportedly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (704, 'reportedly') has the max prob in order to be the next word. The prob is: 0.0007706508622504771
878300 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0212])
The negative loglikelihood (-log p(reportedly)): 7.168275356292725 or 7.168275356292725 or 7.168275356292725
('causes', 'desertification', 'turned', 'vast', 'swathes', 'sahel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11778, 'sahel') has the max prob in order to be the next word. The prob is: 7.861513040552381e-06
878400 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0324])
The negative loglikelihood (-log p(sahel)): 11.753531455993652 or 11.753531455993652 or 11.753531455993652
('baghdad', 'meanwhile', 'police', 'gunmen', 'clashed', 'northern')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (258, 'northern') has the max prob in order to be the next word. The prob is: 0.0025426819920539856
878500 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.3012])
The negative loglikelihood (-log p(northern)): 5.974535942077637 or 5.974535942077637 or 5.974535942077637
('somalia', 'donor', 'conference', 'london', 'next', 'week')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (211, 'week') has the max prob in order to be the next word. The prob is: 0.1779867559671402
878600 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.3249])
The negative loglikelihood (-log p(week)): 1.726046085357666 or 1.726046085357666 or 1.726046085357666
('point', 'major', 'shift', 'among', 'nation', 'youth')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1329, 'youth') has the max prob in order to be the next word. The prob is: 0.00026450512814335525
878700 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0449])
The negative loglikelihood (-log p(youth)): 8.237649917602539 or 8.237649917602539 or 8.237649917602539
('<s>', 'china', 'middle', 'east', 'peace', 'broker')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7672, 'broker') has the max prob in order to be the next word. The prob is: 8.353643352165818e-06
878800 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0952])
The negative loglikelihood (-log p(broker)): 11.6928129196167 or 11.6928129196167 or 11.6928129196167
('says', 'assured', 'would', 'continue', 'delivering', 'bags')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4434, 'bags') has the max prob in order to be the next word. The prob is: 4.904316665488295e-05
878900 The nn predicts (tensor([313]), 'chemical') the next word with max prob: tensor([0.0420])
The negative loglikelihood (-log p(bags)): 9.922809600830078 or 9.922809600830078 or 9.922809600830078
('recent', 'un', 'survey', 'suggested', 'astounding', 'egyptian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (233, 'egyptian') has the max prob in order to be the next word. The prob is: 0.00014677208673674613
879000 The nn predicts (tensor([51]), 'report') the next word with max prob: tensor([0.0316])
The negative loglikelihood (-log p(egyptian)): 8.826629638671875 or 8.826629638671875 or 8.826629638671875
('south', 'koreans', 'leave', 'factory', 'north', 'korea')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22, 'korea') has the max prob in order to be the next word. The prob is: 0.41243162751197815
879100 The nn predicts (tensor([22]), 'korea') the next word with max prob: tensor([0.4124])
The negative loglikelihood (-log p(korea)): 0.8856848478317261 or 0.8856848478317261 or 0.8856848478317261
('western', 'drugs', 'north', 'korea', 'leans', 'cures')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13814, 'cures') has the max prob in order to be the next word. The prob is: 3.4022841646219604e-06
879200 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0348])
The negative loglikelihood (-log p(cures)): 12.591063499450684 or 12.591063499450684 or 12.591063499450684
('ability', 'girls', 'get', 'better', 'motor', 'social')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (505, 'social') has the max prob in order to be the next word. The prob is: 0.00030590829555876553
879300 The nn predicts (tensor([151]), 'help') the next word with max prob: tensor([0.0072])
The negative loglikelihood (-log p(social)): 8.092225074768066 or 8.092225074768066 or 8.092225074768066
('way', 'antarctica', 'avoid', 'ecological', 'disaster', 'chinese')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (41, 'chinese') has the max prob in order to be the next word. The prob is: 0.0013913543662056327
879400 The nn predicts (tensor([47]), 'may') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(chinese)): 6.57747745513916 or 6.57747745513916 or 6.57747745513916
('evans', 'arrested', 'police', 'allegedly', 'raping', 'man')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (56, 'man') has the max prob in order to be the next word. The prob is: 0.01112774945795536
879500 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0503])
The negative loglikelihood (-log p(man)): 4.4983134269714355 or 4.4983134269714355 or 4.4983134269714355
('denial', 'jewish', 'land', 'rights', 'sparks', 'ire')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6853, 'ire') has the max prob in order to be the next word. The prob is: 5.254642019281164e-05
879600 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0326])
The negative loglikelihood (-log p(ire)): 9.853813171386719 or 9.853813171386719 or 9.853813171386719
('incremental', 'progress', 'towards', 'freedom', 'women', 'incredibly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12722, 'incredibly') has the max prob in order to be the next word. The prob is: 4.851651283388492e-06
879700 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.1203])
The negative loglikelihood (-log p(incredibly)): 12.236191749572754 or 12.236191749572754 or 12.236191749572754
('hundreds', 'injured', 'bangladesh', 'chanting', 'allahu', 'akbar')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7210, 'akbar') has the max prob in order to be the next word. The prob is: 3.297093644505367e-05
879800 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0326])
The negative loglikelihood (-log p(akbar)): 10.319884300231934 or 10.319884300231934 or 10.319884300231934
('canada', 'addresses', 'current', 'canadian', 'goverment', 'cabinet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1087, 'cabinet') has the max prob in order to be the next word. The prob is: 0.0005149104399606586
879900 The nn predicts (tensor([79]), 'officials') the next word with max prob: tensor([0.0124])
The negative loglikelihood (-log p(cabinet)): 7.571517467498779 or 7.571517467498779 or 7.571517467498779
('parliament', 'questioned', 'police', 'accusations', 'rape', 'sexual')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (801, 'sexual') has the max prob in order to be the next word. The prob is: 0.004656333941966295
880000 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0635])
The negative loglikelihood (-log p(sexual)): 5.3695268630981445 or 5.3695268630981445 or 5.3695268630981445
('caribbean', 'island', 'curacao', 'helmin', 'wiels', 'shot')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (232, 'shot') has the max prob in order to be the next word. The prob is: 0.0010546725243330002
880100 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0226])
The negative loglikelihood (-log p(shot)): 6.854525089263916 or 6.854525089263916 or 6.854525089263916
('country', 'onslaught', 'bombings', 'gave', 'nigeria', 'sad')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5253, 'sad') has the max prob in order to be the next word. The prob is: 8.730499757803045e-06
880200 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0343])
The negative loglikelihood (-log p(sad)): 11.648688316345215 or 11.648688316345215 or 11.648688316345215
('strong', 'suspicions', 'syrian', 'rebels', 'used', 'sarin')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4622, 'sarin') has the max prob in order to be the next word. The prob is: 2.2759179046261124e-05
880300 The nn predicts (tensor([313]), 'chemical') the next word with max prob: tensor([0.2025])
The negative loglikelihood (-log p(sarin)): 10.690542221069336 or 10.690542221069336 or 10.690542221069336
('admits', 'parking', 'row', 'attack', 'mayor', 'admitted')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2528, 'admitted') has the max prob in order to be the next word. The prob is: 0.00024096616834867746
880400 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0144])
The negative loglikelihood (-log p(admitted)): 8.330854415893555 or 8.330854415893555 or 8.330854415893555
('world', 'predators', 'give', 'finger', 'press', 'freedom')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (649, 'freedom') has the max prob in order to be the next word. The prob is: 0.0022517142351716757
880500 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0099])
The negative loglikelihood (-log p(freedom)): 6.096063613891602 or 6.096063613891602 or 6.096063613891602
('says', 'google', 'palestine', 'page', 'harms', 'peace')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (127, 'peace') has the max prob in order to be the next word. The prob is: 0.02359512820839882
880600 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0800])
The negative loglikelihood (-log p(peace)): 3.7467150688171387 or 3.7467150688171387 or 3.7467150688171387
('holocaust', 'survivors', 'writings', 'authors', 'linked', 'regime')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (422, 'regime') has the max prob in order to be the next word. The prob is: 0.00010181726975133643
880700 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0434])
The negative loglikelihood (-log p(regime)): 9.192331314086914 or 9.192330360412598 or 9.192330360412598
('spot', 'moscow', 'bolotnaya', 'swamp', 'square', 'rally')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (523, 'rally') has the max prob in order to be the next word. The prob is: 0.002463740296661854
880800 The nn predicts (tensor([1009]), 'square') the next word with max prob: tensor([0.0182])
The negative loglikelihood (-log p(rally)): 6.00607442855835 or 6.00607442855835 or 6.00607442855835
('rebels', 'used', 'strong', 'concrete', 'suspicions', 'yet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (881, 'yet') has the max prob in order to be the next word. The prob is: 0.0006309272139333189
880900 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0854])
The negative loglikelihood (-log p(yet)): 7.368319988250732 or 7.368319988250732 or 7.368319988250732
('<s>', '<s>', '<s>', 'sarin', 'gas', 'kills')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (108, 'kills') has the max prob in order to be the next word. The prob is: 0.0040017529390752316
881000 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0511])
The negative loglikelihood (-log p(kills)): 5.521022796630859 or 5.521022796630859 or 5.521022796630859
('system', 'via', 'extinguishing', 'water', 'used', 'firefighters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5243, 'firefighters') has the max prob in order to be the next word. The prob is: 7.737610758340452e-06
881100 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0222])
The negative loglikelihood (-log p(firefighters)): 11.769417762756348 or 11.769417762756348 or 11.769417762756348
('horror', 'home', 'clients', 'parklands', 'residential', 'facility')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1995, 'facility') has the max prob in order to be the next word. The prob is: 0.0004097593773622066
881200 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0339])
The negative loglikelihood (-log p(facility)): 7.799940586090088 or 7.799940586090088 or 7.799940586090088
('erupted', 'leaving', 'least', 'five', 'climbers', 'reported')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (378, 'reported') has the max prob in order to be the next word. The prob is: 0.0025140768848359585
881300 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.1678])
The negative loglikelihood (-log p(reported)): 5.985849380493164 or 5.985849380493164 or 5.985849380493164
('home', 'office', 'claims', 'denied', 'medical', 'treatment')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1084, 'treatment') has the max prob in order to be the next word. The prob is: 0.0037233156617730856
881400 The nn predicts (tensor([174]), 'use') the next word with max prob: tensor([0.0162])
The negative loglikelihood (-log p(treatment)): 5.593140602111816 or 5.593140602111816 or 5.593140602111816
('undermined', 'judiciary', 'presented', 'patriotic', 'man', 'people')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14, 'people') has the max prob in order to be the next word. The prob is: 0.00040336823440156877
881500 The nn predicts (tensor([183]), 'accused') the next word with max prob: tensor([0.0096])
The negative loglikelihood (-log p(people)): 7.81566047668457 or 7.81566047668457 or 7.81566047668457
('syrian', 'attacks', 'may', 'attacked', 'targets', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.06464329361915588
881600 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.0862])
The negative loglikelihood (-log p(syria)): 2.738870859146118 or 2.738870859146118 or 2.738870859146118
('sites', 'women', 'drop', 'unwanted', 'babies', 'without')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (259, 'without') has the max prob in order to be the next word. The prob is: 0.003275663126260042
881700 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0263])
The negative loglikelihood (-log p(without)): 5.7212347984313965 or 5.7212347984313965 or 5.7212347984313965
('<s>', 'israel', 'afraid', 'provoking', 'regional', 'war')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17, 'war') has the max prob in order to be the next word. The prob is: 0.021742351353168488
881800 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0217])
The negative loglikelihood (-log p(war)): 3.828493356704712 or 3.828493356704712 or 3.828493356704712
('abduct', 'un', 'peacekeepers', 'near', 'golan', 'heights')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3226, 'heights') has the max prob in order to be the next word. The prob is: 0.013096334412693977
881900 The nn predicts (tensor([103]), 'border') the next word with max prob: tensor([0.0540])
The negative loglikelihood (-log p(heights)): 4.335422992706299 or 4.335422992706299 or 4.335422992706299
('far', 'back', 'end', 'ice', 'age', 'years')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (33, 'years') has the max prob in order to be the next word. The prob is: 0.02242392674088478
882000 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0224])
The negative loglikelihood (-log p(years)): 3.7976267337799072 or 3.7976267337799072 or 3.7976267337799072
('irish', 'pardon', 'deserters', 'joined', 'britain', 'wwii')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2293, 'wwii') has the max prob in order to be the next word. The prob is: 5.035058711655438e-05
882100 The nn predicts (tensor([163]), 'parliament') the next word with max prob: tensor([0.0107])
The negative loglikelihood (-log p(wwii)): 9.896500587463379 or 9.896500587463379 or 9.896500587463379
('nuclear', 'retaliation', 'designed', 'inflict', 'unacceptable', 'damage')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1193, 'damage') has the max prob in order to be the next word. The prob is: 0.00039076796383596957
882200 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0360])
The negative loglikelihood (-log p(damage)): 7.8473968505859375 or 7.8473968505859375 or 7.8473968505859375
('imran', 'khan', 'chairman', 'pakistan', 'political', 'party')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (118, 'party') has the max prob in order to be the next word. The prob is: 0.12974882125854492
882300 The nn predicts (tensor([118]), 'party') the next word with max prob: tensor([0.1297])
The negative loglikelihood (-log p(party)): 2.0421547889709473 or 2.0421547889709473 or 2.0421547889709473
('crashes', 'control', 'tower', 'italian', 'port', 'genoa')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14792, 'genoa') has the max prob in order to be the next word. The prob is: 7.719033419562038e-06
882400 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0480])
The negative loglikelihood (-log p(genoa)): 11.771821022033691 or 11.771821022033691 or 11.771821022033691
('europeans', 'related', 'go', 'back', 'years', 'scientists')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (305, 'scientists') has the max prob in order to be the next word. The prob is: 0.0007616725633852184
882500 The nn predicts (tensor([608]), 'ago') the next word with max prob: tensor([0.0801])
The negative loglikelihood (-log p(scientists)): 7.179993629455566 or 7.179993629455566 or 7.179993629455566
('companion', 'christchurch', 'lawyer', 'andrew', 'riches', 'cast')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3290, 'cast') has the max prob in order to be the next word. The prob is: 5.302490171743557e-05
882600 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0329])
The negative loglikelihood (-log p(cast)): 9.844748497009277 or 9.844748497009277 or 9.844748497009277
('news', 'italy', 'deaths', 'genoa', 'ship', 'hits')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (301, 'hits') has the max prob in order to be the next word. The prob is: 0.0017161053838208318
882700 The nn predicts (tensor([253]), 'crash') the next word with max prob: tensor([0.0456])
The negative loglikelihood (-log p(hits)): 6.367697715759277 or 6.367697715759277 or 6.367697715759277
('exports', 'often', 'countries', 'questionable', 'human', 'rights')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (67, 'rights') has the max prob in order to be the next word. The prob is: 0.42672431468963623
882800 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.4267])
The negative loglikelihood (-log p(rights)): 0.8516170978546143 or 0.8516170978546143 or 0.8516170978546143
('bulgarian', 'spring', 'highlight', 'desperate', 'electorate', 'protests')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (96, 'protests') has the max prob in order to be the next word. The prob is: 0.00023425134713761508
882900 The nn predicts (tensor([137]), 'end') the next word with max prob: tensor([0.0172])
The negative loglikelihood (-log p(protests)): 8.359115600585938 or 8.359115600585938 or 8.359115600585938
('gmo', 'first', 'country', 'americas', 'ban', 'genetically')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4492, 'genetically') has the max prob in order to be the next word. The prob is: 2.2444988644565456e-05
883000 The nn predicts (tensor([75]), 'women') the next word with max prob: tensor([0.0219])
The negative loglikelihood (-log p(genetically)): 10.704442977905273 or 10.704442977905273 or 10.704442977905273
('see', 'syrian', 'government', 'likely', 'use', 'chemical')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (313, 'chemical') has the max prob in order to be the next word. The prob is: 0.04835183545947075
883100 The nn predicts (tensor([313]), 'chemical') the next word with max prob: tensor([0.0484])
The negative loglikelihood (-log p(chemical)): 3.0292510986328125 or 3.0292510986328125 or 3.0292510986328125
('rat', 'meat', 'rat', 'head', 'found', 'school')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (297, 'school') has the max prob in order to be the next word. The prob is: 0.00032818966428749263
883200 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.1703])
The negative loglikelihood (-log p(school)): 8.021919250488281 or 8.021919250488281 or 8.021919250488281
('hawking', 'israel', 'boycott', 'includes', 'text', 'hawking')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5852, 'hawking') has the max prob in order to be the next word. The prob is: 2.0564655187627068e-06
883300 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.1103])
The negative loglikelihood (-log p(hawking)): 13.094521522521973 or 13.094521522521973 or 13.094521522521973
('charter', 'would', 'allow', 'government', 'track', 'everyone')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2051, 'everyone') has the max prob in order to be the next word. The prob is: 0.000107568186649587
883400 The nn predicts (tensor([198]), 'internet') the next word with max prob: tensor([0.0161])
The negative loglikelihood (-log p(everyone)): 9.137385368347168 or 9.137385368347168 or 9.137385368347168
('coronavirus', 'infection', 'ncov', 'overall', 'infections', 'globally')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4010, 'globally') has the max prob in order to be the next word. The prob is: 0.00010951310832751915
883500 The nn predicts (tensor([237]), 'including') the next word with max prob: tensor([0.0186])
The negative loglikelihood (-log p(globally)): 9.119466781616211 or 9.119466781616211 or 9.119466781616211
('france', 'republican', 'guard', 'costly', 'watchdog', 'rules')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (389, 'rules') has the max prob in order to be the next word. The prob is: 0.0015445156022906303
883600 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0953])
The negative loglikelihood (-log p(rules)): 6.4730448722839355 or 6.4730448722839355 or 6.4730448722839355
('ring', 'police', 'said', 'believe', 'system', 'may')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (47, 'may') has the max prob in order to be the next word. The prob is: 0.003948253579437733
883700 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0150])
The negative loglikelihood (-log p(may)): 5.534482002258301 or 5.534482002258301 or 5.534482002258301
('paparazzo', 'tale', 'bunga', 'bunga', 'blackmail', 'organised')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4948, 'organised') has the max prob in order to be the next word. The prob is: 6.245376425795257e-05
883800 The nn predicts (tensor([992]), 'fraud') the next word with max prob: tensor([0.0101])
The negative loglikelihood (-log p(organised)): 9.681083679199219 or 9.681083679199219 or 9.681083679199219
('although', 'mauritania', 'commitment', 'combating', 'terrorism', 'brought')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1554, 'brought') has the max prob in order to be the next word. The prob is: 0.00037913909181952477
883900 The nn predicts (tensor([331]), 'crimes') the next word with max prob: tensor([0.0262])
The negative loglikelihood (-log p(brought)): 7.877607345581055 or 7.877607345581055 or 7.877607345581055
('workers', 'closed', 'kaesong', 'industrial', 'complex', 'china')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1, 'china') has the max prob in order to be the next word. The prob is: 0.0023134208749979734
884000 The nn predicts (tensor([347]), 'plant') the next word with max prob: tensor([0.0107])
The negative loglikelihood (-log p(china)): 6.069027900695801 or 6.069027900695801 or 6.069027900695801
('mall', 'parking', 'lot', 'construction', 'plans', 'cited')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6351, 'cited') has the max prob in order to be the next word. The prob is: 1.2273926586203743e-05
884100 The nn predicts (tensor([151]), 'help') the next word with max prob: tensor([0.0122])
The negative loglikelihood (-log p(cited)): 11.308032989501953 or 11.308032989501953 or 11.308032989501953
('lauded', 'restoring', 'confidence', 'outside', 'investors', 'others')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (707, 'others') has the max prob in order to be the next word. The prob is: 0.00016050037811510265
884200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0252])
The negative loglikelihood (-log p(others)): 8.737214088439941 or 8.737214088439941 or 8.737214088439941
('fall', 'grace', 'alleged', 'dark', 'dealings', 'gaddafi')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (702, 'gaddafi') has the max prob in order to be the next word. The prob is: 3.517334334901534e-05
884300 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0163])
The negative loglikelihood (-log p(gaddafi)): 10.25522232055664 or 10.25522232055664 or 10.25522232055664
('state', 'media', 'commentary', 'calling', 'question', 'japanese')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (265, 'japanese') has the max prob in order to be the next word. The prob is: 9.729472367325798e-05
884400 The nn predicts (tensor([75]), 'women') the next word with max prob: tensor([0.0115])
The negative loglikelihood (-log p(japanese)): 9.23776626586914 or 9.23776626586914 or 9.23776626586914
('israel', 'presses', 'new', 'west', 'bank', 'houses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2588, 'houses') has the max prob in order to be the next word. The prob is: 4.436026210896671e-05
884500 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0782])
The negative loglikelihood (-log p(houses)): 10.02316665649414 or 10.02316665649414 or 10.02316665649414
('mystery', 'surrounds', 'north', 'korea', 'decision', 'grant')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3500, 'grant') has the max prob in order to be the next word. The prob is: 0.00017571766511537135
884600 The nn predicts (tensor([367]), 'launch') the next word with max prob: tensor([0.0121])
The negative loglikelihood (-log p(grant)): 8.646632194519043 or 8.646632194519043 or 8.646632194519043
('air', 'defense', 'system', 'syria', 'could', 'complicate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10917, 'complicate') has the max prob in order to be the next word. The prob is: 9.060122465598397e-06
884700 The nn predicts (tensor([367]), 'launch') the next word with max prob: tensor([0.0173])
The negative loglikelihood (-log p(complicate)): 11.611627578735352 or 11.611627578735352 or 11.611627578735352
('hezbollah', 'says', 'syria', 'send', 'new', 'arms')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (314, 'arms') has the max prob in order to be the next word. The prob is: 0.020529283210635185
884800 The nn predicts (tensor([149]), 'weapons') the next word with max prob: tensor([0.0446])
The negative loglikelihood (-log p(arms)): 3.8859028816223145 or 3.8859028816223145 or 3.8859028816223145
('says', 'worshipping', 'santa', 'muerte', 'degeneration', 'religion')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1934, 'religion') has the max prob in order to be the next word. The prob is: 0.00032241406734101474
884900 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0073])
The negative loglikelihood (-log p(religion)): 8.039673805236816 or 8.039673805236816 or 8.039673805236816
('twice', 'much', 'illicit', 'financial', 'outflows', 'receives')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2677, 'receives') has the max prob in order to be the next word. The prob is: 1.3091902474116068e-05
885000 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0152])
The negative loglikelihood (-log p(receives)): 11.24351692199707 or 11.24351692199707 or 11.24351692199707
('lives', 'critics', 'wary', 'close', 'involvement', 'pharmaceutical')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6439, 'pharmaceutical') has the max prob in order to be the next word. The prob is: 1.3700765521207359e-05
885100 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0506])
The negative loglikelihood (-log p(pharmaceutical)): 11.19805908203125 or 11.19805908203125 or 11.19805908203125
('<s>', '<s>', 'guatemalan', 'denies', 'massacre', 'charges')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (296, 'charges') has the max prob in order to be the next word. The prob is: 0.0034466118086129427
885200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0291])
The negative loglikelihood (-log p(charges)): 5.670363426208496 or 5.670363426208496 or 5.670363426208496
('separate', 'attacks', 'thursday', 'security', 'teams', 'delivering')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5638, 'delivering') has the max prob in order to be the next word. The prob is: 5.205531124374829e-05
885300 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0425])
The negative loglikelihood (-log p(delivering)): 9.863204002380371 or 9.863204002380371 or 9.863204002380371
('obama', 'visit', 'berlin', 'meet', 'merkel', 'june')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1440, 'june') has the max prob in order to be the next word. The prob is: 0.0007291095098480582
885400 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0136])
The negative loglikelihood (-log p(june)): 7.223686695098877 or 7.223686695098877 or 7.223686695098877
('related', 'alleged', 'via', 'rail', 'terror', 'plan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (169, 'plan') has the max prob in order to be the next word. The prob is: 0.0001432311546523124
885500 The nn predicts (tensor([60]), 'attacks') the next word with max prob: tensor([0.0841])
The negative loglikelihood (-log p(plan)): 8.85105037689209 or 8.85105037689209 or 8.85105037689209
('says', 'plans', 'sell', 'air', 'defense', 'system')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (333, 'system') has the max prob in order to be the next word. The prob is: 0.053977034986019135
885600 The nn predicts (tensor([333]), 'system') the next word with max prob: tensor([0.0540])
The negative loglikelihood (-log p(system)): 2.919196605682373 or 2.919196605682373 or 2.919196605682373
('jewish', 'women', 'group', 'praying', 'jerusalem', 'western')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (275, 'western') has the max prob in order to be the next word. The prob is: 0.0023815827444195747
885700 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0192])
The negative loglikelihood (-log p(western)): 6.039989948272705 or 6.039989948272705 or 6.039989948272705
('fired', 'student', 'slits', 'throat', 'chicken', 'cafeteria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22773, 'cafeteria') has the max prob in order to be the next word. The prob is: 2.0691250028903596e-06
885800 The nn predicts (tensor([514]), 'bangladesh') the next word with max prob: tensor([0.0099])
The negative loglikelihood (-log p(cafeteria)): 13.088384628295898 or 13.088384628295898 or 13.088384628295898
('times', 'automatic', 'pistol', 'driving', 'car', 'city')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (71, 'city') has the max prob in order to be the next word. The prob is: 0.0005118503468111157
885900 The nn predicts (tensor([131]), 'bomb') the next word with max prob: tensor([0.0224])
The negative loglikelihood (-log p(city)): 7.577478408813477 or 7.577478408813477 or 7.577478408813477
('appears', 'half', 'previous', 'size', 'lost', 'much')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (526, 'much') has the max prob in order to be the next word. The prob is: 0.00287246098741889
886000 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.0770])
The negative loglikelihood (-log p(much)): 5.852586269378662 or 5.852586269378662 or 5.852586269378662
('teenage', 'western', 'recruits', 'joining', 'syria', 'jihadist')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1751, 'jihadist') has the max prob in order to be the next word. The prob is: 3.4934728319058195e-05
886100 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0653])
The negative loglikelihood (-log p(jihadist)): 10.262029647827148 or 10.262028694152832 or 10.262028694152832
('pm', 'seeks', 'genuine', 'autonomy', 'communist', 'party')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (118, 'party') has the max prob in order to be the next word. The prob is: 0.47021204233169556
886200 The nn predicts (tensor([118]), 'party') the next word with max prob: tensor([0.4702])
The negative loglikelihood (-log p(party)): 0.7545715570449829 or 0.7545715570449829 or 0.7545715570449829
('voter', 'registration', 'unfavorable', 'polling', 'station', 'atmosphere')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4547, 'atmosphere') has the max prob in order to be the next word. The prob is: 2.6543779313215055e-05
886300 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0261])
The negative loglikelihood (-log p(atmosphere)): 10.536715507507324 or 10.536715507507324 or 10.536715507507324
('labour', 'leader', 'ed', 'miliband', 'says', 'eu')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (43, 'eu') has the max prob in order to be the next word. The prob is: 0.0025318933185189962
886400 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0162])
The negative loglikelihood (-log p(eu)): 5.978787899017334 or 5.978787899017334 or 5.978787899017334
('home', 'mangarahara', 'cichlids', 'known', 'exist', 'urgently')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7552, 'urgently') has the max prob in order to be the next word. The prob is: 1.695855644356925e-05
886500 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0116])
The negative loglikelihood (-log p(urgently)): 10.98473834991455 or 10.98473834991455 or 10.98473834991455
('pakistanis', 'turn', 'vote', 'huge', 'numbers', 'despite')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (243, 'despite') has the max prob in order to be the next word. The prob is: 0.0019145698752254248
886600 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0096])
The negative loglikelihood (-log p(despite)): 6.2582621574401855 or 6.2582621574401855 or 6.2582621574401855
('coast', 'china', 'new', 'pipeline', 'running', 'across')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (266, 'across') has the max prob in order to be the next word. The prob is: 0.0004958207136951387
886700 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(across)): 7.6092963218688965 or 7.6092963218688965 or 7.6092963218688965
('major', 'retailers', 'like', 'walmart', 'benetton', 'gap')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2999, 'gap') has the max prob in order to be the next word. The prob is: 7.506009569624439e-05
886800 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0106])
The negative loglikelihood (-log p(gap)): 9.497221946716309 or 9.497221946716309 or 9.497221946716309
('sharif', 'ahead', 'imran', 'khan', 'contest', 'many')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (247, 'many') has the max prob in order to be the next word. The prob is: 0.00021672942966688424
886900 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0609])
The negative loglikelihood (-log p(many)): 8.436861038208008 or 8.436861038208008 or 8.436861038208008
('deadly', 'incident', 'hit', 'country', 'mining', 'industry')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (695, 'industry') has the max prob in order to be the next word. The prob is: 0.002855118829756975
887000 The nn predicts (tensor([209]), 'workers') the next word with max prob: tensor([0.0289])
The negative loglikelihood (-log p(industry)): 5.858641624450684 or 5.858641624450684 or 5.858641624450684
('recent', 'elections', 'mother', 'frauds', 'kept', 'pressure')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (624, 'pressure') has the max prob in order to be the next word. The prob is: 0.0002501138369552791
887100 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0167])
The negative loglikelihood (-log p(pressure)): 8.293594360351562 or 8.293594360351562 or 8.293594360351562
('says', 'clients', 'lose', 'money', 'cyber', 'fraud')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (992, 'fraud') has the max prob in order to be the next word. The prob is: 0.004052840173244476
887200 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0151])
The negative loglikelihood (-log p(fraud)): 5.508337497711182 or 5.508337497711182 or 5.508337497711182
('officials', 'say', 'volgograd', 'killing', 'homophobic', 'attack')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15, 'attack') has the max prob in order to be the next word. The prob is: 0.012319553643465042
887300 The nn predicts (tensor([791]), 'assault') the next word with max prob: tensor([0.0243])
The negative loglikelihood (-log p(attack)): 4.396567344665527 or 4.396567344665527 or 4.396567344665527
('hunters', 'tirelessly', 'pursue', 'elderly', 'auschwitz', 'war')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17, 'war') has the max prob in order to be the next word. The prob is: 0.0013951099244877696
887400 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0068])
The negative loglikelihood (-log p(war)): 6.574781894683838 or 6.574781894683838 or 6.574781894683838
('tear', 'gas', 'fired', 'palestinians', 'ruins', 'weekend')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1649, 'weekend') has the max prob in order to be the next word. The prob is: 0.0003181764332111925
887500 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0315])
The negative loglikelihood (-log p(weekend)): 8.05290412902832 or 8.05290412902832 or 8.05290412902832
('gawkers', 'believe', 'shape', 'chinese', 'newspaper', 'sizable')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (40208, 'sizable') has the max prob in order to be the next word. The prob is: 1.578785372657876e-06
887600 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0231])
The negative loglikelihood (-log p(sizable)): 13.358855247497559 or 13.358854293823242 or 13.358854293823242
('takes', 'flight', 'attendant', 'job', 'losing', 'bet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4257, 'bet') has the max prob in order to be the next word. The prob is: 1.8701242879615165e-05
887700 The nn predicts (tensor([358]), 'money') the next word with max prob: tensor([0.0093])
The negative loglikelihood (-log p(bet)): 10.886920928955078 or 10.886920928955078 or 10.886920928955078
('daughters', 'back', 'sunday', 'forcibly', 'taken', 'away')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (482, 'away') has the max prob in order to be the next word. The prob is: 0.0037945073563605547
887800 The nn predicts (tensor([619]), 'streets') the next word with max prob: tensor([0.0131])
The negative loglikelihood (-log p(away)): 5.574200630187988 or 5.574200630187988 or 5.574200630187988
('policy', 'conference', 'set', 'friday', 'part', 'last')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (105, 'last') has the max prob in order to be the next word. The prob is: 0.0006967383669689298
887900 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0180])
The negative loglikelihood (-log p(last)): 7.269100666046143 or 7.269100666046143 or 7.269100666046143
('man', 'death', 'family', 'dispute', 'suicide', 'autopsy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9241, 'autopsy') has the max prob in order to be the next word. The prob is: 1.2090717973478604e-05
888000 The nn predicts (tensor([56]), 'man') the next word with max prob: tensor([0.0209])
The negative loglikelihood (-log p(autopsy)): 11.32307243347168 or 11.32307243347168 or 11.32307243347168
('opposition', 'commander', 'accused', 'israel', 'working', 'iran')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9, 'iran') has the max prob in order to be the next word. The prob is: 0.0022838865406811237
888100 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.0334])
The negative loglikelihood (-log p(iran)): 6.081876754760742 or 6.081876754760742 or 6.081876754760742
('lightning', 'ii', 'something', 'else', 'britain', 'moved')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2219, 'moved') has the max prob in order to be the next word. The prob is: 0.00018031096260529011
888200 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0228])
The negative loglikelihood (-log p(moved)): 8.620827674865723 or 8.620827674865723 or 8.620827674865723
('argument', 'sex', 'trial', 'billionaire', 'former', 'italian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (462, 'italian') has the max prob in order to be the next word. The prob is: 0.0003839046403300017
888300 The nn predicts (tensor([109]), 'prime') the next word with max prob: tensor([0.0345])
The negative loglikelihood (-log p(italian)): 7.865116596221924 or 7.865116596221924 or 7.865116596221924
('led', 'speculate', 'may', 'rescued', 'country', 'supreme')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (767, 'supreme') has the max prob in order to be the next word. The prob is: 0.0002425740531180054
888400 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0204])
The negative loglikelihood (-log p(supreme)): 8.324203491210938 or 8.324203491210938 or 8.324203491210938
('high', 'court', 'rules', 'monsanto', 'patent', 'case')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (177, 'case') has the max prob in order to be the next word. The prob is: 0.025416113436222076
888500 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0254])
The negative loglikelihood (-log p(case)): 3.6723718643188477 or 3.6723718643188477 or 3.6723718643188477
('mahmoud', 'mohammad', 'issa', 'mohammad', 'canada', 'news')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (24, 'news') has the max prob in order to be the next word. The prob is: 0.021914832293987274
888600 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0219])
The negative loglikelihood (-log p(news)): 3.820591688156128 or 3.820591688156128 or 3.820591688156128
('appeared', 'impatient', 'manmohan', 'singh', 'commenting', 'laughing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8014, 'laughing') has the max prob in order to be the next word. The prob is: 7.84548683441244e-06
888700 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0184])
The negative loglikelihood (-log p(laughing)): 11.755572319030762 or 11.755572319030762 or 11.755572319030762
('chinazis', 'war', 'china', 'warns', 'dangerous', 'western')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (275, 'western') has the max prob in order to be the next word. The prob is: 0.0010695699602365494
888800 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0233])
The negative loglikelihood (-log p(western)): 6.840498447418213 or 6.840498447418213 or 6.840498447418213
('nasal', 'tubes', 'gitmo', 'revises', 'techniques', 'photos')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (728, 'photos') has the max prob in order to be the next word. The prob is: 8.31232318887487e-05
888900 The nn predicts (tensor([174]), 'use') the next word with max prob: tensor([0.0140])
The negative loglikelihood (-log p(photos)): 9.395186424255371 or 9.395186424255371 or 9.395186424255371
('workers', 'china', 'subject', 'police', 'abuse', 'one')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (37, 'one') has the max prob in order to be the next word. The prob is: 0.0011487927986308932
889000 The nn predicts (tensor([386]), 'abuse') the next word with max prob: tensor([0.0310])
The negative loglikelihood (-log p(one)): 6.769043445587158 or 6.769043445587158 or 6.769043445587158
('boat', 'carrying', 'people', 'sinks', 'west', 'burma')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1582, 'burma') has the max prob in order to be the next word. The prob is: 0.00030353368492797017
889100 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.1029])
The negative loglikelihood (-log p(burma)): 8.100017547607422 or 8.100017547607422 or 8.100017547607422
('sharif', 'beat', 'imran', 'khan', 'happens', 'next')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (219, 'next') has the max prob in order to be the next word. The prob is: 0.0008247040095739067
889200 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0185])
The negative loglikelihood (-log p(next)): 7.100485801696777 or 7.100485801696777 or 7.100485801696777
('man', 'british', 'woman', 'found', 'guilty', 'sex')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (220, 'sex') has the max prob in order to be the next word. The prob is: 0.005919580813497305
889300 The nn predicts (tensor([679]), 'guilty') the next word with max prob: tensor([0.0270])
The negative loglikelihood (-log p(sex)): 5.129489421844482 or 5.129489421844482 or 5.129489421844482
('mastectomy', 'angelina', 'jolie', 'breast', 'removal', 'surgery')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2943, 'surgery') has the max prob in order to be the next word. The prob is: 0.00023669870279263705
889400 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0089])
The negative loglikelihood (-log p(surgery)): 8.348722457885742 or 8.348722457885742 or 8.348722457885742
('north', 'increasing', 'threats', 'including', 'shipping', 'oil')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (66, 'oil') has the max prob in order to be the next word. The prob is: 0.002952761249616742
889500 The nn predicts (tensor([1019]), 'safety') the next word with max prob: tensor([0.0249])
The negative loglikelihood (-log p(oil)): 5.825014591217041 or 5.825014591217041 or 5.825014591217041
('japanese', 'mayor', 'wartime', 'sex', 'slaves', 'necessary')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3232, 'necessary') has the max prob in order to be the next word. The prob is: 0.00029213473317213356
889600 The nn predicts (tensor([220]), 'sex') the next word with max prob: tensor([0.0518])
The negative loglikelihood (-log p(necessary)): 8.13829517364502 or 8.13829517364502 or 8.13829517364502
('zone', 'un', 'climate', 'chief', 'warns', 'world')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7, 'world') has the max prob in order to be the next word. The prob is: 0.0046978541649878025
889700 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0250])
The negative loglikelihood (-log p(world)): 5.360649585723877 or 5.360649585723877 or 5.360649585723877
('france', 'start', 'taxing', 'smartphones', 'tablets', 'fund')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (898, 'fund') has the max prob in order to be the next word. The prob is: 0.00032065430423244834
889800 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.0070])
The negative loglikelihood (-log p(fund)): 8.045146942138672 or 8.045146942138672 or 8.045146942138672
('fishy', 'alleged', 'cia', 'spy', 'arrested', 'moscow')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (317, 'moscow') has the max prob in order to be the next word. The prob is: 0.0029522438999265432
889900 The nn predicts (tensor([390]), 'alleged') the next word with max prob: tensor([0.0232])
The negative loglikelihood (-log p(moscow)): 5.825189590454102 or 5.825189590454102 or 5.825189590454102
('dumb', 'fuck', 'maybe', 'much', 'walking', 'dead')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (48, 'dead') has the max prob in order to be the next word. The prob is: 0.003143246052786708
890000 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(dead)): 5.7624993324279785 or 5.7624993324279785 or 5.7624993324279785
('friendly', 'words', 'white', 'house', 'agreement', 'destruction')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1703, 'destruction') has the max prob in order to be the next word. The prob is: 5.356665496947244e-05
890100 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0240])
The negative loglikelihood (-log p(destruction)): 9.83458423614502 or 9.83458423614502 or 9.83458423614502
('gunmen', 'open', 'fire', 'liquor', 'stores', 'iraq')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44, 'iraq') has the max prob in order to be the next word. The prob is: 0.0005067888414487243
890200 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0163])
The negative loglikelihood (-log p(iraq)): 7.587416172027588 or 7.587416172027588 or 7.587416172027588
('defends', 'gruesome', 'video', 'revenge', 'another', 'video')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (100, 'video') has the max prob in order to be the next word. The prob is: 0.013076798059046268
890300 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0231])
The negative loglikelihood (-log p(video)): 4.336915969848633 or 4.336915969848633 or 4.336915969848633
('unknown', 'powder', 'closed', 'investigation', 'caused', 'immigration')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1205, 'immigration') has the max prob in order to be the next word. The prob is: 9.050737571669742e-05
890400 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0107])
The negative loglikelihood (-log p(immigration)): 9.310079574584961 or 9.310079574584961 or 9.310079574584961
('taiwan', 'coastguards', 'shot', 'dead', 'taiwanese', 'fisherman')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5245, 'fisherman') has the max prob in order to be the next word. The prob is: 0.00016463729843962938
890500 The nn predicts (tensor([401]), 'kong') the next word with max prob: tensor([0.0472])
The negative loglikelihood (-log p(fisherman)): 8.71176528930664 or 8.71176528930664 or 8.71176528930664
('desire', 'normalize', 'relations', 'new', 'delhi', 'subject')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3913, 'subject') has the max prob in order to be the next word. The prob is: 0.0001118472937378101
890600 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0257])
The negative loglikelihood (-log p(subject)): 9.098376274108887 or 9.098376274108887 or 9.098376274108887
('<s>', 'eu', 'pledge', 'euros', 'mali', 'reconstruction')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4548, 'reconstruction') has the max prob in order to be the next word. The prob is: 8.026154682738706e-05
890700 The nn predicts (tensor([78]), 'europe') the next word with max prob: tensor([0.0166])
The negative loglikelihood (-log p(reconstruction)): 9.430219650268555 or 9.430219650268555 or 9.430219650268555
('comes', 'back', 'life', 'funeral', 'zimbabwe', 'funeral')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1670, 'funeral') has the max prob in order to be the next word. The prob is: 0.000999071286059916
890800 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0508])
The negative loglikelihood (-log p(funeral)): 6.908684253692627 or 6.908684253692627 or 6.908684253692627
('three', 'bigha', 'land', 'acquired', 'tata', 'nano')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18055, 'nano') has the max prob in order to be the next word. The prob is: 3.0490195968013722e-06
890900 The nn predicts (tensor([1185]), 'mine') the next word with max prob: tensor([0.0146])
The negative loglikelihood (-log p(nano)): 12.700690269470215 or 12.700690269470215 or 12.700690269470215
('fight', 'chinese', 'project', 'probes', 'genetics', 'genius')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11422, 'genius') has the max prob in order to be the next word. The prob is: 5.583052370639052e-06
891000 The nn predicts (tensor([184]), 'drug') the next word with max prob: tensor([0.0153])
The negative loglikelihood (-log p(genius)): 12.09577465057373 or 12.09577465057373 or 12.09577465057373
('west', 'police', 'blown', 'wedding', 'invitation', 'gupta')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17022, 'gupta') has the max prob in order to be the next word. The prob is: 9.120807362705818e-07
891100 The nn predicts (tensor([317]), 'moscow') the next word with max prob: tensor([0.0084])
The negative loglikelihood (-log p(gupta)): 13.907537460327148 or 13.907537460327148 or 13.907537460327148
('green', 'party', 'germany', 'investigate', 'backing', 'paedophiles')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7192, 'paedophiles') has the max prob in order to be the next word. The prob is: 1.3349724213185254e-05
891200 The nn predicts (tensor([312]), 'bill') the next word with max prob: tensor([0.0118])
The negative loglikelihood (-log p(paedophiles)): 11.224015235900879 or 11.224015235900879 or 11.224015235900879
('japan', 'republic', 'korea', 'singapore', 'granted', 'new')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4, 'new') has the max prob in order to be the next word. The prob is: 0.02713569439947605
891300 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0271])
The negative loglikelihood (-log p(new)): 3.606905221939087 or 3.606905221939087 or 3.606905221939087
('still', 'growing', 'slowly', 'rate', 'france', 'nearly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (392, 'nearly') has the max prob in order to be the next word. The prob is: 0.00036688800901174545
891400 The nn predicts (tensor([78]), 'europe') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(nearly)): 7.910453796386719 or 7.910453796386719 or 7.910453796386719
('france', 'spitting', 'wind', 'anti', 'globalization', 'mentality')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12173, 'mentality') has the max prob in order to be the next word. The prob is: 5.851267815160099e-06
891500 The nn predicts (tensor([606]), 'marriage') the next word with max prob: tensor([0.0094])
The negative loglikelihood (-log p(mentality)): 12.04885196685791 or 12.04885196685791 or 12.04885196685791
('adult', 'tissue', 'first', 'time', 'cells', 'made')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (280, 'made') has the max prob in order to be the next word. The prob is: 0.003174178535118699
891600 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0165])
The negative loglikelihood (-log p(made)): 5.752706527709961 or 5.752706527709961 or 5.752706527709961
('us', 'spy', 'ryan', 'fogle', 'expelled', 'moscow')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (317, 'moscow') has the max prob in order to be the next word. The prob is: 0.00908765010535717
891700 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0307])
The negative loglikelihood (-log p(moscow)): 4.700839042663574 or 4.700839042663574 or 4.700839042663574
('alleged', 'cia', 'officer', 'arrest', 'moscow', 'may')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (47, 'may') has the max prob in order to be the next word. The prob is: 0.001013249740935862
891800 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0191])
The negative loglikelihood (-log p(may)): 6.894592761993408 or 6.894592761993408 or 6.894592761993408
('lawyer', 'contact', 'without', 'invasive', 'body', 'search')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (561, 'search') has the max prob in order to be the next word. The prob is: 0.0002626202767714858
891900 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0194])
The negative loglikelihood (-log p(search)): 8.24480152130127 or 8.24480152130127 or 8.24480152130127
('ship', 'rescues', 'australian', 'sailor', 'middle', 'pacific')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1022, 'pacific') has the max prob in order to be the next word. The prob is: 0.00014061281399335712
892000 The nn predicts (tensor([156]), 'east') the next word with max prob: tensor([0.8669])
The negative loglikelihood (-log p(pacific)): 8.869500160217285 or 8.869500160217285 or 8.869500160217285
('first', 'success', 'cloning', 'human', 'stem', 'cells')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2813, 'cells') has the max prob in order to be the next word. The prob is: 0.0005534027586691082
892100 The nn predicts (tensor([95]), 'global') the next word with max prob: tensor([0.0113])
The negative loglikelihood (-log p(cells)): 7.499424457550049 or 7.499424457550049 or 7.499424457550049
('foreign', 'tourists', 'country', 'leaders', 'meet', 'summit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (630, 'summit') has the max prob in order to be the next word. The prob is: 0.0019355593249201775
892200 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0246])
The negative loglikelihood (-log p(summit)): 6.247358798980713 or 6.247358798980713 or 6.247358798980713
('conflict', 'bbc', 'shown', 'signs', 'chemical', 'attack')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15, 'attack') has the max prob in order to be the next word. The prob is: 0.005279649514704943
892300 The nn predicts (tensor([149]), 'weapons') the next word with max prob: tensor([0.3839])
The negative loglikelihood (-log p(attack)): 5.243895530700684 or 5.243895530700684 or 5.243895530700684
('<s>', '<s>', 'venezuela', 'running', 'toilet', 'paper')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1447, 'paper') has the max prob in order to be the next word. The prob is: 0.00023268182121682912
892400 The nn predicts (tensor([401]), 'kong') the next word with max prob: tensor([0.0131])
The negative loglikelihood (-log p(paper)): 8.365839004516602 or 8.365839004516602 or 8.365839004516602
('intent', 'switch', 'emphasis', 'punishment', 'rehabilitation', 'inmates')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2150, 'inmates') has the max prob in order to be the next word. The prob is: 0.00024215465236920863
892500 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0104])
The negative loglikelihood (-log p(inmates)): 8.325934410095215 or 8.325934410095215 or 8.325934410095215
('market', 'brokers', 'investors', 'worry', 'muslim', 'brotherhood')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1069, 'brotherhood') has the max prob in order to be the next word. The prob is: 0.002390599576756358
892600 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0378])
The negative loglikelihood (-log p(brotherhood)): 6.036211013793945 or 6.036211013793945 or 6.036211013793945
('three', 'policemen', 'four', 'army', 'officers', 'security')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (57, 'security') has the max prob in order to be the next word. The prob is: 0.0006195921450853348
892700 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0721])
The negative loglikelihood (-log p(security)): 7.386449337005615 or 7.386449337005615 or 7.386449337005615
('malaysian', 'couple', 'jailed', 'starving', 'maid', 'death')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (42, 'death') has the max prob in order to be the next word. The prob is: 0.04633096233010292
892800 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0463])
The negative loglikelihood (-log p(death)): 3.0719447135925293 or 3.0719447135925293 or 3.0719447135925293
('chicago', 'bulls', 'memphis', 'stun', 'oklahoma', 'city')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (71, 'city') has the max prob in order to be the next word. The prob is: 0.02597101591527462
892900 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0260])
The negative loglikelihood (-log p(city)): 3.6507742404937744 or 3.6507742404937744 or 3.6507742404937744
('status', 'quo', 'analysts', 'say', 'garment', 'factory')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1015, 'factory') has the max prob in order to be the next word. The prob is: 0.006100600119680166
893000 The nn predicts (tensor([209]), 'workers') the next word with max prob: tensor([0.0236])
The negative loglikelihood (-log p(factory)): 5.099368095397949 or 5.099368095397949 or 5.099368095397949
('food', 'set', 'surge', 'within', 'region', 'productivity')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11561, 'productivity') has the max prob in order to be the next word. The prob is: 4.338534927228466e-05
893100 The nn predicts (tensor([357]), 'region') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(productivity)): 10.045389175415039 or 10.045389175415039 or 10.045389175415039
('wants', 'spy', 'communications', 'vpns', 'able', 'use')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (174, 'use') has the max prob in order to be the next word. The prob is: 0.010025229305028915
893200 The nn predicts (tensor([215]), 'make') the next word with max prob: tensor([0.0117])
The negative loglikelihood (-log p(use)): 4.6026506423950195 or 4.6026506423950195 or 4.6026506423950195
('bubble', 'china', 'counterfeit', 'condoms', 'police', 'confiscated')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5222, 'confiscated') has the max prob in order to be the next word. The prob is: 2.4701574147911742e-05
893300 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0860])
The negative loglikelihood (-log p(confiscated)): 10.608643531799316 or 10.608643531799316 or 10.608643531799316
('begins', 'offensive', 'boko', 'haram', 'islamists', 'nigerian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (748, 'nigerian') has the max prob in order to be the next word. The prob is: 0.00018860885757021606
893400 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0229])
The negative loglikelihood (-log p(nigerian)): 8.575835227966309 or 8.575835227966309 or 8.575835227966309
('region', 'declared', 'president', 'region', 'leaders', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.0310641061514616
893500 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0311])
The negative loglikelihood (-log p(said)): 3.4717023372650146 or 3.4717023372650146 or 3.4717023372650146
('survive', 'without', 'britain', 'says', 'french', 'president')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12, 'president') has the max prob in order to be the next word. The prob is: 0.0225671473890543
893600 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0436])
The negative loglikelihood (-log p(president)): 3.791260004043579 or 3.791260004043579 or 3.791260004043579
('approve', 'four', 'unauthorised', 'west', 'bank', 'settler')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3425, 'settler') has the max prob in order to be the next word. The prob is: 6.023990499670617e-05
893700 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0183])
The negative loglikelihood (-log p(settler)): 9.717175483703613 or 9.717175483703613 or 9.717175483703613
('camel', 'muster', 'master', 'plan', 'gets', 'enterprise')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8384, 'enterprise') has the max prob in order to be the next word. The prob is: 1.2745719686790835e-05
893800 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.0096])
The negative loglikelihood (-log p(enterprise)): 11.270315170288086 or 11.270315170288086 or 11.270315170288086
('cruise', 'missiles', 'could', 'make', 'difficult', 'united')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (140, 'united') has the max prob in order to be the next word. The prob is: 0.0007765043410472572
893900 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.0150])
The negative loglikelihood (-log p(united)): 7.160708427429199 or 7.160708427429199 or 7.160708427429199
('new', 'security', 'strategy', 'tame', 'raging', 'drug')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (184, 'drug') has the max prob in order to be the next word. The prob is: 0.0010743470629677176
894000 The nn predicts (tensor([357]), 'region') the next word with max prob: tensor([0.0194])
The negative loglikelihood (-log p(drug)): 6.836042404174805 or 6.836042404174805 or 6.836042404174805
('video', 'rob', 'ford', 'toronto', 'mayor', 'inhaling')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15716, 'inhaling') has the max prob in order to be the next word. The prob is: 1.829397206165595e-06
894100 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0244])
The negative loglikelihood (-log p(inhaling)): 13.21152400970459 or 13.21152400970459 or 13.21152400970459
('cambodian', 'factory', 'shows', 'extent', 'safety', 'issues')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (717, 'issues') has the max prob in order to be the next word. The prob is: 0.0009396539535373449
894200 The nn predicts (tensor([209]), 'workers') the next word with max prob: tensor([0.0456])
The negative loglikelihood (-log p(issues)): 6.969998836517334 or 6.969998836517334 or 6.969998836517334
('high', 'school', 'massacre', 'threat', 'posted', 'online')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (338, 'online') has the max prob in order to be the next word. The prob is: 0.010803830809891224
894300 The nn predicts (tensor([100]), 'video') the next word with max prob: tensor([0.0117])
The negative loglikelihood (-log p(online)): 4.5278544425964355 or 4.5278544425964355 or 4.5278544425964355
('coahuila', 'state', 'abutting', 'texas', 'picked', 'newsroom')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22286, 'newsroom') has the max prob in order to be the next word. The prob is: 4.758683644467965e-06
894400 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0103])
The negative loglikelihood (-log p(newsroom)): 12.255539894104004 or 12.255539894104004 or 12.255539894104004
('supply', 'weapons', 'syria', 'signed', 'contracts', 'without')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (259, 'without') has the max prob in order to be the next word. The prob is: 0.0012812120839953423
894500 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0343])
The negative loglikelihood (-log p(without)): 6.659948825836182 or 6.659948825836182 or 6.659948825836182
('island', 'town', 'civilian', 'passengers', 'including', 'group')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (62, 'group') has the max prob in order to be the next word. The prob is: 0.00043705795542337
894600 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0529])
The negative loglikelihood (-log p(group)): 7.73544454574585 or 7.73544454574585 or 7.73544454574585
('tallest', 'building', 'middle', 'empty', 'field', 'china')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1, 'china') has the max prob in order to be the next word. The prob is: 0.0007012392161414027
894700 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0105])
The negative loglikelihood (-log p(china)): 7.262661457061768 or 7.262661457061768 or 7.262661457061768
('rich', 'poor', 'registered', 'chile', 'mexico', 'turkey')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (49, 'turkey') has the max prob in order to be the next word. The prob is: 6.5663889472489245e-06
894800 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0218])
The negative loglikelihood (-log p(turkey)): 11.93354606628418 or 11.93354606628418 or 11.93354606628418
('southeastern', 'state', 'tabasco', 'taking', 'country', 'largest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (276, 'largest') has the max prob in order to be the next word. The prob is: 0.0018277394119650126
894900 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0200])
The negative loglikelihood (-log p(largest)): 6.304675579071045 or 6.304675579071045 or 6.304675579071045
('letter', 'spirit', 'un', 'prohibitionist', 'drug', 'policy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (368, 'policy') has the max prob in order to be the next word. The prob is: 0.007086882367730141
895000 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0544])
The negative loglikelihood (-log p(policy)): 4.949509620666504 or 4.949509620666504 or 4.949509620666504
('russian', 'fleet', 'warships', 'enters', 'mediterranean', 'first')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (28, 'first') has the max prob in order to be the next word. The prob is: 0.001181397819891572
895100 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0552])
The negative loglikelihood (-log p(first)): 6.7410569190979 or 6.7410569190979 or 6.7410569190979
('problem', 'americas', 'one', 'defending', 'position', 'neither')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5575, 'neither') has the max prob in order to be the next word. The prob is: 0.00010697100515244529
895200 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0141])
The negative loglikelihood (-log p(neither)): 9.142952919006348 or 9.142952919006348 or 9.142952919006348
('bureau', 'investigation', 'arrests', 'superintendent', 'police', 'investigating')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1548, 'investigating') has the max prob in order to be the next word. The prob is: 0.002199013251811266
895300 The nn predicts (tensor([610]), 'officers') the next word with max prob: tensor([0.0817])
The negative loglikelihood (-log p(investigating)): 6.119746685028076 or 6.119746685028076 or 6.119746685028076
('managed', 'scrape', 'past', 'catastrophe', 'may', 'firefighters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5243, 'firefighters') has the max prob in order to be the next word. The prob is: 1.1473004633444361e-05
895400 The nn predicts (tensor([215]), 'make') the next word with max prob: tensor([0.0181])
The negative loglikelihood (-log p(firefighters)): 11.375514030456543 or 11.375514030456543 or 11.375514030456543
('moon', 'causes', 'explosion', 'visible', 'naked', 'eye')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1972, 'eye') has the max prob in order to be the next word. The prob is: 0.0012072001118212938
895500 The nn predicts (tensor([56]), 'man') the next word with max prob: tensor([0.0149])
The negative loglikelihood (-log p(eye)): 6.719451427459717 or 6.719451427459717 or 6.719451427459717
('part', 'zanu', 'pf', 'leader', 'final', 'wish')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5074, 'wish') has the max prob in order to be the next word. The prob is: 3.0445948141277768e-05
895600 The nn predicts (tensor([303]), 'visit') the next word with max prob: tensor([0.0219])
The negative loglikelihood (-log p(wish)): 10.399558067321777 or 10.399558067321777 or 10.399558067321777
('security', 'thailand', 'next', 'week', 'embassy', 'official')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (107, 'official') has the max prob in order to be the next word. The prob is: 0.00225906353443861
895700 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0194])
The negative loglikelihood (-log p(official)): 6.092804908752441 or 6.092804908752441 or 6.092804908752441
('states', 'air', 'force', 'back', 'operations', 'islamists')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (933, 'islamists') has the max prob in order to be the next word. The prob is: 0.00011683049524435773
895800 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0667])
The negative loglikelihood (-log p(islamists)): 9.054786682128906 or 9.054786682128906 or 9.054786682128906
('blocked', 'law', 'aims', 'protect', 'women', 'freedoms')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4834, 'freedoms') has the max prob in order to be the next word. The prob is: 6.491621752502397e-05
895900 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.1083])
The negative loglikelihood (-log p(freedoms)): 9.642413139343262 or 9.642413139343262 or 9.642413139343262
('ban', 'susilo', 'bambang', 'yudhoyono', 'expected', 'sign')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (550, 'sign') has the max prob in order to be the next word. The prob is: 0.0017158217960968614
896000 The nn predicts (tensor([300]), 'friday') the next word with max prob: tensor([0.0148])
The negative loglikelihood (-log p(sign)): 6.367863178253174 or 6.367863178253174 or 6.367863178253174
('president', 'hollande', 'signs', 'gay', 'marriage', 'bill')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (312, 'bill') has the max prob in order to be the next word. The prob is: 0.10419794172048569
896100 The nn predicts (tensor([312]), 'bill') the next word with max prob: tensor([0.1042])
The negative loglikelihood (-log p(bill)): 2.261462926864624 or 2.261462926864624 or 2.261462926864624
('blunt', 'assessment', 'coronavirus', 'outbreak', 'saudi', 'arabia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (188, 'arabia') has the max prob in order to be the next word. The prob is: 0.37216487526893616
896200 The nn predicts (tensor([188]), 'arabia') the next word with max prob: tensor([0.3722])
The negative loglikelihood (-log p(arabia)): 0.9884183406829834 or 0.9884182810783386 or 0.9884182810783386
('study', 'gamechanging', 'report', 'global', 'drugs', 'trade')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (227, 'trade') has the max prob in order to be the next word. The prob is: 0.027274977415800095
896300 The nn predicts (tensor([227]), 'trade') the next word with max prob: tensor([0.0273])
The negative loglikelihood (-log p(trade)): 3.601785659790039 or 3.601785659790039 or 3.601785659790039
('berlin', 'kabul', 'amid', 'difficult', 'negotiations', 'troop')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2980, 'troop') has the max prob in order to be the next word. The prob is: 9.984755160985515e-05
896400 The nn predicts (tensor([64]), 'deal') the next word with max prob: tensor([0.0142])
The negative loglikelihood (-log p(troop)): 9.21186637878418 or 9.21186637878418 or 9.21186637878418
('gay', 'pride', 'marchers', 'ready', 'defy', 'violence')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (141, 'violence') has the max prob in order to be the next word. The prob is: 0.0030541010200977325
896500 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0464])
The negative loglikelihood (-log p(violence)): 5.7912702560424805 or 5.791269779205322 or 5.791269779205322
('ruled', 'bell', 'liable', 'cellphone', 'users', 'paid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1265, 'paid') has the max prob in order to be the next word. The prob is: 0.0014351001009345055
896600 The nn predicts (tensor([47]), 'may') the next word with max prob: tensor([0.0116])
The negative loglikelihood (-log p(paid)): 6.546520709991455 or 6.546520709991455 or 6.546520709991455
('russia', 'names', 'cia', 'station', 'chief', 'moscow')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (317, 'moscow') has the max prob in order to be the next word. The prob is: 0.005467934999614954
896700 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(moscow)): 5.2088541984558105 or 5.2088541984558105 or 5.2088541984558105
('traffic', 'violations', 'month', 'ago', 'traffic', 'violators')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12065, 'violators') has the max prob in order to be the next word. The prob is: 3.6904282296745805e-06
896800 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0125])
The negative loglikelihood (-log p(violators)): 12.50976848602295 or 12.50976848602295 or 12.50976848602295
('law', 'stipulating', 'much', 'content', 'foreign', 'channels')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5623, 'channels') has the max prob in order to be the next word. The prob is: 5.465599679155275e-05
896900 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0440])
The negative loglikelihood (-log p(channels)): 9.814451217651367 or 9.814451217651367 or 9.814451217651367
('khan', 'party', 'shot', 'dead', 'karachi', 'senior')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (495, 'senior') has the max prob in order to be the next word. The prob is: 0.00023117676028050482
897000 The nn predicts (tensor([131]), 'bomb') the next word with max prob: tensor([0.0168])
The negative loglikelihood (-log p(senior)): 8.37232780456543 or 8.37232780456543 or 8.37232780456543
('indonesian', 'puppet', 'show', 'attracts', 'audience', 'turkey')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (49, 'turkey') has the max prob in order to be the next word. The prob is: 0.00025118159828707576
897100 The nn predicts (tensor([186]), 'public') the next word with max prob: tensor([0.0065])
The negative loglikelihood (-log p(turkey)): 8.289334297180176 or 8.289334297180176 or 8.289334297180176
('bp', 'norway', 'statoil', 'week', 'part', 'investigation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (421, 'investigation') has the max prob in order to be the next word. The prob is: 0.0015688298735767603
897200 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0068])
The negative loglikelihood (-log p(investigation)): 6.457425117492676 or 6.457425117492676 or 6.457425117492676
('philippines', 'waiting', 'tempers', 'taiwan', 'cool', 'settling')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12042, 'settling') has the max prob in order to be the next word. The prob is: 5.8622463257052004e-06
897300 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0079])
The negative loglikelihood (-log p(settling)): 12.046977996826172 or 12.046977996826172 or 12.046977996826172
('border', 'incursions', 'pm', 'voices', 'india', 'serious')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (986, 'serious') has the max prob in order to be the next word. The prob is: 0.0002580535365268588
897400 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0435])
The negative loglikelihood (-log p(serious)): 8.262343406677246 or 8.262343406677246 or 8.262343406677246
('israel', 'threatens', 'strikes', 'lebanese', 'hezbollah', 'militants')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (164, 'militants') has the max prob in order to be the next word. The prob is: 0.037223830819129944
897500 The nn predicts (tensor([164]), 'militants') the next word with max prob: tensor([0.0372])
The negative loglikelihood (-log p(militants)): 3.2908060550689697 or 3.2908060550689697 or 3.2908060550689697
('advanced', 'drone', 'hacked', 'hijacked', 'iran', 'hezbollah')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (670, 'hezbollah') has the max prob in order to be the next word. The prob is: 0.0016057833563536406
897600 The nn predicts (tensor([582]), 'missiles') the next word with max prob: tensor([0.0359])
The negative loglikelihood (-log p(hezbollah)): 6.434143543243408 or 6.434143543243408 or 6.434143543243408
('iran', 'hangs', 'two', 'men', 'spying', 'israel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10, 'israel') has the max prob in order to be the next word. The prob is: 0.017040466889739037
897700 The nn predicts (tensor([304]), 'agency') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(israel)): 4.072164535522461 or 4.072164535522461 or 4.072164535522461
('abroad', 'cost', 'government', 'millions', 'legal', 'fees')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3808, 'fees') has the max prob in order to be the next word. The prob is: 0.00048325289390049875
897800 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0159])
The negative loglikelihood (-log p(fees)): 7.634970664978027 or 7.634970664978027 or 7.634970664978027
('party', 'wins', 'pakistan', 'parliamentary', 'elections', 'protests')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (96, 'protests') has the max prob in order to be the next word. The prob is: 0.0006908098002895713
897900 The nn predicts (tensor([366]), 'elections') the next word with max prob: tensor([0.0214])
The negative loglikelihood (-log p(protests)): 7.277646064758301 or 7.277646064758301 or 7.277646064758301
('<s>', 'egypt', 'muslims', 'attack', 'christian', 'church')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (403, 'church') has the max prob in order to be the next word. The prob is: 0.005497588776051998
898000 The nn predicts (tensor([407]), 'muslims') the next word with max prob: tensor([0.0536])
The negative loglikelihood (-log p(church)): 5.203445911407471 or 5.203445911407471 or 5.203445911407471
('meeting', 'laos', 'china', 'train', 'project', 'linking')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4536, 'linking') has the max prob in order to be the next word. The prob is: 2.0665496776928194e-05
898100 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0201])
The negative loglikelihood (-log p(linking)): 10.7870454788208 or 10.7870454788208 or 10.7870454788208
('cut', 'back', 'lavish', 'living', 'amid', 'growing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (527, 'growing') has the max prob in order to be the next word. The prob is: 0.002671503461897373
898200 The nn predicts (tensor([380]), 'fears') the next word with max prob: tensor([0.0149])
The negative loglikelihood (-log p(growing)): 5.925113677978516 or 5.925113677978516 or 5.925113677978516
('million', 'login', 'names', 'may', 'stolen', 'hack')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1977, 'hack') has the max prob in order to be the next word. The prob is: 8.056492515606806e-05
898300 The nn predicts (tensor([358]), 'money') the next word with max prob: tensor([0.0937])
The negative loglikelihood (-log p(hack)): 9.426446914672852 or 9.426446914672852 or 9.426446914672852
('beijing', 'contains', 'little', 'silk', 'study', 'finds')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (446, 'finds') has the max prob in order to be the next word. The prob is: 0.009631487540900707
898400 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(finds)): 4.6427178382873535 or 4.642717361450195 or 4.642717361450195
('youths', 'burn', 'cars', 'north', 'stockholm', 'riots')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1042, 'riots') has the max prob in order to be the next word. The prob is: 0.00011911641922779381
898500 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0193])
The negative loglikelihood (-log p(riots)): 9.035408973693848 or 9.035408973693848 or 9.035408973693848
('political', 'cairo', 'main', 'forensic', 'facility', 'dealing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3896, 'dealing') has the max prob in order to be the next word. The prob is: 1.9538421838660724e-05
898600 The nn predicts (tensor([361]), 'sunday') the next word with max prob: tensor([0.0201])
The negative loglikelihood (-log p(dealing)): 10.843127250671387 or 10.843127250671387 or 10.843127250671387
('houses', 'order', 'summit', 'june', 'claims', 'tax')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (274, 'tax') has the max prob in order to be the next word. The prob is: 0.0006601913482882082
898700 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0228])
The negative loglikelihood (-log p(tax)): 7.322980880737305 or 7.322980880737305 or 7.322980880737305
('woman', 'phone', 'gets', 'run', 'bus', 'karma')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13299, 'karma') has the max prob in order to be the next word. The prob is: 1.3105604011798277e-05
898800 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0169])
The negative loglikelihood (-log p(karma)): 11.242470741271973 or 11.242470741271973 or 11.242470741271973
('five', 'maps', 'charts', 'graphs', 'prove', 'planet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1663, 'planet') has the max prob in order to be the next word. The prob is: 0.00036691760760731995
898900 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0156])
The negative loglikelihood (-log p(planet)): 7.910373210906982 or 7.910373210906982 or 7.910373210906982
('lawmakers', 'block', 'legislation', 'protecting', 'afghan', 'women')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (75, 'women') has the max prob in order to be the next word. The prob is: 0.015952976420521736
899000 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0961])
The negative loglikelihood (-log p(women)): 4.1381096839904785 or 4.1381096839904785 or 4.1381096839904785
('conflict', 'branches', 'divided', 'syrian', 'opposition', 'held')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (292, 'held') has the max prob in order to be the next word. The prob is: 0.0003810376219917089
899100 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0611])
The negative loglikelihood (-log p(held)): 7.872612476348877 or 7.872612476348877 or 7.872612476348877
('take', 'role', 'kim', 'control', 'country', 'crumbles')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10794, 'crumbles') has the max prob in order to be the next word. The prob is: 3.5858299725077813e-06
899200 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0359])
The negative loglikelihood (-log p(crumbles)): 12.538520812988281 or 12.538520812988281 or 12.538520812988281
('burn', 'water', 'perpetually', 'poured', 'tepco', 'proposing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7600, 'proposing') has the max prob in order to be the next word. The prob is: 1.299341420235578e-05
899300 The nn predicts (tensor([347]), 'plant') the next word with max prob: tensor([0.0241])
The negative loglikelihood (-log p(proposing)): 11.251068115234375 or 11.251068115234375 or 11.251068115234375
('military', 'leader', 'efrain', 'rios', 'montt', 'conviction')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3019, 'conviction') has the max prob in order to be the next word. The prob is: 0.0003474550321698189
899400 The nn predicts (tensor([875]), 'genocide') the next word with max prob: tensor([0.0178])
The negative loglikelihood (-log p(conviction)): 7.964875221252441 or 7.964875221252441 or 7.964875221252441
('controversial', 'law', 'paves', 'way', 'president', 'evo')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5880, 'evo') has the max prob in order to be the next word. The prob is: 2.178645991079975e-05
899500 The nn predicts (tensor([1088]), 'bashar') the next word with max prob: tensor([0.0129])
The negative loglikelihood (-log p(evo)): 10.734222412109375 or 10.734221458435059 or 10.734221458435059
('<s>', '<s>', 'indian', 'media', 'ties', 'beijing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (449, 'beijing') has the max prob in order to be the next word. The prob is: 0.0004927890840917826
899600 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1891])
The negative loglikelihood (-log p(beijing)): 7.615429401397705 or 7.615429401397705 or 7.615429401397705
('philippines', 'protested', 'presence', 'chinese', 'warship', 'vessels')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3299, 'vessels') has the max prob in order to be the next word. The prob is: 0.00015438969421666116
899700 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0276])
The negative loglikelihood (-log p(vessels)): 8.776030540466309 or 8.776030540466309 or 8.776030540466309
('proposal', 'classify', 'oil', 'tar', 'sands', 'highly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2505, 'highly') has the max prob in order to be the next word. The prob is: 0.00011831412848550826
899800 The nn predicts (tensor([66]), 'oil') the next word with max prob: tensor([0.0686])
The negative loglikelihood (-log p(highly)): 9.042167663574219 or 9.042167663574219 or 9.042167663574219
('outrage', 'saying', 'comfort', 'women', 'military', 'necessity')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12895, 'necessity') has the max prob in order to be the next word. The prob is: 1.3970218788017519e-05
899900 The nn predicts (tensor([204]), 'force') the next word with max prob: tensor([0.0393])
The negative loglikelihood (-log p(necessity)): 11.178583145141602 or 11.178583145141602 or 11.178583145141602
('president', 'abdelaziz', 'bouteflika', 'latest', 'health', 'crisis')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (65, 'crisis') has the max prob in order to be the next word. The prob is: 0.0266040600836277
900000 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0589])
The negative loglikelihood (-log p(crisis)): 3.6266913414001465 or 3.6266913414001465 or 3.6266913414001465
('choros', 'small', 'fishing', 'town', 'north', 'la')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3028, 'la') has the max prob in order to be the next word. The prob is: 5.765728565165773e-05
900100 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0877])
The negative loglikelihood (-log p(la)): 9.760993957519531 or 9.760993957519531 or 9.760993957519531
('business', 'newspaper', 'les', 'echos', 'reported', 'saturday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (493, 'saturday') has the max prob in order to be the next word. The prob is: 0.006204837467521429
900200 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0206])
The negative loglikelihood (-log p(saturday)): 5.082426071166992 or 5.082426071166992 or 5.082426071166992
('serial', 'killer', 'thomas', 'quick', 'goes', 'sture')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44436, 'sture') has the max prob in order to be the next word. The prob is: 7.789012670400552e-07
900300 The nn predicts (tensor([100]), 'video') the next word with max prob: tensor([0.0171])
The negative loglikelihood (-log p(sture)): 14.06538200378418 or 14.06538200378418 or 14.06538200378418
('insists', 'uk', 'pakistani', 'community', 'must', 'tackle')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1886, 'tackle') has the max prob in order to be the next word. The prob is: 0.0005949964979663491
900400 The nn predicts (tensor([151]), 'help') the next word with max prob: tensor([0.0361])
The negative loglikelihood (-log p(tackle)): 7.426955223083496 or 7.426955223083496 or 7.426955223083496
('gaza', 'security', 'forces', 'uphold', 'standards', 'manliness')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (63061, 'manliness') has the max prob in order to be the next word. The prob is: 1.1853562682517804e-06
900500 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0400])
The negative loglikelihood (-log p(manliness)): 13.645466804504395 or 13.645466804504395 or 13.645466804504395
('three', 'charged', 'alleged', 'assault', 'downtown', 'toronto')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1213, 'toronto') has the max prob in order to be the next word. The prob is: 0.0007830238901078701
900600 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0375])
The negative loglikelihood (-log p(toronto)): 7.152347564697266 or 7.152347564697266 or 7.152347564697266
('state', 'department', 'official', 'confirmed', 'tuesday', 'iran')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9, 'iran') has the max prob in order to be the next word. The prob is: 0.0007823038031347096
900700 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0219])
The negative loglikelihood (-log p(iran)): 7.1532673835754395 or 7.1532673835754395 or 7.1532673835754395
('remedy', 'extracted', 'captive', 'bears', 'stirs', 'furor')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9764, 'furor') has the max prob in order to be the next word. The prob is: 1.0171898793487344e-05
900800 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(furor)): 11.495882034301758 or 11.495882034301758 or 11.495882034301758
('ahmadinejad', 'vowed', 'take', 'decision', 'disqualify', 'close')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (428, 'close') has the max prob in order to be the next word. The prob is: 0.00011052754416596144
900900 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0282])
The negative loglikelihood (-log p(close)): 9.110245704650879 or 9.110245704650879 or 9.110245704650879
('debt', 'ceiling', 'crisis', 'right', 'around', 'corner')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6725, 'corner') has the max prob in order to be the next word. The prob is: 6.158982432680205e-05
901000 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.1310])
The negative loglikelihood (-log p(corner)): 9.695013999938965 or 9.695013999938965 or 9.695013999938965
('artist', 'ai', 'weiwei', 'released', 'music', 'video')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (100, 'video') has the max prob in order to be the next word. The prob is: 0.014646303839981556
901100 The nn predicts (tensor([100]), 'video') the next word with max prob: tensor([0.0146])
The negative loglikelihood (-log p(video)): 4.223567485809326 or 4.223567485809326 or 4.223567485809326
('reveals', 'india', 'new', 'york', 'times', 'analysis')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1783, 'analysis') has the max prob in order to be the next word. The prob is: 0.00020897989452350885
901200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0125])
The negative loglikelihood (-log p(analysis)): 8.473272323608398 or 8.473272323608398 or 8.473272323608398
('orphans', 'bus', 'stop', 'gets', 'half', 'years')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (33, 'years') has the max prob in order to be the next word. The prob is: 0.02834049053490162
901300 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.0499])
The negative loglikelihood (-log p(years)): 3.5634636878967285 or 3.5634636878967285 or 3.5634636878967285
('called', 'woolwich', 'shooting', 'live', 'updates', 'witnesses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2290, 'witnesses') has the max prob in order to be the next word. The prob is: 0.0003533294366206974
901400 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0738])
The negative loglikelihood (-log p(witnesses)): 7.9481096267700195 or 7.9481096267700195 or 7.9481096267700195
('death', 'fascism', 'day', 'historian', 'dominique', 'venner')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44440, 'venner') has the max prob in order to be the next word. The prob is: 3.5068035231233807e-06
901500 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0209])
The negative loglikelihood (-log p(venner)): 12.560805320739746 or 12.560805320739746 or 12.560805320739746
('british', 'columbia', 'gives', 'enrollment', 'priority', 'experienced')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6183, 'experienced') has the max prob in order to be the next word. The prob is: 1.2656979379244149e-05
901600 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0161])
The negative loglikelihood (-log p(experienced)): 11.277301788330078 or 11.277301788330078 or 11.277301788330078
('man', 'talks', 'camera', 'bloody', 'hands', 'hacking')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1073, 'hacking') has the max prob in order to be the next word. The prob is: 5.05905745740165e-06
901700 The nn predicts (tensor([31]), 'two') the next word with max prob: tensor([0.0141])
The negative loglikelihood (-log p(hacking)): 12.194330215454102 or 12.194330215454102 or 12.194330215454102
('man', 'hacked', 'death', 'london', 'street', 'suspected')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (241, 'suspected') has the max prob in order to be the next word. The prob is: 0.0008415449410676956
901800 The nn predicts (tensor([207]), 'london') the next word with max prob: tensor([0.0328])
The negative loglikelihood (-log p(suspected)): 7.080271244049072 or 7.080271244049072 or 7.080271244049072
('video', 'capture', 'paranormal', 'activity', 'accidentally', 'films')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5434, 'films') has the max prob in order to be the next word. The prob is: 2.878477971535176e-05
901900 The nn predicts (tensor([197]), 'men') the next word with max prob: tensor([0.0119])
The negative loglikelihood (-log p(films)): 10.455663681030273 or 10.455663681030273 or 10.455663681030273
('five', 'men', 'surveillance', 'libya', 'wanted', 'questioning')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3954, 'questioning') has the max prob in order to be the next word. The prob is: 8.006239659152925e-05
902000 The nn predicts (tensor([128]), 'kill') the next word with max prob: tensor([0.0076])
The negative loglikelihood (-log p(questioning)): 9.432703971862793 or 9.432703971862793 or 9.432703971862793
('islamic', 'centre', 'second', 'attack', 'mosque', 'area')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (686, 'area') has the max prob in order to be the next word. The prob is: 0.000731274951249361
902100 The nn predicts (tensor([207]), 'london') the next word with max prob: tensor([0.0420])
The negative loglikelihood (-log p(area)): 7.220721244812012 or 7.220721244812012 or 7.220721244812012
('obama', 'reaffirms', 'hope', 'stronger', 'ties', 'russia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6, 'russia') has the max prob in order to be the next word. The prob is: 0.006897134240716696
902200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1659])
The negative loglikelihood (-log p(russia)): 4.976649284362793 or 4.976649284362793 or 4.976649284362793
('news', 'woolwich', 'machete', 'attack', 'killed', 'man')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (56, 'man') has the max prob in order to be the next word. The prob is: 0.020852815359830856
902300 The nn predicts (tensor([31]), 'two') the next word with max prob: tensor([0.0462])
The negative loglikelihood (-log p(man)): 3.8702664375305176 or 3.8702661991119385 or 3.8702661991119385
('twitter', 'facebook', 'security', 'patent', 'authentication', 'system')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (333, 'system') has the max prob in order to be the next word. The prob is: 0.0006061046151444316
902400 The nn predicts (tensor([600]), 'access') the next word with max prob: tensor([0.0058])
The negative loglikelihood (-log p(system)): 7.4084577560424805 or 7.4084577560424805 or 7.4084577560424805
('firefights', 'one', 'outside', 'military', 'barracks', 'whilst')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10315, 'whilst') has the max prob in order to be the next word. The prob is: 1.2200805940665305e-05
902500 The nn predicts (tensor([842]), 'province') the next word with max prob: tensor([0.0246])
The negative loglikelihood (-log p(whilst)): 11.314008712768555 or 11.314008712768555 or 11.314008712768555
('malaysian', 'govt', 'begins', 'crackdown', 'opposition', 'mps')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (934, 'mps') has the max prob in order to be the next word. The prob is: 0.00028153869789093733
902600 The nn predicts (tensor([118]), 'party') the next word with max prob: tensor([0.0495])
The negative loglikelihood (-log p(mps)): 8.175240516662598 or 8.175240516662598 or 8.175240516662598
('discovered', 'female', 'criminal', 'gang', 'running', 'honeypot')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (27448, 'honeypot') has the max prob in order to be the next word. The prob is: 1.0401543022453552e-06
902700 The nn predicts (tensor([56]), 'man') the next word with max prob: tensor([0.0240])
The negative loglikelihood (-log p(honeypot)): 13.776141166687012 or 13.776141166687012 or 13.776141166687012
('confirms', 'worst', 'thoughts', 'said', 'sir', 'david')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (498, 'david') has the max prob in order to be the next word. The prob is: 0.0005933269276283681
902800 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0137])
The negative loglikelihood (-log p(david)): 7.429765224456787 or 7.429765224456787 or 7.429765224456787
('proposed', 'world', 'heritage', 'site', 'requires', 'clearing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5059, 'clearing') has the max prob in order to be the next word. The prob is: 2.8148157070972957e-05
902900 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0102])
The negative loglikelihood (-log p(clearing)): 10.478028297424316 or 10.478028297424316 or 10.478028297424316
('israeli', 'sentenced', 'jail', 'time', 'refusal', 'serve')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2952, 'serve') has the max prob in order to be the next word. The prob is: 0.00041855144081637263
903000 The nn predicts (tensor([733]), 'allow') the next word with max prob: tensor([0.0123])
The negative loglikelihood (-log p(serve)): 7.778710842132568 or 7.778710842132568 or 7.778710842132568
('auschwitz', 'nazi', 'death', 'camp', 'pray', 'holocaust')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1354, 'holocaust') has the max prob in order to be the next word. The prob is: 0.0007981216185726225
903100 The nn predicts (tensor([207]), 'london') the next word with max prob: tensor([0.0450])
The negative loglikelihood (-log p(holocaust)): 7.133249759674072 or 7.133249759674072 or 7.133249759674072
('fungus', 'currently', 'threatens', 'ruin', 'much', 'percent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (497, 'percent') has the max prob in order to be the next word. The prob is: 0.0019123792881146073
903200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0195])
The negative loglikelihood (-log p(percent)): 6.259407043457031 or 6.259407043457031 or 6.259407043457031
('greenwald', 'london', 'killing', 'british', 'soldier', 'terrorism')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (454, 'terrorism') has the max prob in order to be the next word. The prob is: 0.0005676030414178967
903300 The nn predicts (tensor([207]), 'london') the next word with max prob: tensor([0.0307])
The negative loglikelihood (-log p(terrorism)): 7.474088191986084 or 7.474088191986084 or 7.474088191986084
('legalisation', 'grow', 'number', 'rhinos', 'poached', 'increasing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1930, 'increasing') has the max prob in order to be the next word. The prob is: 0.0003272787434980273
903400 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.0707])
The negative loglikelihood (-log p(increasing)): 8.024698257446289 or 8.024698257446289 or 8.024698257446289
('quite', 'close', 'airbus', 'close', 'encounter', 'ufo')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5193, 'ufo') has the max prob in order to be the next word. The prob is: 1.336596869805362e-05
903500 The nn predicts (tensor([40]), 'japan') the next word with max prob: tensor([0.0187])
The negative loglikelihood (-log p(ufo)): 11.222798347473145 or 11.222798347473145 or 11.222798347473145
('<s>', 'magnitude', 'earthquake', 'sea', 'north', 'japan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (40, 'japan') has the max prob in order to be the next word. The prob is: 0.05829757824540138
903600 The nn predicts (tensor([22]), 'korea') the next word with max prob: tensor([0.1711])
The negative loglikelihood (-log p(japan)): 2.8421947956085205 or 2.8421947956085205 or 2.8421947956085205
('health', 'organisation', 'urged', 'countries', 'possible', 'cases')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (668, 'cases') has the max prob in order to be the next word. The prob is: 0.0011022633407264948
903700 The nn predicts (tensor([372]), 'action') the next word with max prob: tensor([0.0158])
The negative loglikelihood (-log p(cases)): 6.810389518737793 or 6.810389518737793 or 6.810389518737793
('woolwich', 'attacker', 'eye', 'eye', 'tooth', 'tooth')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9589, 'tooth') has the max prob in order to be the next word. The prob is: 1.448947841709014e-05
903800 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0282])
The negative loglikelihood (-log p(tooth)): 11.142087936401367 or 11.142087936401367 or 11.142087936401367
('capable', 'spreading', 'human', 'human', 'transmitted', 'direct')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1720, 'direct') has the max prob in order to be the next word. The prob is: 4.6072538680164143e-05
903900 The nn predicts (tensor([51]), 'report') the next word with max prob: tensor([0.0338])
The negative loglikelihood (-log p(direct)): 9.9852933883667 or 9.9852933883667 or 9.9852933883667
('within', 'european', 'union', 'becomes', 'automatically', 'new')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4, 'new') has the max prob in order to be the next word. The prob is: 0.007556123659014702
904000 The nn predicts (tensor([312]), 'bill') the next word with max prob: tensor([0.0121])
The negative loglikelihood (-log p(new)): 4.885396957397461 or 4.885396957397461 or 4.885396957397461
('passengers', 'pakistan', 'england', 'plane', 'landed', 'stansted')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (21454, 'stansted') has the max prob in order to be the next word. The prob is: 3.7359648104029475e-06
904100 The nn predicts (tensor([111]), 'fire') the next word with max prob: tensor([0.0332])
The negative loglikelihood (-log p(stansted)): 12.497504234313965 or 12.497504234313965 or 12.497504234313965
('bad', 'army', 'dispatch', 'troops', 'quell', 'violence')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (141, 'violence') has the max prob in order to be the next word. The prob is: 0.006300932262092829
904200 The nn predicts (tensor([273]), 'assad') the next word with max prob: tensor([0.0171])
The negative loglikelihood (-log p(violence)): 5.0670576095581055 or 5.0670576095581055 or 5.0670576095581055
('afghan', 'university', 'students', 'protest', 'women', 'rights')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (67, 'rights') has the max prob in order to be the next word. The prob is: 0.03849899023771286
904300 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.0385])
The negative loglikelihood (-log p(rights)): 3.2571232318878174 or 3.2571232318878174 or 3.2571232318878174
('outstanding', 'courage', 'confront', 'killers', 'pray', 'slaughtered')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4873, 'slaughtered') has the max prob in order to be the next word. The prob is: 4.32680499216076e-05
904400 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0156])
The negative loglikelihood (-log p(slaughtered)): 10.048095703125 or 10.048095703125 or 10.048095703125
('allegedly', 'unlawful', 'killings', 'civilians', 'british', 'military')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18, 'military') has the max prob in order to be the next word. The prob is: 0.01107480376958847
904500 The nn predicts (tensor([576]), 'soldier') the next word with max prob: tensor([0.0763])
The negative loglikelihood (-log p(military)): 4.503082752227783 or 4.503082752227783 or 4.503082752227783
('maduro', 'says', 'venezuela', 'create', 'new', 'workers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (209, 'workers') has the max prob in order to be the next word. The prob is: 0.0005340205389074981
904600 The nn predicts (tensor([1053]), 'york') the next word with max prob: tensor([0.0452])
The negative loglikelihood (-log p(workers)): 7.535076141357422 or 7.535076141357422 or 7.535076141357422
('us', 'criticised', 'called', 'attempts', 'undermine', 'peace')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (127, 'peace') has the max prob in order to be the next word. The prob is: 0.013807405717670918
904700 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0375])
The negative loglikelihood (-log p(peace)): 4.28255033493042 or 4.28255033493042 or 4.28255033493042
('disintegrating', 'alarming', 'rate', 'giving', 'station', 'little')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1017, 'little') has the max prob in order to be the next word. The prob is: 0.0002814539475366473
904800 The nn predicts (tensor([31]), 'two') the next word with max prob: tensor([0.0123])
The negative loglikelihood (-log p(little)): 8.175541877746582 or 8.175541877746582 or 8.175541877746582
('top', 'priority', 'resisting', 'west', 'spreading', 'islam')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (720, 'islam') has the max prob in order to be the next word. The prob is: 0.0012417378602549434
904900 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0278])
The negative loglikelihood (-log p(islam)): 6.6912431716918945 or 6.6912431716918945 or 6.6912431716918945
('believing', 'pregnant', 'give', 'factory', 'made', 'babies')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1826, 'babies') has the max prob in order to be the next word. The prob is: 0.0003364698204677552
905000 The nn predicts (tensor([20]), 'uk') the next word with max prob: tensor([0.0103])
The negative loglikelihood (-log p(babies)): 7.997002124786377 or 7.997002124786377 or 7.997002124786377
('farmer', 'lost', 'hands', 'freak', 'accident', 'built')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1425, 'built') has the max prob in order to be the next word. The prob is: 0.00025789960636757314
905100 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0204])
The negative loglikelihood (-log p(built)): 8.262940406799316 or 8.262940406799316 or 8.262940406799316
('making', 'racist', 'comments', 'facebook', 'twitter', 'british')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (63, 'british') has the max prob in order to be the next word. The prob is: 0.0004217350797262043
905200 The nn predicts (tensor([581]), 'twitter') the next word with max prob: tensor([0.0274])
The negative loglikelihood (-log p(british)): 7.7711334228515625 or 7.7711334228515625 or 7.7711334228515625
('reality', 'europe', 'leaders', 'sleepwalking', 'economic', 'wasteland')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15944, 'wasteland') has the max prob in order to be the next word. The prob is: 1.7400540173184709e-06
905300 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.2217])
The negative loglikelihood (-log p(wasteland)): 13.261594772338867 or 13.261594772338867 or 13.261594772338867
('stockholm', 'sweden', 'security', 'message', 'citizens', 'pdf')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7854, 'pdf') has the max prob in order to be the next word. The prob is: 2.2134463506517932e-05
905400 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0211])
The negative loglikelihood (-log p(pdf)): 10.718374252319336 or 10.718374252319336 or 10.718374252319336
('fight', 'syria', 'involves', 'people', 'different', 'countries')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (168, 'countries') has the max prob in order to be the next word. The prob is: 0.010745198465883732
905500 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0183])
The negative loglikelihood (-log p(countries)): 4.53329610824585 or 4.53329610824585 or 4.53329610824585
('china', 'launches', 'piranha', 'hunt', 'attacks', 'telegraph')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1431, 'telegraph') has the max prob in order to be the next word. The prob is: 0.00031178040080703795
905600 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0255])
The negative loglikelihood (-log p(telegraph)): 8.073211669921875 or 8.073211669921875 or 8.073211669921875
('egypt', 'believed', 'affairs', 'latest', 'apparent', 'example')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3508, 'example') has the max prob in order to be the next word. The prob is: 4.6136381570249796e-05
905700 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.1146])
The negative loglikelihood (-log p(example)): 9.983908653259277 or 9.983908653259277 or 9.983908653259277
('rebels', 'future', 'plans', 'president', 'omar', 'hassan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3281, 'hassan') has the max prob in order to be the next word. The prob is: 5.7161549193551764e-05
905800 The nn predicts (tensor([1088]), 'bashar') the next word with max prob: tensor([0.0438])
The negative loglikelihood (-log p(hassan)): 9.76962947845459 or 9.76962947845459 or 9.76962947845459
('mostly', 'foreign', 'nationals', 'violence', 'orchestrated', 'outside')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (548, 'outside') has the max prob in order to be the next word. The prob is: 0.00043934248969890177
905900 The nn predicts (tensor([141]), 'violence') the next word with max prob: tensor([0.0237])
The negative loglikelihood (-log p(outside)): 7.730231285095215 or 7.730231285095215 or 7.730231285095215
('says', 'fighters', 'committed', 'syria', 'war', 'end')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (137, 'end') has the max prob in order to be the next word. The prob is: 0.0007841903134249151
906000 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1737])
The negative loglikelihood (-log p(end)): 7.1508588790893555 or 7.1508588790893555 or 7.1508588790893555
('native', 'population', 'complains', 'cancerous', 'growths', 'aching')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (63142, 'aching') has the max prob in order to be the next word. The prob is: 3.979595476266695e-06
906100 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0105])
The negative loglikelihood (-log p(aching)): 12.434329986572266 or 12.434329986572266 or 12.434329986572266
('opposition', 'unity', 'stalled', 'peace', 'talks', 'jpost')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7719, 'jpost') has the max prob in order to be the next word. The prob is: 9.322355253971182e-06
906200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0604])
The negative loglikelihood (-log p(jpost)): 11.58309555053711 or 11.58309555053711 or 11.58309555053711
('guard', 'converts', 'islam', 'living', 'faith', 'muslim')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (166, 'muslim') has the max prob in order to be the next word. The prob is: 0.010093818418681622
906300 The nn predicts (tensor([720]), 'islam') the next word with max prob: tensor([0.0197])
The negative loglikelihood (-log p(muslim)): 4.595831871032715 or 4.595831871032715 or 4.595831871032715
('chairman', 'schmidt', 'help', 'uk', 'gov', 'censor')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5097, 'censor') has the max prob in order to be the next word. The prob is: 3.0215151127777062e-05
906400 The nn predicts (tensor([114]), 'chief') the next word with max prob: tensor([0.0209])
The negative loglikelihood (-log p(censor)): 10.407167434692383 or 10.407167434692383 or 10.407167434692383
('websites', 'reports', 'region', 'world', 'ahram', 'online')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (338, 'online') has the max prob in order to be the next word. The prob is: 0.04824123904109001
906500 The nn predicts (tensor([338]), 'online') the next word with max prob: tensor([0.0482])
The negative loglikelihood (-log p(online)): 3.031541109085083 or 3.031541109085083 or 3.031541109085083
('attack', 'cameron', 'fire', 'terrorism', 'blunders', 'emerge')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3412, 'emerge') has the max prob in order to be the next word. The prob is: 5.1129791245330125e-05
906600 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0149])
The negative loglikelihood (-log p(emerge)): 9.881143569946289 or 9.881143569946289 or 9.881143569946289
('oil', 'minister', 'hani', 'hussein', 'resigns', 'coming')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (994, 'coming') has the max prob in order to be the next word. The prob is: 7.535391341662034e-05
906700 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(coming)): 9.493314743041992 or 9.493314743041992 or 9.493314743041992
('brazil', 'signed', 'air', 'services', 'agreement', 'asa')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19636, 'asa') has the max prob in order to be the next word. The prob is: 1.0027034704762627e-06
906800 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0260])
The negative loglikelihood (-log p(asa)): 13.812810897827148 or 13.812810897827148 or 13.812810897827148
('although', 'stories', 'hear', 'violence', 'probably', 'true')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2440, 'true') has the max prob in order to be the next word. The prob is: 0.00031210578163154423
906900 The nn predicts (tensor([37]), 'one') the next word with max prob: tensor([0.0204])
The negative loglikelihood (-log p(true)): 8.072168350219727 or 8.072168350219727 or 8.072168350219727
('uses', 'chemical', 'weapons', 'syria', 'according', 'french')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (70, 'french') has the max prob in order to be the next word. The prob is: 0.0009568258537910879
907000 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.0536])
The negative loglikelihood (-log p(french)): 6.9518890380859375 or 6.9518890380859375 or 6.9518890380859375
('le', 'monde', 'witness', 'chemical', 'warfare', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.03497377783060074
907100 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0350])
The negative loglikelihood (-log p(syria)): 3.353156805038452 or 3.353156805038452 or 3.353156805038452
('thousands', 'continue', 'rampage', 'across', 'paris', 'gay')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (251, 'gay') has the max prob in order to be the next word. The prob is: 0.00028967077378183603
907200 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0639])
The negative loglikelihood (-log p(gay)): 8.14676570892334 or 8.14676570892334 or 8.14676570892334
('railway', 'company', 'deutsche', 'bahn', 'plans', 'test')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (444, 'test') has the max prob in order to be the next word. The prob is: 0.0005835140473209321
907300 The nn predicts (tensor([294]), 'pay') the next word with max prob: tensor([0.0362])
The negative loglikelihood (-log p(test)): 7.446442127227783 or 7.446442127227783 or 7.446442127227783
('muslims', 'fear', 'backlash', 'uk', 'soldier', 'killing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (150, 'killing') has the max prob in order to be the next word. The prob is: 0.00537196034565568
907400 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0331])
The negative loglikelihood (-log p(killing)): 5.2265625 or 5.2265625 or 5.2265625
('tea', 'involves', 'thick', 'scone', 'jam', 'spread')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (953, 'spread') has the max prob in order to be the next word. The prob is: 0.0006014725076965988
907500 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0092])
The negative loglikelihood (-log p(spread)): 7.4161295890808105 or 7.4161295890808105 or 7.4161295890808105
('prisoner', 'lashed', 'hanged', 'latest', 'savage', 'execution')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1468, 'execution') has the max prob in order to be the next word. The prob is: 0.00018930896476376802
907600 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0731])
The negative loglikelihood (-log p(execution)): 8.57213020324707 or 8.57213020324707 or 8.57213020324707
('service', 'country', 'operation', 'enduring', 'freedom', 'operation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (595, 'operation') has the max prob in order to be the next word. The prob is: 0.00040515352156944573
907700 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0121])
The negative loglikelihood (-log p(operation)): 7.811244487762451 or 7.811244487762451 or 7.811244487762451
('british', 'citizen', 'receiving', 'complaints', 'speech', 'broadcast')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3490, 'broadcast') has the max prob in order to be the next word. The prob is: 0.00012697414786089212
907800 The nn predicts (tensor([569]), 'saying') the next word with max prob: tensor([0.0132])
The negative loglikelihood (-log p(broadcast)): 8.971527099609375 or 8.971527099609375 or 8.971527099609375
('breaks', 'cruise', 'ship', 'passengers', 'sent', 'evacuation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2478, 'evacuation') has the max prob in order to be the next word. The prob is: 0.0003226756234653294
907900 The nn predicts (tensor([111]), 'fire') the next word with max prob: tensor([0.0546])
The negative loglikelihood (-log p(evacuation)): 8.038863182067871 or 8.038863182067871 or 8.038863182067871
('lift', 'arms', 'embargo', 'syrian', 'opposition', 'maintaining')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7917, 'maintaining') has the max prob in order to be the next word. The prob is: 1.2096011232642923e-05
908000 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.0518])
The negative loglikelihood (-log p(maintaining)): 11.32263469696045 or 11.32263469696045 or 11.32263469696045
('member', 'uruguay', 'special', 'olympics', 'swim', 'team')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (628, 'team') has the max prob in order to be the next word. The prob is: 0.0005134570528753102
908100 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0100])
The negative loglikelihood (-log p(team)): 7.574344158172607 or 7.574344158172607 or 7.574344158172607
('lawyer', 'wins', 'firm', 'hire', 'unusually', 'vague')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11642, 'vague') has the max prob in order to be the next word. The prob is: 9.33747742237756e-06
908200 The nn predicts (tensor([63]), 'british') the next word with max prob: tensor([0.0080])
The negative loglikelihood (-log p(vague)): 11.581474304199219 or 11.581474304199219 or 11.581474304199219
('army', 'fsa', 'raided', 'village', 'reef', 'outskirts')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5262, 'outskirts') has the max prob in order to be the next word. The prob is: 0.0001688380871200934
908300 The nn predicts (tensor([112]), 'near') the next word with max prob: tensor([0.0336])
The negative loglikelihood (-log p(outskirts)): 8.686570167541504 or 8.686570167541504 or 8.686570167541504
('israeli', 'haredim', 'rebel', 'army', 'draft', 'plan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (169, 'plan') has the max prob in order to be the next word. The prob is: 0.003399849869310856
908400 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.1009])
The negative loglikelihood (-log p(plan)): 5.684023857116699 or 5.684023857116699 or 5.684023857116699
('swine', 'flu', 'outbreak', 'venezuela', 'dead', 'infected')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2802, 'infected') has the max prob in order to be the next word. The prob is: 0.00019615566998254508
908500 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0248])
The negative loglikelihood (-log p(infected)): 8.536602020263672 or 8.536602020263672 or 8.536602020263672
('adviser', 'tom', 'donilon', 'pushed', 'tuesday', 'stronger')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3568, 'stronger') has the max prob in order to be the next word. The prob is: 5.6812270486261696e-05
908600 The nn predicts (tensor([295]), 'monday') the next word with max prob: tensor([0.0260])
The negative loglikelihood (-log p(stronger)): 9.775757789611816 or 9.775757789611816 or 9.775757789611816
('deter', 'southern', 'insurgents', 'moving', 'freely', 'across')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (266, 'across') has the max prob in order to be the next word. The prob is: 0.0006225422257557511
908700 The nn predicts (tensor([76]), 'army') the next word with max prob: tensor([0.0304])
The negative loglikelihood (-log p(across)): 7.381699085235596 or 7.381699085235596 or 7.381699085235596
('eu', 'lift', 'embargo', 'syria', 'rebel', 'arms')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (314, 'arms') has the max prob in order to be the next word. The prob is: 0.006638931576162577
908800 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0843])
The negative loglikelihood (-log p(arms)): 5.014804363250732 or 5.014804363250732 or 5.014804363250732
('china', 'steals', 'new', 'australia', 'spy', 'agency')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (304, 'agency') has the max prob in order to be the next word. The prob is: 0.03000255487859249
908900 The nn predicts (tensor([304]), 'agency') the next word with max prob: tensor([0.0300])
The negative loglikelihood (-log p(agency)): 3.5064728260040283 or 3.5064728260040283 or 3.5064728260040283
('rebels', 'go', 'away', 'believe', 'govt', 'description')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11424, 'description') has the max prob in order to be the next word. The prob is: 1.4136311619949993e-05
909000 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0197])
The negative loglikelihood (-log p(description)): 11.166764259338379 or 11.166763305664062 or 11.166763305664062
('applicants', 'included', 'women', 'representatives', 'country', 'ethnic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1352, 'ethnic') has the max prob in order to be the next word. The prob is: 0.0003757882514037192
909100 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0184])
The negative loglikelihood (-log p(ethnic)): 7.886484622955322 or 7.886484622955322 or 7.886484622955322
('also', 'best', 'fabricating', 'phony', 'dollar', 'bills')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3194, 'bills') has the max prob in order to be the next word. The prob is: 0.0005478733801282942
909200 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0165])
The negative loglikelihood (-log p(bills)): 7.509466171264648 or 7.509466171264648 or 7.509466171264648
('chile', 'argentina', 'order', 'evacuation', 'around', 'stirring')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9166, 'stirring') has the max prob in order to be the next word. The prob is: 2.2100450223661028e-05
909300 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0535])
The negative loglikelihood (-log p(stirring)): 10.7199125289917 or 10.7199125289917 or 10.7199125289917
('warns', 'moscow', 'attack', 'shipments', 'russian', 'weapons')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (149, 'weapons') has the max prob in order to be the next word. The prob is: 0.022335385903716087
909400 The nn predicts (tensor([103]), 'border') the next word with max prob: tensor([0.0396])
The negative loglikelihood (-log p(weapons)): 3.8015830516815186 or 3.8015830516815186 or 3.8015830516815186
('haftbefehl', 'gesucht', 'wird', 'bitte', 'schaut', 'auf')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19209, 'auf') has the max prob in order to be the next word. The prob is: 4.92971730636782e-06
909500 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0061])
The negative loglikelihood (-log p(auf)): 12.220229148864746 or 12.220229148864746 or 12.220229148864746
('russian', 'plan', 'give', 'missiles', 'syria', 'assad')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (273, 'assad') has the max prob in order to be the next word. The prob is: 0.04503706097602844
909600 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1000])
The negative loglikelihood (-log p(assad)): 3.1002695560455322 or 3.1002695560455322 or 3.1002695560455322
('news', 'york', 'mosque', 'praised', 'offering', 'edl')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11566, 'edl') has the max prob in order to be the next word. The prob is: 4.668043857236626e-06
909700 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0188])
The negative loglikelihood (-log p(edl)): 12.274770736694336 or 12.274770736694336 or 12.274770736694336
('taiwan', 'proposes', 'block', 'overseas', 'internet', 'services')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (919, 'services') has the max prob in order to be the next word. The prob is: 0.0034660985693335533
909800 The nn predicts (tensor([227]), 'trade') the next word with max prob: tensor([0.0181])
The negative loglikelihood (-log p(services)): 5.6647257804870605 or 5.6647257804870605 or 5.6647257804870605
('cuba', 'open', 'public', 'internet', 'salons', 'islandwide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (63216, 'islandwide') has the max prob in order to be the next word. The prob is: 1.1796688568210811e-06
909900 The nn predicts (tensor([338]), 'online') the next word with max prob: tensor([0.0178])
The negative loglikelihood (-log p(islandwide)): 13.650277137756348 or 13.650277137756348 or 13.650277137756348
('exchange', 'liberty', 'reserve', 'accusing', 'costa', 'company')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (353, 'company') has the max prob in order to be the next word. The prob is: 0.004708250518888235
910000 The nn predicts (tensor([66]), 'oil') the next word with max prob: tensor([0.0048])
The negative loglikelihood (-log p(company)): 5.358438968658447 or 5.358438968658447 or 5.358438968658447
('walk', 'without', 'stepping', 'bodies', 'either', 'slaughtered')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4873, 'slaughtered') has the max prob in order to be the next word. The prob is: 9.762223635334522e-05
910100 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0246])
The negative loglikelihood (-log p(slaughtered)): 9.234405517578125 or 9.234405517578125 or 9.234405517578125
('news', 'sources', 'foreign', 'local', 'mandatory', 'performance')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5378, 'performance') has the max prob in order to be the next word. The prob is: 6.914634286658838e-05
910200 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0192])
The negative loglikelihood (-log p(performance)): 9.579285621643066 or 9.579285621643066 or 9.579285621643066
('weapons', 'deploys', 'air', 'defense', 'israel', 'warns')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (146, 'warns') has the max prob in order to be the next word. The prob is: 0.001030846149660647
910300 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0550])
The negative loglikelihood (-log p(warns)): 6.87737512588501 or 6.87737512588501 or 6.87737512588501
('jamaica', 'opposition', 'spokesman', 'industry', 'commerce', 'energy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (307, 'energy') has the max prob in order to be the next word. The prob is: 0.002962111495435238
910400 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.1027])
The negative loglikelihood (-log p(energy)): 5.821852684020996 or 5.821852684020996 or 5.821852684020996
('west', 'bengal', 'teen', 'kills', 'allegedly', 'beaten')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1500, 'beaten') has the max prob in order to be the next word. The prob is: 0.00044188409810885787
910500 The nn predicts (tensor([576]), 'soldier') the next word with max prob: tensor([0.0308])
The negative loglikelihood (-log p(beaten)): 7.724462985992432 or 7.724462985992432 or 7.724462985992432
('egypt', 'morsi', 'appoints', 'nine', 'islamists', 'key')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (455, 'key') has the max prob in order to be the next word. The prob is: 0.00014370617282111198
910600 The nn predicts (tensor([166]), 'muslim') the next word with max prob: tensor([0.0162])
The negative loglikelihood (-log p(key)): 8.847740173339844 or 8.847740173339844 or 8.847740173339844
('forces', 'detaining', 'dozens', 'afghanistan', 'philip', 'hammond')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6732, 'hammond') has the max prob in order to be the next word. The prob is: 4.9580653467273805e-06
910700 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0290])
The negative loglikelihood (-log p(hammond)): 12.214494705200195 or 12.214494705200195 or 12.214494705200195
('bbc', 'news', 'mount', 'everest', 'base', 'jump')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4173, 'jump') has the max prob in order to be the next word. The prob is: 4.519278445513919e-05
910800 The nn predicts (tensor([111]), 'fire') the next word with max prob: tensor([0.0186])
The negative loglikelihood (-log p(jump)): 10.004572868347168 or 10.004572868347168 or 10.004572868347168
('become', 'focal', 'point', 'broad', 'battle', 'abortion')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1415, 'abortion') has the max prob in order to be the next word. The prob is: 8.583810267737135e-05
910900 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0303])
The negative loglikelihood (-log p(abortion)): 9.36304759979248 or 9.36304759979248 or 9.36304759979248
('flashpoint', 'south', 'china', 'sea', 'china', 'five')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (225, 'five') has the max prob in order to be the next word. The prob is: 4.703822560259141e-05
911000 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.1324])
The negative loglikelihood (-log p(five)): 9.964550018310547 or 9.964550018310547 or 9.964550018310547
('pipe', 'stunning', 'ordeal', 'caught', 'video', 'raised')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1889, 'raised') has the max prob in order to be the next word. The prob is: 9.703503747005016e-05
911100 The nn predicts (tensor([100]), 'video') the next word with max prob: tensor([0.0601])
The negative loglikelihood (-log p(raised)): 9.240438461303711 or 9.240438461303711 or 9.240438461303711
('halting', 'money', 'transfers', 'foreign', 'humanitarian', 'groups')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (327, 'groups') has the max prob in order to be the next word. The prob is: 0.0010559766087681055
911200 The nn predicts (tensor([149]), 'weapons') the next word with max prob: tensor([0.0338])
The negative loglikelihood (-log p(groups)): 6.8532891273498535 or 6.8532891273498535 or 6.8532891273498535
('laws', 'form', 'torture', 'say', 'rights', 'groups')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (327, 'groups') has the max prob in order to be the next word. The prob is: 0.13241156935691833
911300 The nn predicts (tensor([327]), 'groups') the next word with max prob: tensor([0.1324])
The negative loglikelihood (-log p(groups)): 2.0218403339385986 or 2.0218403339385986 or 2.0218403339385986
('bastion', 'detentions', 'latest', 'mod', 'set', 'return')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (432, 'return') has the max prob in order to be the next word. The prob is: 0.0008860493544489145
911400 The nn predicts (tensor([111]), 'fire') the next word with max prob: tensor([0.0429])
The negative loglikelihood (-log p(return)): 7.028738021850586 or 7.028738021850586 or 7.028738021850586
('say', 'man', 'pencil', 'head', 'years', 'reminiscent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10331, 'reminiscent') has the max prob in order to be the next word. The prob is: 4.98484314448433e-06
911500 The nn predicts (tensor([608]), 'ago') the next word with max prob: tensor([0.1473])
The negative loglikelihood (-log p(reminiscent)): 12.209108352661133 or 12.209108352661133 or 12.209108352661133
('nasa', 'apollo', 'navigator', 'found', 'dead', 'rope')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11610, 'rope') has the max prob in order to be the next word. The prob is: 2.3266404241439886e-05
911600 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0108])
The negative loglikelihood (-log p(rope)): 10.668499946594238 or 10.668499946594238 or 10.668499946594238
('activists', 'arrested', 'tunisia', 'staged', 'imprisonment', 'fellow')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3648, 'fellow') has the max prob in order to be the next word. The prob is: 7.884741353336722e-05
911700 The nn predicts (tensor([91]), 'protest') the next word with max prob: tensor([0.0229])
The negative loglikelihood (-log p(fellow)): 9.447996139526367 or 9.447996139526367 or 9.447996139526367
('wednesday', 'strain', 'genetically', 'engineered', 'wheat', 'discovered')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (675, 'discovered') has the max prob in order to be the next word. The prob is: 0.0006064024637453258
911800 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0124])
The negative loglikelihood (-log p(discovered)): 7.407966613769531 or 7.407966613769531 or 7.407966613769531
('worker', 'al', 'qaeda', 'prompted', 'moktar', 'belmoktar')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (37238, 'belmoktar') has the max prob in order to be the next word. The prob is: 2.6640539090294624e-06
911900 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0245])
The negative loglikelihood (-log p(belmoktar)): 12.835661888122559 or 12.835661888122559 or 12.835661888122559
('lama', 'itigelov', 'living', 'dead', 'buddhist', 'monk')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5020, 'monk') has the max prob in order to be the next word. The prob is: 4.61224808532279e-05
912000 The nn predicts (tensor([407]), 'muslims') the next word with max prob: tensor([0.0118])
The negative loglikelihood (-log p(monk)): 9.984210014343262 or 9.984210014343262 or 9.984210014343262
('prime', 'minister', 'house', 'size', 'cabinet', 'exceed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6862, 'exceed') has the max prob in order to be the next word. The prob is: 2.3300390239455737e-05
912100 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0517])
The negative loglikelihood (-log p(exceed)): 10.667040824890137 or 10.667040824890137 or 10.667040824890137
('budovsky', 'may', 'paid', 'marry', 'costa', 'rican')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9849, 'rican') has the max prob in order to be the next word. The prob is: 4.600576176017057e-06
912200 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0111])
The negative loglikelihood (-log p(rican)): 12.289328575134277 or 12.289328575134277 or 12.289328575134277
('europe', 'aims', 'end', 'mobile', 'roaming', 'fees')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3808, 'fees') has the max prob in order to be the next word. The prob is: 0.000579913379624486
912300 The nn predicts (tensor([130]), 'ban') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(fees)): 7.452631950378418 or 7.452631950378418 or 7.452631950378418
('victor', 'orbán', 'trampled', 'european', 'union', 'values')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3118, 'values') has the max prob in order to be the next word. The prob is: 0.00017004444089252502
912400 The nn predicts (tensor([78]), 'europe') the next word with max prob: tensor([0.0182])
The negative loglikelihood (-log p(values)): 8.679450988769531 or 8.679450988769531 or 8.679450988769531
('home', 'bacon', 'smithfield', 'deal', 'concerns', 'production')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1211, 'production') has the max prob in order to be the next word. The prob is: 0.0007077149930410087
912500 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0206])
The negative loglikelihood (-log p(production)): 7.253468990325928 or 7.253468990325928 or 7.253468990325928
('color', 'purple', 'alicia', 'keys', 'boycott', 'palestinian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (99, 'palestinian') has the max prob in order to be the next word. The prob is: 0.012612072750926018
912600 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0166])
The negative loglikelihood (-log p(palestinian)): 4.373100757598877 or 4.373100757598877 or 4.373100757598877
('sexual', 'revolution', 'could', 'eventually', 'topple', 'regime')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (422, 'regime') has the max prob in order to be the next word. The prob is: 0.03445474058389664
912700 The nn predicts (tensor([422]), 'regime') the next word with max prob: tensor([0.0345])
The negative loglikelihood (-log p(regime)): 3.3681087493896484 or 3.3681087493896484 or 3.3681087493896484
('traffic', 'jam', 'summit', 'mount', 'everest', 'number')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (445, 'number') has the max prob in order to be the next word. The prob is: 0.00022731289209332317
912800 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0144])
The negative loglikelihood (-log p(number)): 8.389183044433594 or 8.389183044433594 or 8.389183044433594
('agreed', 'drastically', 'reform', 'fishing', 'policy', 'thursday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (385, 'thursday') has the max prob in order to be the next word. The prob is: 0.00012963359768036753
912900 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0154])
The negative loglikelihood (-log p(thursday)): 8.950798988342285 or 8.950798988342285 or 8.950798988342285
('better', 'conditions', 'autonomy', 'sex', 'workers', 'worked')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2866, 'worked') has the max prob in order to be the next word. The prob is: 0.00013909715926274657
913000 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0288])
The negative loglikelihood (-log p(worked)): 8.880337715148926 or 8.880337715148926 or 8.880337715148926
('czech', 'mother', 'defies', 'million', 'odds', 'first')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (28, 'first') has the max prob in order to be the next word. The prob is: 0.0019469645339995623
913100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0195])
The negative loglikelihood (-log p(first)): 6.241483688354492 or 6.241483688354492 or 6.241483688354492
('police', 'finds', 'sarin', 'possession', 'al', 'nusra')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4587, 'nusra') has the max prob in order to be the next word. The prob is: 2.110960622303537e-06
913200 The nn predicts (tensor([659]), 'qaeda') the next word with max prob: tensor([0.1282])
The negative loglikelihood (-log p(nusra)): 13.068367004394531 or 13.068367004394531 or 13.068367004394531
('boy', 'rescued', 'sewer', 'pipe', 'trapped', 'accident')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1366, 'accident') has the max prob in order to be the next word. The prob is: 0.006314953323453665
913300 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0120])
The negative loglikelihood (-log p(accident)): 5.064835071563721 or 5.064835071563721 or 5.064835071563721
('long', 'suspected', 'astronauts', 'mars', 'mission', 'would')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (104, 'would') has the max prob in order to be the next word. The prob is: 0.0031666692811995745
913400 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0196])
The negative loglikelihood (-log p(would)): 5.755074977874756 or 5.755074977874756 or 5.755074977874756
('core', 'weakens', 'iran', 'stepped', 'support', 'terrorism')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (454, 'terrorism') has the max prob in order to be the next word. The prob is: 0.001126874703913927
913500 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0250])
The negative loglikelihood (-log p(terrorism)): 6.788307189941406 or 6.788307189941406 or 6.788307189941406
('german', 'president', 'joachim', 'gauck', 'made', 'clear')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1309, 'clear') has the max prob in order to be the next word. The prob is: 0.0003199409693479538
913600 The nn predicts (tensor([113]), 'european') the next word with max prob: tensor([0.0109])
The negative loglikelihood (-log p(clear)): 8.04737377166748 or 8.04737377166748 or 8.04737377166748
('three', 'lebanese', 'northern', 'nigeria', 'suspicion', 'members')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (339, 'members') has the max prob in order to be the next word. The prob is: 0.005027871113270521
913700 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0211])
The negative loglikelihood (-log p(members)): 5.292758464813232 or 5.292758464813232 or 5.292758464813232
('boston', 'bomber', 'speaks', 'family', 'first', 'time')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (101, 'time') has the max prob in order to be the next word. The prob is: 0.0662645697593689
913800 The nn predicts (tensor([101]), 'time') the next word with max prob: tensor([0.0663])
The negative loglikelihood (-log p(time)): 2.714099884033203 or 2.714099884033203 or 2.714099884033203
('japan', 'offer', 'billion', 'aid', 'africa', 'coupled')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18294, 'coupled') has the max prob in order to be the next word. The prob is: 3.326608748466242e-06
913900 The nn predicts (tensor([168]), 'countries') the next word with max prob: tensor([0.0271])
The negative loglikelihood (-log p(coupled)): 12.613556861877441 or 12.613556861877441 or 12.613556861877441
('parents', 'hit', 'children', 'could', 'soon', 'cured')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8115, 'cured') has the max prob in order to be the next word. The prob is: 1.393008824379649e-05
914000 The nn predicts (tensor([263]), 'go') the next word with max prob: tensor([0.0124])
The negative loglikelihood (-log p(cured)): 11.181459426879883 or 11.181459426879883 or 11.181459426879883
('france', 'high', 'speed', 'train', 'catches', 'fire')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (111, 'fire') has the max prob in order to be the next word. The prob is: 0.02026926912367344
914100 The nn predicts (tensor([111]), 'fire') the next word with max prob: tensor([0.0203])
The negative loglikelihood (-log p(fire)): 3.8986494541168213 or 3.8986494541168213 or 3.8986494541168213
('setting', 'google', 'search', 'force', 'users', 'seeking')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1132, 'seeking') has the max prob in order to be the next word. The prob is: 0.0016511225840076804
914200 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0126])
The negative loglikelihood (-log p(seeking)): 6.406300067901611 or 6.406300067901611 or 6.406300067901611
('engaged', 'peruvian', 'woman', 'according', 'attorney', 'wedding')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2114, 'wedding') has the max prob in order to be the next word. The prob is: 0.00012005786993540823
914300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0303])
The negative loglikelihood (-log p(wedding)): 9.027536392211914 or 9.027536392211914 or 9.027536392211914
('us', 'wheat', 'imports', 'unapproved', 'mansanto', 'mutant')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11520, 'mutant') has the max prob in order to be the next word. The prob is: 1.0773153917398304e-05
914400 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0305])
The negative loglikelihood (-log p(mutant)): 11.438453674316406 or 11.438453674316406 or 11.438453674316406
('germany', 'urge', 'russia', 'arm', 'syria', 'military')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18, 'military') has the max prob in order to be the next word. The prob is: 0.0027952997479587793
914500 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0770])
The negative loglikelihood (-log p(military)): 5.879816055297852 or 5.879816055297852 or 5.879816055297852
('says', 'fatal', 'beaver', 'attack', 'belarus', 'anomaly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16650, 'anomaly') has the max prob in order to be the next word. The prob is: 2.587582230262342e-06
914600 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0144])
The negative loglikelihood (-log p(anomaly)): 12.864787101745605 or 12.864786148071289 or 12.864786148071289
('disability', 'mental', 'illness', 'haunt', 'elderly', 'pose')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3401, 'pose') has the max prob in order to be the next word. The prob is: 8.080758561845869e-05
914700 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0359])
The negative loglikelihood (-log p(pose)): 9.423439979553223 or 9.423439979553223 or 9.423439979553223
('ali', 'almanasfi', 'killed', 'along', 'us', 'woman')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (129, 'woman') has the max prob in order to be the next word. The prob is: 0.0010439952602609992
914800 The nn predicts (tensor([365]), 'drone') the next word with max prob: tensor([0.0742])
The negative loglikelihood (-log p(woman)): 6.8647003173828125 or 6.8647003173828125 or 6.8647003173828125
('river', 'nile', 'part', 'controversial', 'scheme', 'build')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (636, 'build') has the max prob in order to be the next word. The prob is: 0.0013037682510912418
914900 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0125])
The negative loglikelihood (-log p(build)): 6.642496585845947 or 6.642496585845947 or 6.642496585845947
('daily', 'website', 'invites', 'articles', 'continuing', 'series')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1429, 'series') has the max prob in order to be the next word. The prob is: 0.000730768486391753
915000 The nn predicts (tensor([132]), 'media') the next word with max prob: tensor([0.0137])
The negative loglikelihood (-log p(series)): 7.221413612365723 or 7.221414089202881 or 7.221414089202881
('dung', 'called', 'solution', 'territorial', 'disputes', 'south')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (23, 'south') has the max prob in order to be the next word. The prob is: 0.006761886645108461
915100 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.1111])
The negative loglikelihood (-log p(south)): 4.996453285217285 or 4.996453285217285 or 4.996453285217285
('iraq', 'may', 'highest', 'death', 'toll', 'years')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (33, 'years') has the max prob in order to be the next word. The prob is: 0.018020328134298325
915200 The nn predicts (tensor([31]), 'two') the next word with max prob: tensor([0.0236])
The negative loglikelihood (-log p(years)): 4.016254901885986 or 4.016254901885986 or 4.016254901885986
('suffragettes', 'kurdish', 'woman', 'fighting', 'female', 'genital')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4250, 'genital') has the max prob in order to be the next word. The prob is: 3.120330075034872e-05
915300 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0525])
The negative loglikelihood (-log p(genital)): 10.37498664855957 or 10.37498664855957 or 10.37498664855957
('forces', 'arrest', 'mayor', 'dagestan', 'city', 'tsarnaev')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9731, 'tsarnaev') has the max prob in order to be the next word. The prob is: 1.6664511349517852e-05
915400 The nn predicts (tensor([112]), 'near') the next word with max prob: tensor([0.0362])
The negative loglikelihood (-log p(tsarnaev)): 11.002228736877441 or 11.002228736877441 or 11.002228736877441
('wardak', 'province', 'afghan', 'investigators', 'said', 'disappeared')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2976, 'disappeared') has the max prob in order to be the next word. The prob is: 2.3247992430697195e-05
915500 The nn predicts (tensor([295]), 'monday') the next word with max prob: tensor([0.0729])
The negative loglikelihood (-log p(disappeared)): 10.669291496276855 or 10.669291496276855 or 10.669291496276855
('arrests', 'mayor', 'tied', 'killing', 'dagestan', 'newspaper')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (805, 'newspaper') has the max prob in order to be the next word. The prob is: 0.0002873686025850475
915600 The nn predicts (tensor([576]), 'soldier') the next word with max prob: tensor([0.0290])
The negative loglikelihood (-log p(newspaper)): 8.154745101928711 or 8.154745101928711 or 8.154745101928711
('office', 'middle', 'chaos', 'turkey', 'treating', 'injured')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (328, 'injured') has the max prob in order to be the next word. The prob is: 0.00028944291989319026
915700 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0234])
The negative loglikelihood (-log p(injured)): 8.147552490234375 or 8.147552490234375 or 8.147552490234375
('south', 'china', 'sea', 'islands', 'controlled', 'japan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (40, 'japan') has the max prob in order to be the next word. The prob is: 0.010218915529549122
915800 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.1326])
The negative loglikelihood (-log p(japan)): 4.58351469039917 or 4.58351469039917 or 4.58351469039917
('safe', 'passage', 'sought', 'qusayr', 'civilians', 'middle')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (344, 'middle') has the max prob in order to be the next word. The prob is: 0.00181290612090379
915900 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0332])
The negative loglikelihood (-log p(middle)): 6.312824249267578 or 6.312824249267578 or 6.312824249267578
('saudi', 'arabia', 'reported', 'three', 'people', 'died')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (425, 'died') has the max prob in order to be the next word. The prob is: 0.01603187993168831
916000 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.1603])
The negative loglikelihood (-log p(died)): 4.133175849914551 or 4.133175849914551 or 4.133175849914551
('clashes', 'hezbollah', 'lebanon', 'border', 'region', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.022053193300962448
916100 The nn predicts (tensor([534]), 'lebanon') the next word with max prob: tensor([0.0788])
The negative loglikelihood (-log p(syria)): 3.814297914505005 or 3.814297914505005 or 3.814297914505005
('access', 'vulnerable', 'people', 'across', 'six', 'decades')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (971, 'decades') has the max prob in order to be the next word. The prob is: 0.025091959163546562
916200 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.1156])
The negative loglikelihood (-log p(decades)): 3.6852078437805176 or 3.6852078437805176 or 3.6852078437805176
('sell', 'least', 'mig', 'fighter', 'jets', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.04998074471950531
916300 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0500])
The negative loglikelihood (-log p(syria)): 2.99611759185791 or 2.996117353439331 or 2.996117353439331
('<s>', '<s>', 'twitter', 'troublemaker', 'turkish', 'pm')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (117, 'pm') has the max prob in order to be the next word. The prob is: 0.007367982994765043
916400 The nn predicts (tensor([103]), 'border') the next word with max prob: tensor([0.0275])
The negative loglikelihood (-log p(pm)): 4.910611152648926 or 4.910611152648926 or 4.910611152648926
('new', 'weapons', 'defiance', 'treaty', 'report', 'claims')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (125, 'claims') has the max prob in order to be the next word. The prob is: 0.013784690760076046
916500 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.1059])
The negative loglikelihood (-log p(claims)): 4.284196853637695 or 4.284196853637695 or 4.284196853637695
('drive', 'thailand', 'grim', 'trade', 'dog', 'meat')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1617, 'meat') has the max prob in order to be the next word. The prob is: 0.0022452727425843477
916600 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0055])
The negative loglikelihood (-log p(meat)): 6.098928451538086 or 6.098928451538086 or 6.098928451538086
('minister', 'recep', 'tayyip', 'erdogan', 'fight', 'islamism')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12536, 'islamism') has the max prob in order to be the next word. The prob is: 1.73885709955357e-05
916700 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0284])
The negative loglikelihood (-log p(islamism)): 10.959697723388672 or 10.959697723388672 or 10.959697723388672
('exclusively', 'national', 'said', 'talks', 'way', 'potential')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1229, 'potential') has the max prob in order to be the next word. The prob is: 0.00029315880965441465
916800 The nn predicts (tensor([137]), 'end') the next word with max prob: tensor([0.0088])
The negative loglikelihood (-log p(potential)): 8.134796142578125 or 8.134796142578125 or 8.134796142578125
('near', 'milan', 'planning', 'set', 'recycling', 'business')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (580, 'business') has the max prob in order to be the next word. The prob is: 0.0002074002695735544
916900 The nn predicts (tensor([149]), 'weapons') the next word with max prob: tensor([0.0169])
The negative loglikelihood (-log p(business)): 8.480859756469727 or 8.480859756469727 or 8.480859756469727
('mujica', 'think', 'let', 'going', 'make', 'revolution')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (784, 'revolution') has the max prob in order to be the next word. The prob is: 0.0003865189210046083
917000 The nn predicts (tensor([78]), 'europe') the next word with max prob: tensor([0.0136])
The negative loglikelihood (-log p(revolution)): 7.858329772949219 or 7.858329772949219 or 7.858329772949219
('morning', 'fire', 'plant', 'layout', 'locked', 'gate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5278, 'gate') has the max prob in order to be the next word. The prob is: 5.000101373298094e-05
917100 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0201])
The negative loglikelihood (-log p(gate)): 9.903467178344727 or 9.903467178344727 or 9.903467178344727
('square', 'istanbul', 'actually', 'looks', 'like', 'incredible')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4699, 'incredible') has the max prob in order to be the next word. The prob is: 3.7728666939074174e-05
917200 The nn predicts (tensor([78]), 'europe') the next word with max prob: tensor([0.0233])
The negative loglikelihood (-log p(incredible)): 10.185090065002441 or 10.185090065002441 or 10.185090065002441
('seize', 'nearly', 'tons', 'cocaine', 'high', 'seas')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3525, 'seas') has the max prob in order to be the next word. The prob is: 0.0005399342044256628
917300 The nn predicts (tensor([66]), 'oil') the next word with max prob: tensor([0.0071])
The negative loglikelihood (-log p(seas)): 7.5240631103515625 or 7.5240631103515625 or 7.5240631103515625
('<s>', '<s>', '<s>', 'sets', 'complaints', 'department')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1258, 'department') has the max prob in order to be the next word. The prob is: 0.00011709741374943405
917400 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0139])
The negative loglikelihood (-log p(department)): 9.052504539489746 or 9.052504539489746 or 9.052504539489746
('yen', 'could', 'burst', 'china', 'asset', 'bubbles')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14455, 'bubbles') has the max prob in order to be the next word. The prob is: 3.440936325205257e-06
917500 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0109])
The negative loglikelihood (-log p(bubbles)): 12.579767227172852 or 12.579767227172852 or 12.579767227172852
('africa', 'letter', 'south', 'african', 'president', 'jacob')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5973, 'jacob') has the max prob in order to be the next word. The prob is: 0.00032032051240094006
917600 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0559])
The negative loglikelihood (-log p(jacob)): 8.046188354492188 or 8.046188354492188 or 8.046188354492188
('people', 'town', 'conditions', 'becoming', 'increasingly', 'dire')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5453, 'dire') has the max prob in order to be the next word. The prob is: 7.192264456534758e-05
917700 The nn predicts (tensor([1523]), 'society') the next word with max prob: tensor([0.0122])
The negative loglikelihood (-log p(dire)): 9.53991985321045 or 9.53991985321045 or 9.53991985321045
('pay', 'mention', 'singapore', 'websites', 'viewers', 'post')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (723, 'post') has the max prob in order to be the next word. The prob is: 0.0009668563143350184
917800 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0356])
The negative loglikelihood (-log p(post)): 6.941460609436035 or 6.941460609436035 or 6.941460609436035
('recapturing', 'remote', 'islands', 'despite', 'protests', 'china')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1, 'china') has the max prob in order to be the next word. The prob is: 0.013986111618578434
917900 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0146])
The negative loglikelihood (-log p(china)): 4.26969051361084 or 4.26969051361084 or 4.26969051361084
('singapore', 'ink', 'new', 'military', 'training', 'pact')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1444, 'pact') has the max prob in order to be the next word. The prob is: 0.00021081654995214194
918000 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0241])
The negative loglikelihood (-log p(pact)): 8.464522361755371 or 8.464522361755371 or 8.464522361755371
('held', 'accountable', 'war', 'reaches', 'new', 'levels')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (915, 'levels') has the max prob in order to be the next word. The prob is: 0.0003475091070868075
918100 The nn predicts (tensor([583]), 'zealand') the next word with max prob: tensor([0.0748])
The negative loglikelihood (-log p(levels)): 7.964719772338867 or 7.964719772338867 or 7.964719772338867
('kenyans', 'aghast', 'deadly', 'attacks', 'bring', 'country')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35, 'country') has the max prob in order to be the next word. The prob is: 0.012513518333435059
918200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0168])
The negative loglikelihood (-log p(country)): 4.380945682525635 or 4.380945682525635 or 4.380945682525635
('influence', 'syria', 'situation', 'force', 'cause', 'humanitarian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1066, 'humanitarian') has the max prob in order to be the next word. The prob is: 0.0006213634624145925
918300 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0209])
The negative loglikelihood (-log p(humanitarian)): 7.383594512939453 or 7.383594512939453 or 7.383594512939453
('spanish', 'police', 'tuesday', 'arrested', 'suspected', 'members')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (339, 'members') has the max prob in order to be the next word. The prob is: 0.003873944515362382
918400 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0339])
The negative loglikelihood (-log p(members)): 5.5534820556640625 or 5.5534820556640625 or 5.5534820556640625
('site', 'operators', 'committing', 'offenses', 'serious', 'crime')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (507, 'crime') has the max prob in order to be the next word. The prob is: 0.007126899436116219
918500 The nn predicts (tensor([90]), 'human') the next word with max prob: tensor([0.0142])
The negative loglikelihood (-log p(crime)): 4.943879127502441 or 4.943879127502441 or 4.943879127502441
('fined', 'officer', 'reduced', 'ranks', 'admitted', 'abusing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4227, 'abusing') has the max prob in order to be the next word. The prob is: 0.00012603482173290104
918600 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0239])
The negative loglikelihood (-log p(abusing)): 8.978952407836914 or 8.978952407836914 or 8.978952407836914
('israeli', 'soldiers', 'get', 'trouble', 'lightly', 'dressed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4391, 'dressed') has the max prob in order to be the next word. The prob is: 0.00013137904170434922
918700 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0852])
The negative loglikelihood (-log p(dressed)): 8.937423706054688 or 8.937423706054688 or 8.937423706054688
('syrian', 'rebels', 'say', 'respect', 'holy', 'sites')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (857, 'sites') has the max prob in order to be the next word. The prob is: 0.001114996732212603
918800 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0905])
The negative loglikelihood (-log p(sites)): 6.798903942108154 or 6.798903942108154 or 6.798903942108154
('rebels', 'accused', 'expelling', 'black', 'residents', 'mali')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (732, 'mali') has the max prob in order to be the next word. The prob is: 0.001177312107756734
918900 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0586])
The negative loglikelihood (-log p(mali)): 6.744521141052246 or 6.744521141052246 or 6.744521141052246
('union', 'imposes', 'tariffs', 'solar', 'panels', 'china')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1, 'china') has the max prob in order to be the next word. The prob is: 0.007257305085659027
919000 The nn predicts (tensor([40]), 'japan') the next word with max prob: tensor([0.0079])
The negative loglikelihood (-log p(china)): 4.925746917724609 or 4.925746917724609 or 4.925746917724609
('brazil', 'drops', 'happy', 'prostitute', 'advertising', 'campaign')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (330, 'campaign') has the max prob in order to be the next word. The prob is: 0.0008221417083404958
919100 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0134])
The negative loglikelihood (-log p(campaign)): 7.103597640991211 or 7.103597640991211 or 7.103597640991211
('rising', 'indonesia', 'authorities', 'fail', 'stop', 'poaching')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3375, 'poaching') has the max prob in order to be the next word. The prob is: 0.0002607445640023798
919200 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0144])
The negative loglikelihood (-log p(poaching)): 8.251969337463379 or 8.251969337463379 or 8.251969337463379
('election', 'without', 'reforms', 'party', 'spokesperson', 'douglas')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12000, 'douglas') has the max prob in order to be the next word. The prob is: 5.193754986976273e-06
919300 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0343])
The negative loglikelihood (-log p(douglas)): 12.16805362701416 or 12.16805362701416 or 12.16805362701416
('campaign', 'egypt', 'streets', 'tests', 'morsi', 'goal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2711, 'goal') has the max prob in order to be the next word. The prob is: 7.215692312456667e-05
919400 The nn predicts (tensor([1009]), 'square') the next word with max prob: tensor([0.0426])
The negative loglikelihood (-log p(goal)): 9.536666870117188 or 9.536666870117188 or 9.536666870117188
('putin', 'defended', 'russia', 'arms', 'deals', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.0454782173037529
919500 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0455])
The negative loglikelihood (-log p(syria)): 3.090521812438965 or 3.090521812438965 or 3.090521812438965
('boeing', 'dreamliner', 'returns', 'skies', 'japanese', 'pilots')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2153, 'pilots') has the max prob in order to be the next word. The prob is: 7.067058322718367e-05
919600 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0211])
The negative loglikelihood (-log p(pilots)): 9.557480812072754 or 9.557480812072754 or 9.557480812072754
('minh', 'city', 'earned', 'least', 'million', 'clients')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4906, 'clients') has the max prob in order to be the next word. The prob is: 3.797431054408662e-05
919700 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.2750])
The negative loglikelihood (-log p(clients)): 10.178600311279297 or 10.178600311279297 or 10.178600311279297
('russia', 'tax', 'hikes', 'force', 'inbev', 'close')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (428, 'close') has the max prob in order to be the next word. The prob is: 0.0001183137865155004
919800 The nn predicts (tensor([353]), 'company') the next word with max prob: tensor([0.0135])
The negative loglikelihood (-log p(close)): 9.042170524597168 or 9.042170524597168 or 9.042170524597168
('human', 'waste', 'free', 'efficient', 'way', 'boost')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1051, 'boost') has the max prob in order to be the next word. The prob is: 0.001681721769273281
919900 The nn predicts (tensor([964]), 'prevent') the next word with max prob: tensor([0.0073])
The negative loglikelihood (-log p(boost)): 6.387937068939209 or 6.387937068939209 or 6.387937068939209
('owners', 'win', 'partial', 'victory', 'coffee', 'shop')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3362, 'shop') has the max prob in order to be the next word. The prob is: 0.00015700387302786112
920000 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.0065])
The negative loglikelihood (-log p(shop)): 8.75924015045166 or 8.75924015045166 or 8.75924015045166
('<s>', 'imf', 'admit', 'mistakes', 'greece', 'bailout')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1098, 'bailout') has the max prob in order to be the next word. The prob is: 0.0036128591746091843
920100 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.0548])
The negative loglikelihood (-log p(bailout)): 5.623255729675293 or 5.623255729675293 or 5.623255729675293
('payout', 'uk', 'expresses', 'regret', 'abuse', 'william')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3366, 'william') has the max prob in order to be the next word. The prob is: 4.1649353079264984e-05
920200 The nn predicts (tensor([296]), 'charges') the next word with max prob: tensor([0.0268])
The negative loglikelihood (-log p(william)): 10.086224555969238 or 10.086224555969238 or 10.086224555969238
('major', 'newspapers', 'pakistan', 'wednesday', 'gave', 'wide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4145, 'wide') has the max prob in order to be the next word. The prob is: 6.365857552736998e-05
920300 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0212])
The negative loglikelihood (-log p(wide)): 9.66197681427002 or 9.66197681427002 or 9.66197681427002
('died', 'indonesia', 'sumatra', 'island', 'less', 'decade')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1361, 'decade') has the max prob in order to be the next word. The prob is: 0.00786955002695322
920400 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.1014])
The negative loglikelihood (-log p(decade)): 4.844754219055176 or 4.844754219055176 or 4.844754219055176
('syria', 'assad', 'even', 'want', 'use', 'chemical')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (313, 'chemical') has the max prob in order to be the next word. The prob is: 0.10327783226966858
920500 The nn predicts (tensor([313]), 'chemical') the next word with max prob: tensor([0.1033])
The negative loglikelihood (-log p(chemical)): 2.2703325748443604 or 2.2703325748443604 or 2.2703325748443604
('town', 'new', 'rand', 'portable', 'flush', 'toilet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3091, 'toilet') has the max prob in order to be the next word. The prob is: 0.0005246083019301295
920600 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0096])
The negative loglikelihood (-log p(toilet)): 7.552858829498291 or 7.552858829498291 or 7.552858829498291
('smash', 'chinese', 'trafficking', 'ring', 'group', 'suspected')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (241, 'suspected') has the max prob in order to be the next word. The prob is: 0.0027666327077895403
920700 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0175])
The negative loglikelihood (-log p(suspected)): 5.890124320983887 or 5.890124320983887 or 5.890124320983887
('greek', 'bailout', 'underestimating', 'effects', 'austerity', 'making')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (625, 'making') has the max prob in order to be the next word. The prob is: 0.000505712116137147
920800 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.0920])
The negative loglikelihood (-log p(making)): 7.589542865753174 or 7.589542865753174 or 7.589542865753174
('acquiring', 'overseas', 'technology', 'directly', 'chinese', 'scientists')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (305, 'scientists') has the max prob in order to be the next word. The prob is: 0.0006933191325515509
920900 The nn predicts (tensor([41]), 'chinese') the next word with max prob: tensor([0.0334])
The negative loglikelihood (-log p(scientists)): 7.274020195007324 or 7.274020195007324 or 7.274020195007324
('exporter', 'unmanned', 'aerial', 'systems', 'surmounting', 'aerospace')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9768, 'aerospace') has the max prob in order to be the next word. The prob is: 4.766048732562922e-06
921000 The nn predicts (tensor([857]), 'sites') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(aerospace)): 12.253993034362793 or 12.253993034362793 or 12.253993034362793
('extended', 'married', 'people', 'violated', 'civil', 'rights')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (67, 'rights') has the max prob in order to be the next word. The prob is: 0.06515242904424667
921100 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.2830])
The negative loglikelihood (-log p(rights)): 2.7310256958007812 or 2.7310256958007812 or 2.7310256958007812
('elite', 'vacuum', 'benefits', 'rest', 'population', 'remains')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (924, 'remains') has the max prob in order to be the next word. The prob is: 0.00024423934519290924
921200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0816])
The negative loglikelihood (-log p(remains)): 8.317361831665039 or 8.317361831665039 or 8.317361831665039
('snap', 'analysis', 'regime', 'forces', 'take', 'qusayr')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11935, 'qusayr') has the max prob in order to be the next word. The prob is: 9.54925490077585e-05
921300 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0342])
The negative loglikelihood (-log p(qusayr)): 9.256462097167969 or 9.256462097167969 or 9.256462097167969
('<s>', '<s>', '<s>', 'liquor', 'spanish', 'mps')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (934, 'mps') has the max prob in order to be the next word. The prob is: 0.0002944036677945405
921400 The nn predicts (tensor([606]), 'marriage') the next word with max prob: tensor([0.0438])
The negative loglikelihood (-log p(mps)): 8.130558967590332 or 8.130558967590332 or 8.130558967590332
('turkish', 'prime', 'minister', 'erdogan', 'driving', 'wedge')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18311, 'wedge') has the max prob in order to be the next word. The prob is: 4.89588046548306e-06
921500 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0267])
The negative loglikelihood (-log p(wedge)): 12.227116584777832 or 12.227116584777832 or 12.227116584777832
('sex', 'determination', 'tests', 'following', 'spike', 'proportion')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10568, 'proportion') has the max prob in order to be the next word. The prob is: 3.588449544622563e-06
921600 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0612])
The negative loglikelihood (-log p(proportion)): 12.537790298461914 or 12.537790298461914 or 12.537790298461914
('successfully', 'compromised', 'high', 'profile', 'victims', 'countries')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (168, 'countries') has the max prob in order to be the next word. The prob is: 0.00015635322779417038
921700 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(countries)): 8.763392448425293 or 8.763392448425293 or 8.763392448425293
('govt', 'surveillance', 'programs', 'prism', 'phone', 'surveillance')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (641, 'surveillance') has the max prob in order to be the next word. The prob is: 0.0009103670017793775
921800 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0241])
The negative loglikelihood (-log p(surveillance)): 7.001662731170654 or 7.001662731170654 or 7.001662731170654
('turkey', 'including', 'cnn', 'franchise', 'turkey', 'video')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (100, 'video') has the max prob in order to be the next word. The prob is: 0.0015044991159811616
921900 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0473])
The negative loglikelihood (-log p(video)): 6.499295234680176 or 6.499295234680176 or 6.499295234680176
('testifies', 'alien', 'species', 'visiting', 'earth', 'thousands')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (120, 'thousands') has the max prob in order to be the next word. The prob is: 4.350964445620775e-05
922000 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0180])
The negative loglikelihood (-log p(thousands)): 10.04252815246582 or 10.04252815246582 or 10.04252815246582
('even', 'horrendous', 'distractions', 'economic', 'crisis', 'street')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (725, 'street') has the max prob in order to be the next word. The prob is: 7.679904229007661e-05
922100 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0473])
The negative loglikelihood (-log p(street)): 9.474318504333496 or 9.474318504333496 or 9.474318504333496
('established', 'huge', 'presence', 'uk', 'officials', 'failed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (697, 'failed') has the max prob in order to be the next word. The prob is: 0.0008716038428246975
922200 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.3375])
The negative loglikelihood (-log p(failed)): 7.045175552368164 or 7.045175552368164 or 7.045175552368164
('new', 'poll', 'iranian', 'election', 'shows', 'rouhani')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1985, 'rouhani') has the max prob in order to be the next word. The prob is: 1.0347431498303195e-06
922300 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0137])
The negative loglikelihood (-log p(rouhani)): 13.781357765197754 or 13.781357765197754 or 13.781357765197754
('obama', 'administration', 'collecting', 'phone', 'records', 'tens')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1253, 'tens') has the max prob in order to be the next word. The prob is: 7.564374391222373e-05
922400 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0108])
The negative loglikelihood (-log p(tens)): 9.489476203918457 or 9.489476203918457 or 9.489476203918457
('<s>', 'million', 'pakistan', 'million', 'olds', 'school')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (297, 'school') has the max prob in order to be the next word. The prob is: 0.00023318640887737274
922500 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.4904])
The negative loglikelihood (-log p(school)): 8.363672256469727 or 8.363672256469727 or 8.363672256469727
('reilly', 'mainstreams', 'leader', 'violent', 'british', 'hate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1521, 'hate') has the max prob in order to be the next word. The prob is: 0.00015732526662759483
922600 The nn predicts (tensor([109]), 'prime') the next word with max prob: tensor([0.0255])
The negative loglikelihood (-log p(hate)): 8.757195472717285 or 8.757195472717285 or 8.757195472717285
('<s>', 'cfb', 'edmonton', 'base', 'raise', 'flag')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1337, 'flag') has the max prob in order to be the next word. The prob is: 0.00027096024132333696
922700 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0158])
The negative loglikelihood (-log p(flag)): 8.21353816986084 or 8.21353816986084 or 8.21353816986084
('estimated', 'others', 'currently', 'death', 'row', 'mostly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2448, 'mostly') has the max prob in order to be the next word. The prob is: 0.00018744489352684468
922800 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0153])
The negative loglikelihood (-log p(mostly)): 8.582025527954102 or 8.582025527954102 or 8.582025527954102
('mandela', 'back', 'hospital', 'recurring', 'lung', 'infection')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4157, 'infection') has the max prob in order to be the next word. The prob is: 0.0003479819861240685
922900 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0233])
The negative loglikelihood (-log p(infection)): 7.963359832763672 or 7.963359832763672 or 7.963359832763672
('prime', 'minister', 'recep', 'tayyip', 'erdogan', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.05117783695459366
923000 The nn predicts (tensor([615]), 'erdogan') the next word with max prob: tensor([0.1695])
The negative loglikelihood (-log p(said)): 2.9724488258361816 or 2.9724488258361816 or 2.9724488258361816
('subject', 'american', 'law', 'next', 'desirable', 'feature')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6839, 'feature') has the max prob in order to be the next word. The prob is: 1.4999107406765688e-05
923100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0452])
The negative loglikelihood (-log p(feature)): 11.10752010345459 or 11.10752010345459 or 11.10752010345459
('warning', 'last', 'month', 'south', 'sudan', 'supporting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2136, 'supporting') has the max prob in order to be the next word. The prob is: 7.858158642193303e-05
923200 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0376])
The negative loglikelihood (-log p(supporting)): 9.451373100280762 or 9.451373100280762 or 9.451373100280762
('illegal', 'dumping', 'asbestos', 'waste', 'around', 'country')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35, 'country') has the max prob in order to be the next word. The prob is: 0.01608632318675518
923300 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.3022])
The negative loglikelihood (-log p(country)): 4.129786014556885 or 4.129786014556885 or 4.129786014556885
('shopping', 'mall', 'one', 'green', 'patches', 'central')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (230, 'central') has the max prob in order to be the next word. The prob is: 0.003201725659891963
923400 The nn predicts (tensor([288]), 'around') the next word with max prob: tensor([0.0133])
The negative loglikelihood (-log p(central)): 5.744065284729004 or 5.744065284729004 or 5.744065284729004
('recently', 'seized', 'government', 'forces', 'arrived', 'lebanon')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (534, 'lebanon') has the max prob in order to be the next word. The prob is: 0.0035361682530492544
923500 The nn predicts (tensor([122]), 'afghanistan') the next word with max prob: tensor([0.0577])
The negative loglikelihood (-log p(lebanon)): 5.644711494445801 or 5.644711494445801 or 5.644711494445801
('facing', 'charges', 'obstructing', 'justice', 'magistrate', 'recused')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (63396, 'recused') has the max prob in order to be the next word. The prob is: 1.8587702470540535e-06
923600 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0535])
The negative loglikelihood (-log p(recused)): 13.195595741271973 or 13.195595741271973 or 13.195595741271973
('rebels', 'fought', 'moamar', 'gadhafi', 'civilians', 'launched')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (905, 'launched') has the max prob in order to be the next word. The prob is: 0.00048716680612415075
923700 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0744])
The negative loglikelihood (-log p(launched)): 7.626904010772705 or 7.626904010772705 or 7.626904010772705
('australia', 'remote', 'christmas', 'island', 'authorities', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.129400834441185
923800 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.1627])
The negative loglikelihood (-log p(said)): 2.0448403358459473 or 2.0448403358459473 or 2.0448403358459473
('government', 'holds', 'secret', 'meeting', 'ethiopian', 'dam')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1926, 'dam') has the max prob in order to be the next word. The prob is: 0.00029505923157557845
923900 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0868])
The negative loglikelihood (-log p(dam)): 8.128334045410156 or 8.128334045410156 or 8.128334045410156
('garbage', 'televangelist', 'malafaia', 'roared', 'microphone', 'cheering')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8579, 'cheering') has the max prob in order to be the next word. The prob is: 1.803839586500544e-05
924000 The nn predicts (tensor([100]), 'video') the next word with max prob: tensor([0.0107])
The negative loglikelihood (-log p(cheering)): 10.92300796508789 or 10.92300796508789 or 10.92300796508789
('<s>', '<s>', '<s>', 'summit', 'ends', 'accord')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3100, 'accord') has the max prob in order to be the next word. The prob is: 3.53222967532929e-05
924100 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0538])
The negative loglikelihood (-log p(accord)): 10.250996589660645 or 10.250996589660645 or 10.250996589660645
('locke', 'us', 'spy', 'agency', 'may', 'passing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4120, 'passing') has the max prob in order to be the next word. The prob is: 5.482365668285638e-05
924200 The nn predicts (tensor([148]), 'take') the next word with max prob: tensor([0.0220])
The negative loglikelihood (-log p(passing)): 9.811388969421387 or 9.811388969421387 or 9.811388969421387
('spirited', 'commitment', 'free', 'speech', 'right', 'political')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (178, 'political') has the max prob in order to be the next word. The prob is: 0.00041400225018151104
924300 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0184])
The negative loglikelihood (-log p(political)): 7.789638996124268 or 7.789638996124268 or 7.789638996124268
('shelter', 'medical', 'help', 'stay', 'mobile', 'avoid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1121, 'avoid') has the max prob in order to be the next word. The prob is: 0.00059845409123227
924400 The nn predicts (tensor([316]), 'data') the next word with max prob: tensor([0.0078])
The negative loglikelihood (-log p(avoid)): 7.421160697937012 or 7.421160697937012 or 7.421160697937012
('spill', 'originated', 'ecuador', 'travelling', 'downstream', 'towards')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1101, 'towards') has the max prob in order to be the next word. The prob is: 0.0001940605725394562
924500 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0188])
The negative loglikelihood (-log p(towards)): 8.547340393066406 or 8.547340393066406 or 8.547340393066406
('syrian', 'teenager', 'mohammad', 'qataa', 'executed', 'islamists')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (933, 'islamists') has the max prob in order to be the next word. The prob is: 0.0006901294109411538
924600 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0194])
The negative loglikelihood (-log p(islamists)): 7.278631210327148 or 7.278631210327148 or 7.278631210327148
('adopted', 'traffic', 'police', 'demand', 'regular', 'maintenance')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9189, 'maintenance') has the max prob in order to be the next word. The prob is: 1.0207634659309406e-05
924700 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0087])
The negative loglikelihood (-log p(maintenance)): 11.492374420166016 or 11.492374420166016 or 11.492374420166016
('train', 'drivers', 'wear', 'skirts', 'work', 'longer')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1024, 'longer') has the max prob in order to be the next word. The prob is: 0.00022076827008277178
924800 The nn predicts (tensor([298]), 'work') the next word with max prob: tensor([0.0262])
The negative loglikelihood (-log p(longer)): 8.418396949768066 or 8.418396949768066 or 8.418396949768066
('front', 'family', 'syrian', 'observatory', 'human', 'rights')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (67, 'rights') has the max prob in order to be the next word. The prob is: 0.7938489317893982
924900 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.7938])
The negative loglikelihood (-log p(rights)): 0.230862095952034 or 0.230862095952034 or 0.230862095952034
('suggested', 'nsa', 'whistleblower', 'edward', 'snowden', 'leave')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (342, 'leave') has the max prob in order to be the next word. The prob is: 0.00015780470857862383
925000 The nn predicts (tensor([766]), 'assange') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(leave)): 8.754152297973633 or 8.754152297973633 or 8.754152297973633
('initially', 'weak', 'supreme', 'leader', 'one', 'five')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (225, 'five') has the max prob in order to be the next word. The prob is: 0.004708549007773399
925100 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0188])
The negative loglikelihood (-log p(five)): 5.358375549316406 or 5.358375549316406 or 5.358375549316406
('capital', 'budapest', 'goes', 'high', 'alert', 'river')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1400, 'river') has the max prob in order to be the next word. The prob is: 0.000424836587626487
925200 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0232])
The negative loglikelihood (-log p(river)): 7.763805866241455 or 7.763805866241455 or 7.763805866241455
('turkish', 'leader', 'defends', 'police', 'crackdown', 'protesters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (119, 'protesters') has the max prob in order to be the next word. The prob is: 0.12038324773311615
925300 The nn predicts (tensor([119]), 'protesters') the next word with max prob: tensor([0.1204])
The negative loglikelihood (-log p(protesters)): 2.117074966430664 or 2.117074966430664 or 2.117074966430664
('emir', 'inciting', 'regime', 'change', 'insulting', 'religious')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (658, 'religious') has the max prob in order to be the next word. The prob is: 0.0028322283178567886
925400 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0357])
The negative loglikelihood (-log p(religious)): 5.866691589355469 or 5.866691589355469 or 5.866691589355469
('almost', 'one', 'quarter', 'workers', 'employed', 'sector')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1763, 'sector') has the max prob in order to be the next word. The prob is: 0.00040031931712292135
925500 The nn predicts (tensor([209]), 'workers') the next word with max prob: tensor([0.0095])
The negative loglikelihood (-log p(sector)): 7.823247909545898 or 7.823247909545898 or 7.823247909545898
('asylum', 'seeker', 'bodies', 'left', 'sea', 'authorities')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (239, 'authorities') has the max prob in order to be the next word. The prob is: 0.0006051383097656071
925600 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0321])
The negative loglikelihood (-log p(authorities)): 7.410053730010986 or 7.410053730010986 or 7.410053730010986
('hong', 'kong', 'believed', 'get', 'fair', 'trial')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (238, 'trial') has the max prob in order to be the next word. The prob is: 0.001642167684622109
925700 The nn predicts (tensor([298]), 'work') the next word with max prob: tensor([0.0090])
The negative loglikelihood (-log p(trial)): 6.411738395690918 or 6.41173791885376 or 6.41173791885376
('shias', 'wounding', 'dozens', 'others', 'clashes', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.024748392403125763
925800 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0247])
The negative loglikelihood (-log p(police)): 3.6989946365356445 or 3.6989946365356445 or 3.6989946365356445
('finds', 'diary', 'hitler', 'aide', 'alfred', 'rosenberg')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (20060, 'rosenberg') has the max prob in order to be the next word. The prob is: 1.4528119208989665e-05
925900 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0165])
The negative loglikelihood (-log p(rosenberg)): 11.139424324035645 or 11.139424324035645 or 11.139424324035645
('deport', 'foreigners', 'suspected', 'trafficking', 'drugs', 'country')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35, 'country') has the max prob in order to be the next word. The prob is: 0.0020146467722952366
926000 The nn predicts (tensor([1379]), 'trafficking') the next word with max prob: tensor([0.0411])
The negative loglikelihood (-log p(country)): 6.207311630249023 or 6.207311630249023 or 6.207311630249023
('account', 'fell', 'asleep', 'instant', 'pushing', 'onto')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3331, 'onto') has the max prob in order to be the next word. The prob is: 5.045525904279202e-05
926100 The nn predicts (tensor([115]), 'back') the next word with max prob: tensor([0.0124])
The negative loglikelihood (-log p(onto)): 9.894423484802246 or 9.894423484802246 or 9.894423484802246
('expect', 'salary', 'increases', 'offset', 'eu', 'bonus')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6236, 'bonus') has the max prob in order to be the next word. The prob is: 4.5055290684103966e-05
926200 The nn predicts (tensor([1098]), 'bailout') the next word with max prob: tensor([0.0166])
The negative loglikelihood (-log p(bonus)): 10.007619857788086 or 10.007619857788086 or 10.007619857788086
('london', 'building', 'activists', 'organising', 'carnival', 'capitalism')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4915, 'capitalism') has the max prob in order to be the next word. The prob is: 9.364582183479797e-06
926300 The nn predicts (tensor([1009]), 'square') the next word with max prob: tensor([0.1305])
The negative loglikelihood (-log p(capitalism)): 11.57857608795166 or 11.57857608795166 or 11.57857608795166
('snowden', 'safe', 'hong', 'kong', 'warns', 'human')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (90, 'human') has the max prob in order to be the next word. The prob is: 0.002233068225905299
926400 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0481])
The negative loglikelihood (-log p(human)): 6.104378700256348 or 6.104378700256348 or 6.104378700256348
('france', 'marine', 'le', 'pen', 'national', 'front')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (779, 'front') has the max prob in order to be the next word. The prob is: 0.0006025806651450694
926500 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0209])
The negative loglikelihood (-log p(front)): 7.4142889976501465 or 7.4142889976501465 or 7.4142889976501465
('take', 'vacant', 'buildings', 'france', 'faces', 'highest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1152, 'highest') has the max prob in order to be the next word. The prob is: 0.0001618044188944623
926600 The nn predicts (tensor([137]), 'end') the next word with max prob: tensor([0.0122])
The negative loglikelihood (-log p(highest)): 8.729122161865234 or 8.729122161865234 or 8.729122161865234
('underlying', 'prism', 'program', 'needs', 'stay', 'secret')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (309, 'secret') has the max prob in order to be the next word. The prob is: 0.002447038423269987
926700 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0380])
The negative loglikelihood (-log p(secret)): 6.012876987457275 or 6.012876987457275 or 6.012876987457275
('waste', 'ahead', 'planned', 'protest', 'dubbed', 'poo')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9593, 'poo') has the max prob in order to be the next word. The prob is: 3.2166951768886065e-06
926800 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0100])
The negative loglikelihood (-log p(poo)): 12.64715576171875 or 12.64715576171875 or 12.64715576171875
('russian', 'lawmakers', 'back', 'jail', 'terms', 'insulting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2062, 'insulting') has the max prob in order to be the next word. The prob is: 0.0013043330982327461
926900 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0218])
The negative loglikelihood (-log p(insulting)): 6.642063617706299 or 6.642063617706299 or 6.642063617706299
('afghan', 'supreme', 'court', 'staff', 'killed', 'suicide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (179, 'suicide') has the max prob in order to be the next word. The prob is: 0.0036557598505169153
927000 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0228])
The negative loglikelihood (-log p(suicide)): 5.611451148986816 or 5.611451148986816 or 5.611451148986816
('fire', 'tear', 'gas', 'effort', 'clear', 'istanbul')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1204, 'istanbul') has the max prob in order to be the next word. The prob is: 0.0013747736811637878
927100 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(istanbul)): 6.589466094970703 or 6.589466094970703 or 6.589466094970703
('cruise', 'ships', 'sparking', 'ire', 'world', 'biggest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (326, 'biggest') has the max prob in order to be the next word. The prob is: 0.014239654876291752
927200 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.3195])
The negative loglikelihood (-log p(biggest)): 4.251724720001221 or 4.251724720001221 or 4.251724720001221
('pushes', 'explain', 'surveillance', 'european', 'union', 'top')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (116, 'top') has the max prob in order to be the next word. The prob is: 0.0010684089502319694
927300 The nn predicts (tensor([596]), 'program') the next word with max prob: tensor([0.0128])
The negative loglikelihood (-log p(top)): 6.8415846824646 or 6.8415846824646 or 6.8415846824646
('sterilization', 'india', 'poorest', 'women', 'buys', 'food')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (222, 'food') has the max prob in order to be the next word. The prob is: 0.0015673766611143947
927400 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0104])
The negative loglikelihood (-log p(food)): 6.458352088928223 or 6.458352088928223 or 6.458352088928223
('fee', 'increase', 'around', 'people', 'streets', 'buses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4032, 'buses') has the max prob in order to be the next word. The prob is: 3.4168126148870215e-05
927500 The nn predicts (tensor([619]), 'streets') the next word with max prob: tensor([0.0422])
The negative loglikelihood (-log p(buses)): 10.284217834472656 or 10.28421688079834 or 10.28421688079834
('greek', 'broadcaster', 'defying', 'order', 'close', 'refuse')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2124, 'refuse') has the max prob in order to be the next word. The prob is: 1.6039561160141602e-05
927600 The nn predicts (tensor([163]), 'parliament') the next word with max prob: tensor([0.0192])
The negative loglikelihood (-log p(refuse)): 11.040452003479004 or 11.040452003479004 or 11.040452003479004
('globe', 'less', 'peaceful', 'five', 'years', 'ago')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (608, 'ago') has the max prob in order to be the next word. The prob is: 0.4182855188846588
927700 The nn predicts (tensor([608]), 'ago') the next word with max prob: tensor([0.4183])
The negative loglikelihood (-log p(ago)): 0.8715910315513611 or 0.8715910315513611 or 0.8715910315513611
('peruvian', 'amazon', 'oil', 'spilled', 'pipeline', 'landslide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2555, 'landslide') has the max prob in order to be the next word. The prob is: 1.9453451386652887e-05
927800 The nn predicts (tensor([180]), 'gas') the next word with max prob: tensor([0.0208])
The negative loglikelihood (-log p(landslide)): 10.84748649597168 or 10.84748649597168 or 10.84748649597168
('anger', 'mounts', 'congress', 'spying', 'americans', 'world')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7, 'world') has the max prob in order to be the next word. The prob is: 0.0039951554499566555
927900 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0211])
The negative loglikelihood (-log p(world)): 5.522672653198242 or 5.522672653198242 or 5.522672653198242
('giant', 'confirms', 'acquisition', 'popular', 'israeli', 'navigation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6444, 'navigation') has the max prob in order to be the next word. The prob is: 3.227478373446502e-06
928000 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0339])
The negative loglikelihood (-log p(navigation)): 12.64380931854248 or 12.64380931854248 or 12.64380931854248
('banned', 'works', 'galileo', 'scientists', 'call', 'drugs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (645, 'drugs') has the max prob in order to be the next word. The prob is: 7.677077519474551e-05
928100 The nn predicts (tensor([251]), 'gay') the next word with max prob: tensor([0.0192])
The negative loglikelihood (-log p(drugs)): 9.474686622619629 or 9.474686622619629 or 9.474686622619629
('chinese', 'national', 'gets', 'years', 'pirated', 'software')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2531, 'software') has the max prob in order to be the next word. The prob is: 0.0013344628969207406
928200 The nn predicts (tensor([641]), 'surveillance') the next word with max prob: tensor([0.0158])
The negative loglikelihood (-log p(software)): 6.619226455688477 or 6.619226455688477 or 6.619226455688477
('says', 'neither', 'traitor', 'hero', 'american', 'second')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (311, 'second') has the max prob in order to be the next word. The prob is: 8.511818305123597e-05
928300 The nn predicts (tensor([172]), 'soldiers') the next word with max prob: tensor([0.0137])
The negative loglikelihood (-log p(second)): 9.371469497680664 or 9.371469497680664 or 9.371469497680664
('recovered', 'missiles', 'capable', 'shooting', 'commercial', 'airliners')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16800, 'airliners') has the max prob in order to be the next word. The prob is: 2.1386724711192073e-06
928400 The nn predicts (tensor([156]), 'east') the next word with max prob: tensor([0.0338])
The negative loglikelihood (-log p(airliners)): 13.055325508117676 or 13.055325508117676 or 13.055325508117676
('grid', 'security', 'alert', 'singapore', 'scientists', 'find')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (478, 'find') has the max prob in order to be the next word. The prob is: 0.0027177513111382723
928500 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0684])
The negative loglikelihood (-log p(find)): 5.907950401306152 or 5.907950401306152 or 5.907950401306152
('finds', 'around', 'white', 'powder', 'produce', 'supplied')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5690, 'supplied') has the max prob in order to be the next word. The prob is: 8.470536704408005e-05
928600 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0296])
The negative loglikelihood (-log p(supplied)): 9.376331329345703 or 9.376331329345703 or 9.376331329345703
('blog', 'shutdown', 'ert', 'national', 'broadcaster', 'greece')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (221, 'greece') has the max prob in order to be the next word. The prob is: 0.0003433639940340072
928700 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0132])
The negative loglikelihood (-log p(greece)): 7.976719379425049 or 7.976719379425049 or 7.976719379425049
('software', 'great', 'see', 'germany', 'major', 'parties')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1360, 'parties') has the max prob in order to be the next word. The prob is: 0.00033988477662205696
928800 The nn predicts (tensor([596]), 'program') the next word with max prob: tensor([0.0148])
The negative loglikelihood (-log p(parties)): 7.986904144287109 or 7.986903667449951 or 7.986903667449951
('world', 'working', 'domestic', 'workers', 'hazardous', 'sometimes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4614, 'sometimes') has the max prob in order to be the next word. The prob is: 8.63205932546407e-05
928900 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0291])
The negative loglikelihood (-log p(sometimes)): 9.357442855834961 or 9.357441902160645 or 9.357441902160645
('rebels', 'recruit', 'children', 'fight', 'child', 'detainees')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2298, 'detainees') has the max prob in order to be the next word. The prob is: 0.00025985456886701286
929000 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0770])
The negative loglikelihood (-log p(detainees)): 8.255388259887695 or 8.255388259887695 or 8.255388259887695
('approval', 'says', 'safety', 'issue', 'team', 'plays')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3975, 'plays') has the max prob in order to be the next word. The prob is: 1.4791397006774787e-05
929100 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.0083])
The negative loglikelihood (-log p(plays)): 11.121464729309082 or 11.121464729309082 or 11.121464729309082
('civil', 'war', 'tortured', 'government', 'forces', 'links')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1089, 'links') has the max prob in order to be the next word. The prob is: 8.670685929246247e-05
929200 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0394])
The negative loglikelihood (-log p(links)): 9.352977752685547 or 9.352977752685547 or 9.352977752685547
('called', 'march', 'june', 'counter', 'planned', 'protest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (91, 'protest') has the max prob in order to be the next word. The prob is: 0.012317103333771229
929300 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0343])
The negative loglikelihood (-log p(protest)): 4.396766662597656 or 4.396766662597656 or 4.396766662597656
('follows', 'pattern', 'convictions', 'muslims', 'main', 'victims')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (379, 'victims') has the max prob in order to be the next word. The prob is: 0.00023777510796207935
929400 The nn predicts (tensor([118]), 'party') the next word with max prob: tensor([0.0306])
The negative loglikelihood (-log p(victims)): 8.344184875488281 or 8.344184875488281 or 8.344184875488281
('situation', 'southern', 'syria', 'desperate', 'syrian', 'doctors')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (848, 'doctors') has the max prob in order to be the next word. The prob is: 7.57148300181143e-05
929500 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.1978])
The negative loglikelihood (-log p(doctors)): 9.488536834716797 or 9.488536834716797 or 9.488536834716797
('uk', 'farmers', 'also', 'appear', 'shifting', 'favor')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3184, 'favor') has the max prob in order to be the next word. The prob is: 6.219281203811988e-05
929600 The nn predicts (tensor([95]), 'global') the next word with max prob: tensor([0.0081])
The negative loglikelihood (-log p(favor)): 9.685271263122559 or 9.685271263122559 or 9.685271263122559
('must', 'disclose', 'authors', 'tweets', 'french', 'appeals')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2610, 'appeals') has the max prob in order to be the next word. The prob is: 6.73862305120565e-05
929700 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.1541])
The negative loglikelihood (-log p(appeals)): 9.605070114135742 or 9.605070114135742 or 9.605070114135742
('closure', 'greek', 'unions', 'stage', 'protest', 'strike')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (139, 'strike') has the max prob in order to be the next word. The prob is: 0.0006739032105542719
929800 The nn predicts (tensor([91]), 'protest') the next word with max prob: tensor([0.0874])
The negative loglikelihood (-log p(strike)): 7.30242395401001 or 7.30242395401001 or 7.30242395401001
('doctor', 'killed', 'angry', 'mob', 'examining', 'female')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (535, 'female') has the max prob in order to be the next word. The prob is: 0.0003938846057280898
929900 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0132])
The negative loglikelihood (-log p(female)): 7.839452743530273 or 7.839452743530273 or 7.839452743530273
('us', 'court', 'seeks', 'permission', 'market', 'glivec')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (21628, 'glivec') has the max prob in order to be the next word. The prob is: 1.4678475054097362e-05
930000 The nn predicts (tensor([20]), 'uk') the next word with max prob: tensor([0.0062])
The negative loglikelihood (-log p(glivec)): 11.129128456115723 or 11.129128456115723 or 11.129128456115723
('turkey', 'pm', 'gives', 'final', 'warning', 'protesters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (119, 'protesters') has the max prob in order to be the next word. The prob is: 0.008167470805346966
930100 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0968])
The negative loglikelihood (-log p(protesters)): 4.807596206665039 or 4.807596206665039 or 4.807596206665039
('blast', 'chemical', 'plant', 'us', 'state', 'louisiana')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14694, 'louisiana') has the max prob in order to be the next word. The prob is: 1.3420105915429303e-06
930200 The nn predicts (tensor([132]), 'media') the next word with max prob: tensor([0.0338])
The negative loglikelihood (-log p(louisiana)): 13.521341323852539 or 13.521341323852539 or 13.521341323852539
('wearing', 'un', 'insignia', 'take', 'irregulars', 'sow')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13604, 'sow') has the max prob in order to be the next word. The prob is: 3.50484606315149e-06
930300 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1023])
The negative loglikelihood (-log p(sow)): 12.56136417388916 or 12.56136417388916 or 12.56136417388916
('govt', 'opens', 'arms', 'harper', 'shamed', 'internationally')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6386, 'internationally') has the max prob in order to be the next word. The prob is: 2.038868478848599e-05
930400 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(internationally)): 10.800530433654785 or 10.800530433654785 or 10.800530433654785
('house', 'syria', 'crosses', 'red', 'line', 'use')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (174, 'use') has the max prob in order to be the next word. The prob is: 0.0004896699683740735
930500 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0197])
The negative loglikelihood (-log p(use)): 7.621778964996338 or 7.621778964996338 or 7.621778964996338
('white', 'house', 'give', 'syria', 'rebels', 'military')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18, 'military') has the max prob in order to be the next word. The prob is: 0.0007475097081623971
930600 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0209])
The negative loglikelihood (-log p(military)): 7.198763370513916 or 7.198763370513916 or 7.198763370513916
('determining', 'government', 'used', 'chemical', 'weapons', 'opposition')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (138, 'opposition') has the max prob in order to be the next word. The prob is: 0.00041081150993704796
930700 The nn predicts (tensor([174]), 'use') the next word with max prob: tensor([0.0911])
The negative loglikelihood (-log p(opposition)): 7.7973761558532715 or 7.7973761558532715 or 7.7973761558532715
('deal', 'curbing', 'hfcs', 'good', 'climate', 'hfc')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (29745, 'hfc') has the max prob in order to be the next word. The prob is: 6.736127033946104e-07
930800 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.2817])
The negative loglikelihood (-log p(hfc)): 14.210610389709473 or 14.210610389709473 or 14.210610389709473
('embassy', 'still', 'walk', 'free', 'woman', 'bares')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (25563, 'bares') has the max prob in order to be the next word. The prob is: 3.2729190024838317e-06
930900 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0124])
The negative loglikelihood (-log p(bares)): 12.629828453063965 or 12.629828453063965 or 12.629828453063965
('spy', 'chief', 'indicted', 'charges', 'meddling', 'last')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (105, 'last') has the max prob in order to be the next word. The prob is: 0.0004584834969136864
931000 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0252])
The negative loglikelihood (-log p(last)): 7.687586307525635 or 7.687586307525635 or 7.687586307525635
('edward', 'snowden', 'board', 'flights', 'uk', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.003470388939604163
931100 The nn predicts (tensor([1073]), 'hacking') the next word with max prob: tensor([0.0191])
The negative loglikelihood (-log p(says)): 5.663488388061523 or 5.663488388061523 or 5.663488388061523
('photos', 'violent', 'protests', 'sao', 'paulo', 'result')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1458, 'result') has the max prob in order to be the next word. The prob is: 0.0001195805671159178
931200 The nn predicts (tensor([867]), 'park') the next word with max prob: tensor([0.0565])
The negative loglikelihood (-log p(result)): 9.031519889831543 or 9.031519889831543 or 9.031519889831543
('nsa', 'whistleblower', 'likely', 'refused', 'entry', 'uk')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (20, 'uk') has the max prob in order to be the next word. The prob is: 0.08068646490573883
931300 The nn predicts (tensor([20]), 'uk') the next word with max prob: tensor([0.0807])
The negative loglikelihood (-log p(uk)): 2.5171844959259033 or 2.5171844959259033 or 2.5171844959259033
('extrajudicial', 'killings', 'continue', 'unabated', 'war', 'torn')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5878, 'torn') has the max prob in order to be the next word. The prob is: 1.6442962078144774e-05
931400 The nn predicts (tensor([331]), 'crimes') the next word with max prob: tensor([0.0696])
The negative loglikelihood (-log p(torn)): 11.015612602233887 or 11.015612602233887 or 11.015612602233887
('<s>', 'syria', 'us', 'arm', 'rebels', 'latest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (369, 'latest') has the max prob in order to be the next word. The prob is: 0.00020002765813842416
931500 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1177])
The negative loglikelihood (-log p(latest)): 8.517054557800293 or 8.517054557800293 or 8.517054557800293
('aiding', 'us', 'security', 'services', 'cyber', 'attacks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (60, 'attacks') has the max prob in order to be the next word. The prob is: 0.13971692323684692
931600 The nn predicts (tensor([60]), 'attacks') the next word with max prob: tensor([0.1397])
The negative loglikelihood (-log p(attacks)): 1.9681369066238403 or 1.9681369066238403 or 1.9681369066238403
('work', 'together', 'promote', 'family', 'values', 'based')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1653, 'based') has the max prob in order to be the next word. The prob is: 0.0002336970646865666
931700 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0136])
The negative loglikelihood (-log p(based)): 8.36148452758789 or 8.36148452758789 or 8.36148452758789
('italians', 'say', 'prism', 'would', 'illegal', 'eu')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (43, 'eu') has the max prob in order to be the next word. The prob is: 0.0036035000812262297
931800 The nn predicts (tensor([174]), 'use') the next word with max prob: tensor([0.0165])
The negative loglikelihood (-log p(eu)): 5.625849723815918 or 5.625849723815918 or 5.625849723815918
('crocheted', 'doily', 'appeared', 'railway', 'bridge', 'bristol')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10614, 'bristol') has the max prob in order to be the next word. The prob is: 1.806172576834797e-06
931900 The nn predicts (tensor([867]), 'park') the next word with max prob: tensor([0.0181])
The negative loglikelihood (-log p(bristol)): 13.224300384521484 or 13.224300384521484 or 13.224300384521484
('snowden', 'leaks', 'china', 'could', 'affect', 'role')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (539, 'role') has the max prob in order to be the next word. The prob is: 0.0006865294999442995
932000 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0251])
The negative loglikelihood (-log p(role)): 7.28386116027832 or 7.28386116027832 or 7.28386116027832
('opposition', 'obama', 'war', 'syria', 'say', 'us')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3, 'us') has the max prob in order to be the next word. The prob is: 0.018369169905781746
932100 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.0804])
The negative loglikelihood (-log p(us)): 3.9970815181732178 or 3.9970815181732178 or 3.9970815181732178
('student', 'debt', 'raise', 'interest', 'rates', 'argument')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5680, 'argument') has the max prob in order to be the next word. The prob is: 2.6361129130236804e-05
932200 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0162])
The negative loglikelihood (-log p(argument)): 10.543620109558105 or 10.543620109558105 or 10.543620109558105
('brazilian', 'military', 'police', 'brutalizing', 'protesters', 'são')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8465, 'são') has the max prob in order to be the next word. The prob is: 4.360474576969864e-06
932300 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0359])
The negative loglikelihood (-log p(são)): 12.34292984008789 or 12.34292984008789 or 12.34292984008789
('would', 'member', 'suppliers', 'group', 'signed', 'nuclear')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (32, 'nuclear') has the max prob in order to be the next word. The prob is: 0.00014394789468497038
932400 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0144])
The negative loglikelihood (-log p(nuclear)): 8.84605884552002 or 8.84605884552002 or 8.84605884552002
('share', 'nile', 'river', 'waters', 'raising', 'political')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (178, 'political') has the max prob in order to be the next word. The prob is: 0.0002593636454548687
932500 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0165])
The negative loglikelihood (-log p(political)): 8.257279396057129 or 8.257279396057129 or 8.257279396057129
('assange', 'provides', 'times', 'india', 'receipt', 'issued')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1315, 'issued') has the max prob in order to be the next word. The prob is: 0.000491637212689966
932600 The nn predicts (tensor([641]), 'surveillance') the next word with max prob: tensor([0.0165])
The negative loglikelihood (-log p(issued)): 7.617769718170166 or 7.617769718170166 or 7.617769718170166
('attention', 'new', 'menace', 'whose', 'development', 'hope')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1177, 'hope') has the max prob in order to be the next word. The prob is: 0.00016182426770683378
932700 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0185])
The negative loglikelihood (-log p(hope)): 8.728999137878418 or 8.728999137878418 or 8.728999137878418
('nuclear', 'safety', 'rules', 'amid', 'international', 'debate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (808, 'debate') has the max prob in order to be the next word. The prob is: 0.00403977744281292
932800 The nn predicts (tensor([57]), 'security') the next word with max prob: tensor([0.0499])
The negative loglikelihood (-log p(debate)): 5.511565685272217 or 5.511565685272217 or 5.511565685272217
('enough', 'nail', 'accused', 'sexual', 'assault', 'bombay')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12229, 'bombay') has the max prob in order to be the next word. The prob is: 5.859982593392488e-06
932900 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0435])
The negative loglikelihood (-log p(bombay)): 12.047364234924316 or 12.047364234924316 or 12.047364234924316
('justice', 'merciless', 'oppression', 'rare', 'public', 'statement')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1226, 'statement') has the max prob in order to be the next word. The prob is: 0.0003534638381097466
933000 The nn predicts (tensor([57]), 'security') the next word with max prob: tensor([0.0129])
The negative loglikelihood (-log p(statement)): 7.947729587554932 or 7.947729587554932 or 7.947729587554932
('deadline', 'july', 'emerged', 'expatriates', 'western', 'province')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (842, 'province') has the max prob in order to be the next word. The prob is: 0.0016558549832552671
933100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0383])
The negative loglikelihood (-log p(province)): 6.403437614440918 or 6.403437614440918 or 6.403437614440918
('material', 'court', 'sentenced', 'one', 'offender', 'days')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (210, 'days') has the max prob in order to be the next word. The prob is: 0.0007105190888978541
933200 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0100])
The negative loglikelihood (-log p(days)): 7.249514579772949 or 7.249514579772949 or 7.249514579772949
('ass', 'general', 'calls', 'officers', 'army', 'disgraceful')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14015, 'disgraceful') has the max prob in order to be the next word. The prob is: 4.9197783482668456e-06
933300 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.0374])
The negative loglikelihood (-log p(disgraceful)): 12.222247123718262 or 12.222247123718262 or 12.222247123718262
('syrian', 'rebels', 'embroiling', 'yet', 'another', 'war')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17, 'war') has the max prob in order to be the next word. The prob is: 0.06840884685516357
933400 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0684])
The negative loglikelihood (-log p(war)): 2.682253122329712 or 2.682253122329712 or 2.682253122329712
('kraft', 'give', 'superbowl', 'ring', 'putin', 'putin')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (81, 'putin') has the max prob in order to be the next word. The prob is: 0.004023829475045204
933500 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0204])
The negative loglikelihood (-log p(putin)): 5.515521049499512 or 5.515521049499512 or 5.515521049499512
('example', 'scotland', 'clear', 'debate', 'less', 'drama')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4981, 'drama') has the max prob in order to be the next word. The prob is: 3.6813493352383375e-05
933600 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0349])
The negative loglikelihood (-log p(drama)): 10.209646224975586 or 10.209646224975586 or 10.209646224975586
('rouhani', 'seen', 'best', 'hope', 'ending', 'nuclear')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (32, 'nuclear') has the max prob in order to be the next word. The prob is: 0.0016394921112805605
933700 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0730])
The negative loglikelihood (-log p(nuclear)): 6.4133687019348145 or 6.4133687019348145 or 6.4133687019348145
('work', 'exposed', 'alleged', 'human', 'rights', 'abuses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1848, 'abuses') has the max prob in order to be the next word. The prob is: 0.013548486866056919
933800 The nn predicts (tensor([62]), 'group') the next word with max prob: tensor([0.0888])
The negative loglikelihood (-log p(abuses)): 4.301480293273926 or 4.301480293273926 or 4.301480293273926
('cairo', 'closed', 'morsi', 'also', 'called', 'lebanon')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (534, 'lebanon') has the max prob in order to be the next word. The prob is: 0.00021369529713410884
933900 The nn predicts (tensor([300]), 'friday') the next word with max prob: tensor([0.0308])
The negative loglikelihood (-log p(lebanon)): 8.450959205627441 or 8.450959205627441 or 8.450959205627441
('philippine', 'coast', 'guard', 'officers', 'may', 'minister')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (25, 'minister') has the max prob in order to be the next word. The prob is: 1.5584540960844606e-05
934000 The nn predicts (tensor([148]), 'take') the next word with max prob: tensor([0.0218])
The negative loglikelihood (-log p(minister)): 11.069231033325195 or 11.069231033325195 or 11.069231033325195
('people', 'suspected', 'trying', 'collect', 'ransom', 'held')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (292, 'held') has the max prob in order to be the next word. The prob is: 0.0003690353187266737
934100 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0098])
The negative loglikelihood (-log p(held)): 7.904618263244629 or 7.904618263244629 or 7.904618263244629
('former', 'south', 'african', 'president', 'nelson', 'mandela')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1485, 'mandela') has the max prob in order to be the next word. The prob is: 0.006344156339764595
934200 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0932])
The negative loglikelihood (-log p(mandela)): 5.060221195220947 or 5.060221195220947 or 5.060221195220947
('jihadists', 'bomb', 'bus', 'carrying', 'schoolgirls', 'launch')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (367, 'launch') has the max prob in order to be the next word. The prob is: 0.00019188101578038186
934300 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0522])
The negative loglikelihood (-log p(launch)): 8.558634757995605 or 8.558634757995605 or 8.558634757995605
('activists', 'turkey', 'claim', 'police', 'targeted', 'medical')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (826, 'medical') has the max prob in order to be the next word. The prob is: 0.00019854478887282312
934400 The nn predicts (tensor([313]), 'chemical') the next word with max prob: tensor([0.0317])
The negative loglikelihood (-log p(medical)): 8.524496078491211 or 8.524496078491211 or 8.524496078491211
('villagers', 'central', 'african', 'republic', 'military', 'source')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (997, 'source') has the max prob in order to be the next word. The prob is: 0.000718320079613477
934500 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.1120])
The negative loglikelihood (-log p(source)): 7.238595485687256 or 7.238595485687256 or 7.238595485687256
('president', 'pocketed', 'super', 'bowl', 'ring', 'belonged')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9690, 'belonged') has the max prob in order to be the next word. The prob is: 1.9144918042002246e-05
934600 The nn predicts (tensor([98]), 'arrested') the next word with max prob: tensor([0.0073])
The negative loglikelihood (-log p(belonged)): 10.863472938537598 or 10.863472938537598 or 10.863472938537598
('warned', 'international', 'pressure', 'iran', 'must', 'loosened')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (20981, 'loosened') has the max prob in order to be the next word. The prob is: 1.5566884030704387e-06
934700 The nn predicts (tensor([148]), 'take') the next word with max prob: tensor([0.0178])
The negative loglikelihood (-log p(loosened)): 13.372949600219727 or 13.372949600219727 or 13.372949600219727
('nostra', 'boss', 'matteo', 'messina', 'denaro', 'believed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1320, 'believed') has the max prob in order to be the next word. The prob is: 0.0002443963021505624
934800 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0140])
The negative loglikelihood (-log p(believed)): 8.316719055175781 or 8.316719055175781 or 8.316719055175781
('muslim', 'states', 'including', 'saudi', 'arabia', 'egypt')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (46, 'egypt') has the max prob in order to be the next word. The prob is: 0.0006967539084143937
934900 The nn predicts (tensor([188]), 'arabia') the next word with max prob: tensor([0.0113])
The negative loglikelihood (-log p(egypt)): 7.269078254699707 or 7.269078254699707 or 7.269078254699707
('another', 'blogger', 'abusing', 'democratic', 'freedoms', 'posting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3564, 'posting') has the max prob in order to be the next word. The prob is: 0.00016471445269417018
935000 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(posting)): 8.711297035217285 or 8.711297035217285 or 8.711297035217285
('czech', 'pm', 'petr', 'necas', 'resign', 'aide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2534, 'aide') has the max prob in order to be the next word. The prob is: 0.0006034522084519267
935100 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0260])
The negative loglikelihood (-log p(aide)): 7.412843704223633 or 7.412843704223633 or 7.412843704223633
('china', 'plans', 'move', 'million', 'rural', 'residents')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (729, 'residents') has the max prob in order to be the next word. The prob is: 0.0061835660599172115
935200 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0237])
The negative loglikelihood (-log p(residents)): 5.085860252380371 or 5.085860252380371 or 5.085860252380371
('illegal', 'european', 'parliament', 'made', 'disconnections', 'unquestionably')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (42682, 'unquestionably') has the max prob in order to be the next word. The prob is: 1.3471957345245755e-06
935300 The nn predicts (tensor([20]), 'uk') the next word with max prob: tensor([0.0411])
The negative loglikelihood (-log p(unquestionably)): 13.517485618591309 or 13.517485618591309 or 13.517485618591309
('people', 'killed', 'recent', 'protests', 'turkey', 'laid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4466, 'laid') has the max prob in order to be the next word. The prob is: 3.5229913919465616e-05
935400 The nn predicts (tensor([1204]), 'istanbul') the next word with max prob: tensor([0.0352])
The negative loglikelihood (-log p(laid)): 10.253615379333496 or 10.253615379333496 or 10.253615379333496
('alberto', 'fujimori', 'according', 'poll', 'released', 'sunday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (361, 'sunday') has the max prob in order to be the next word. The prob is: 0.01215396635234356
935500 The nn predicts (tensor([77]), 'former') the next word with max prob: tensor([0.0409])
The negative loglikelihood (-log p(sunday)): 4.410099506378174 or 4.410099506378174 or 4.410099506378174
('effects', 'although', 'interesting', 'findings', 'effect', 'cooperative')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16082, 'cooperative') has the max prob in order to be the next word. The prob is: 4.9429422688263e-06
935600 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(cooperative)): 12.217550277709961 or 12.217550277709961 or 12.217550277709961
('offer', 'peace', 'talks', 'government', 'still', 'mourning')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5237, 'mourning') has the max prob in order to be the next word. The prob is: 2.0213374227751046e-05
935700 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0135])
The negative loglikelihood (-log p(mourning)): 10.809165954589844 or 10.809165954589844 or 10.809165954589844
('struggle', 'pollution', 'china', 'newly', 'announced', 'measures')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1026, 'measures') has the max prob in order to be the next word. The prob is: 0.0004632502677850425
935800 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0285])
The negative loglikelihood (-log p(measures)): 7.677243232727051 or 7.677243232727051 or 7.677243232727051
('scientists', 'people', 'fund', 'aims', 'usher', 'new')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4, 'new') has the max prob in order to be the next word. The prob is: 0.023591654375195503
935900 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0236])
The negative loglikelihood (-log p(new)): 3.7468621730804443 or 3.7468621730804443 or 3.7468621730804443
('montreal', 'mayor', 'arrested', 'quebec', 'corruption', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.012325466610491276
936000 The nn predicts (tensor([436]), 'scandal') the next word with max prob: tensor([0.0481])
The negative loglikelihood (-log p(police)): 4.396087646484375 or 4.396087646484375 or 4.396087646484375
('turkish', 'government', 'says', 'may', 'use', 'army')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (76, 'army') has the max prob in order to be the next word. The prob is: 0.0011065865401178598
936100 The nn predicts (tensor([313]), 'chemical') the next word with max prob: tensor([0.1094])
The negative loglikelihood (-log p(army)): 6.8064751625061035 or 6.8064751625061035 or 6.8064751625061035
('ecuador', 'fail', 'break', 'deadlock', 'wikileaks', 'founder')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1398, 'founder') has the max prob in order to be the next word. The prob is: 0.007882892154157162
936200 The nn predicts (tensor([766]), 'assange') the next word with max prob: tensor([0.0211])
The negative loglikelihood (-log p(founder)): 4.843060493469238 or 4.843060493469238 or 4.843060493469238
('misleadingly', 'claims', 'chinese', 'academy', 'sciences', 'support')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (189, 'support') has the max prob in order to be the next word. The prob is: 0.0008711607661098242
936300 The nn predicts (tensor([41]), 'chinese') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(support)): 7.045683860778809 or 7.045683860778809 or 7.045683860778809
('iranian', 'hassan', 'rouhani', 'pledges', 'path', 'moderation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17538, 'moderation') has the max prob in order to be the next word. The prob is: 3.4178433452325407e-06
936400 The nn predicts (tensor([273]), 'assad') the next word with max prob: tensor([0.0709])
The negative loglikelihood (-log p(moderation)): 12.586501121520996 or 12.586501121520996 or 12.586501121520996
('hit', 'bangladesh', 'garment', 'workers', 'doctors', 'say')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (53, 'say') has the max prob in order to be the next word. The prob is: 0.013947056606411934
936500 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0647])
The negative loglikelihood (-log p(say)): 4.272486686706543 or 4.272486686706543 or 4.272486686706543
('sang', 'recent', 'documentary', 'vice', 'called', 'world')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7, 'world') has the max prob in order to be the next word. The prob is: 0.0016929818084463477
936600 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0448])
The negative loglikelihood (-log p(world)): 6.381263732910156 or 6.381263732910156 or 6.381263732910156
('replace', 'patriots', 'owner', 'kraft', 'stolen', 'super')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2237, 'super') has the max prob in order to be the next word. The prob is: 4.889610499958508e-05
936700 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(super)): 9.925812721252441 or 9.925812721252441 or 9.925812721252441
('local', 'chinese', 'officials', 'turn', 'black', 'pr')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4785, 'pr') has the max prob in order to be the next word. The prob is: 8.840990631142631e-06
936800 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0358])
The negative loglikelihood (-log p(pr)): 11.63611125946045 or 11.63611125946045 or 11.63611125946045
('melting', 'ice', 'shelves', 'making', 'vulnerable', 'collapse')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (714, 'collapse') has the max prob in order to be the next word. The prob is: 0.00014079833636060357
936900 The nn predicts (tensor([168]), 'countries') the next word with max prob: tensor([0.0300])
The negative loglikelihood (-log p(collapse)): 8.868182182312012 or 8.868182182312012 or 8.868182182312012
('illegally', 'smuggling', 'weapons', 'ecuador', 'croatia', 'violation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2883, 'violation') has the max prob in order to be the next word. The prob is: 0.0001526204141555354
937000 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0174])
The negative loglikelihood (-log p(violation)): 8.787556648254395 or 8.787556648254395 or 8.787556648254395
('newark', 'airport', 'poison', 'threat', 'daniel', 'perry')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13440, 'perry') has the max prob in order to be the next word. The prob is: 8.206858183257282e-06
937100 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0093])
The negative loglikelihood (-log p(perry)): 11.710540771484375 or 11.710540771484375 or 11.710540771484375
('unless', 'going', 'business', 'japanese', 'firms', 'barred')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2817, 'barred') has the max prob in order to be the next word. The prob is: 2.4039396521402523e-05
937200 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0148])
The negative loglikelihood (-log p(barred)): 10.63581657409668 or 10.63581657409668 or 10.63581657409668
('minister', 'office', 'sent', 'unsolicited', 'info', 'opposing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5241, 'opposing') has the max prob in order to be the next word. The prob is: 2.319804480066523e-05
937300 The nn predicts (tensor([20]), 'uk') the next word with max prob: tensor([0.0203])
The negative loglikelihood (-log p(opposing)): 10.671442985534668 or 10.671442985534668 or 10.671442985534668
('bosnian', 'serb', 'president', 'rejects', 'urgent', 'border')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (103, 'border') has the max prob in order to be the next word. The prob is: 4.8138474085135385e-05
937400 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0243])
The negative loglikelihood (-log p(border)): 9.941429138183594 or 9.941429138183594 or 9.941429138183594
('amanpour', 'cnn', 'lied', 'gezi', 'park', 'protests')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (96, 'protests') has the max prob in order to be the next word. The prob is: 0.036710672080516815
937500 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0367])
The negative loglikelihood (-log p(protests)): 3.304687738418579 or 3.304687738418579 or 3.304687738418579
('protesters', 'push', 'stop', 'billion', 'newmont', 'mine')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1185, 'mine') has the max prob in order to be the next word. The prob is: 0.0007765428745187819
937600 The nn predicts (tensor([64]), 'deal') the next word with max prob: tensor([0.0275])
The negative loglikelihood (-log p(mine)): 7.160658836364746 or 7.160658836364746 or 7.160658836364746
('canberra', 'windfarm', 'protesters', 'demand', 'end', 'renewable')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2013, 'renewable') has the max prob in order to be the next word. The prob is: 5.65095069759991e-05
937700 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0399])
The negative loglikelihood (-log p(renewable)): 9.78110122680664 or 9.78110122680664 or 9.78110122680664
('armed', 'forces', 'taking', 'lead', 'security', 'nationwide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2659, 'nationwide') has the max prob in order to be the next word. The prob is: 5.2996623708168045e-05
937800 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.3721])
The negative loglikelihood (-log p(nationwide)): 9.845282554626465 or 9.845282554626465 or 9.845282554626465
('ship', 'detained', 'ploughed', 'coral', 'reef', 'central')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (230, 'central') has the max prob in order to be the next word. The prob is: 0.0011760705383494496
937900 The nn predicts (tensor([112]), 'near') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(central)): 6.74557638168335 or 6.74557638168335 or 6.74557638168335
('afghans', 'take', 'national', 'security', 'nato', 'forces')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (54, 'forces') has the max prob in order to be the next word. The prob is: 0.15223923325538635
938000 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.1522])
The negative loglikelihood (-log p(forces)): 1.8823020458221436 or 1.8823020458221436 or 1.8823020458221436
('parliament', 'rejected', 'bill', 'would', 'allow', 'swiss')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (716, 'swiss') has the max prob in order to be the next word. The prob is: 0.00037687525036744773
938100 The nn predicts (tensor([312]), 'bill') the next word with max prob: tensor([0.0223])
The negative loglikelihood (-log p(swiss)): 7.883596420288086 or 7.883596420288086 or 7.883596420288086
('gone', 'far', 'fighting', 'climate', 'change', 'church')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (403, 'church') has the max prob in order to be the next word. The prob is: 0.0002772439329419285
938200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0141])
The negative loglikelihood (-log p(church)): 8.19061279296875 or 8.19061279296875 or 8.19061279296875
('opposition', 'forces', 'appear', 'credible', 'say', 'independent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1224, 'independent') has the max prob in order to be the next word. The prob is: 0.00028320966521278024
938300 The nn predicts (tensor([119]), 'protesters') the next word with max prob: tensor([0.0451])
The negative loglikelihood (-log p(independent)): 8.169322967529297 or 8.169322967529297 or 8.169322967529297
('taliban', 'signal', 'readiness', 'begin', 'peace', 'negotiations')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1502, 'negotiations') has the max prob in order to be the next word. The prob is: 0.0018460972933098674
938400 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.5114])
The negative loglikelihood (-log p(negotiations)): 6.294681549072266 or 6.294681549072266 or 6.294681549072266
('banks', 'pressured', 'brokers', 'spread', 'false', 'data')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (316, 'data') has the max prob in order to be the next word. The prob is: 0.007504265755414963
938500 The nn predicts (tensor([132]), 'media') the next word with max prob: tensor([0.0234])
The negative loglikelihood (-log p(data)): 4.8922834396362305 or 4.8922834396362305 or 4.8922834396362305
('turkey', 'turn', 'fake', 'doctors', 'without', 'license')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4836, 'license') has the max prob in order to be the next word. The prob is: 8.097007957985625e-05
938600 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0071])
The negative loglikelihood (-log p(license)): 9.421430587768555 or 9.421430587768555 or 9.421430587768555
('violence', 'home', 'countries', 'homosexual', 'muslims', 'flocking')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12950, 'flocking') has the max prob in order to be the next word. The prob is: 3.3385688311682316e-06
938700 The nn predicts (tensor([75]), 'women') the next word with max prob: tensor([0.0264])
The negative loglikelihood (-log p(flocking)): 12.609968185424805 or 12.609968185424805 or 12.609968185424805
('control', 'foreign', 'plot', 'reduce', 'turkish', 'population')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (616, 'population') has the max prob in order to be the next word. The prob is: 0.00038531768950633705
938800 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.3861])
The negative loglikelihood (-log p(population)): 7.861442565917969 or 7.861442565917969 or 7.861442565917969
('latest', 'violence', 'wrack', 'volatile', 'region', 'witnesses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2290, 'witnesses') has the max prob in order to be the next word. The prob is: 0.00014256079157348722
938900 The nn predicts (tensor([357]), 'region') the next word with max prob: tensor([0.0421])
The negative loglikelihood (-log p(witnesses)): 8.855742454528809 or 8.855742454528809 or 8.855742454528809
('eavesdropping', 'agency', 'helped', 'spy', 'documents', 'suggest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2480, 'suggest') has the max prob in order to be the next word. The prob is: 0.0004174956411588937
939000 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(suggest)): 7.78123664855957 or 7.78123664855957 or 7.78123664855957
('police', 'force', 'protests', 'took', 'place', 'last')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (105, 'last') has the max prob in order to be the next word. The prob is: 0.003570379689335823
939100 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0306])
The negative loglikelihood (-log p(last)): 5.635083198547363 or 5.635083198547363 or 5.635083198547363
('board', 'psychology', 'says', 'approving', 'gay', 'cure')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3631, 'cure') has the max prob in order to be the next word. The prob is: 5.861254976480268e-05
939200 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.1812])
The negative loglikelihood (-log p(cure)): 9.744562149047852 or 9.744562149047852 or 9.744562149047852
('recent', 'pipeline', 'break', 'leaked', 'gallons', 'oil')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (66, 'oil') has the max prob in order to be the next word. The prob is: 0.005662914365530014
939300 The nn predicts (tensor([180]), 'gas') the next word with max prob: tensor([0.0181])
The negative loglikelihood (-log p(oil)): 5.173816680908203 or 5.173816680908203 or 5.173816680908203
('national', 'football', 'coach', 'luis', 'felipe', 'scolari')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (27880, 'scolari') has the max prob in order to be the next word. The prob is: 6.644909717579139e-06
939400 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0078])
The negative loglikelihood (-log p(scolari)): 11.921659469604492 or 11.921659469604492 or 11.921659469604492
('india', 'army', 'air', 'force', 'evacuated', 'nearly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (392, 'nearly') has the max prob in order to be the next word. The prob is: 0.0006335208308883011
939500 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.0204])
The negative loglikelihood (-log p(nearly)): 7.364217758178711 or 7.364217758178711 or 7.364217758178711
('nation', 'economy', 'much', 'greater', 'expected', 'writes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4650, 'writes') has the max prob in order to be the next word. The prob is: 7.598597676405916e-06
939600 The nn predicts (tensor([137]), 'end') the next word with max prob: tensor([0.0231])
The negative loglikelihood (-log p(writes)): 11.78754711151123 or 11.78754711151123 or 11.78754711151123
('company', 'grasberg', 'mine', 'deadly', 'cave', 'killed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13, 'killed') has the max prob in order to be the next word. The prob is: 0.0020715456921607256
939700 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(killed)): 6.179460048675537 or 6.179460048675537 or 6.179460048675537
('imf', 'calls', 'urgent', 'steps', 'spain', 'unemployment')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1812, 'unemployment') has the max prob in order to be the next word. The prob is: 0.0006943243206478655
939800 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.0287])
The negative loglikelihood (-log p(unemployment)): 7.272571563720703 or 7.272571563720703 or 7.272571563720703
('woman', 'alone', 'street', 'aggressively', 'pepper', 'sprayed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9642, 'sprayed') has the max prob in order to be the next word. The prob is: 1.238606000697473e-05
939900 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0116])
The negative loglikelihood (-log p(sprayed)): 11.298938751220703 or 11.298938751220703 or 11.298938751220703
('knowing', 'need', 'find', 'might', 'threat', 'earth')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (647, 'earth') has the max prob in order to be the next word. The prob is: 0.0004660678096115589
940000 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0133])
The negative loglikelihood (-log p(earth)): 7.671179294586182 or 7.671179294586182 or 7.671179294586182
('turkey', 'top', 'officials', 'threaten', 'military', 'crackdown')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (538, 'crackdown') has the max prob in order to be the next word. The prob is: 0.005598078481853008
940100 The nn predicts (tensor([204]), 'force') the next word with max prob: tensor([0.0229])
The negative loglikelihood (-log p(crackdown)): 5.18533182144165 or 5.18533182144165 or 5.18533182144165
('always', 'knew', 'english', 'politics', 'comes', 'another')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (270, 'another') has the max prob in order to be the next word. The prob is: 0.0009890879737213254
940200 The nn predicts (tensor([302]), 'way') the next word with max prob: tensor([0.0213])
The negative loglikelihood (-log p(another)): 6.918727397918701 or 6.918727397918701 or 6.918727397918701
('<s>', 'afghanistan', 'boycott', 'peace', 'talks', 'taliban')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (185, 'taliban') has the max prob in order to be the next word. The prob is: 0.006930298637598753
940300 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1465])
The negative loglikelihood (-log p(taliban)): 4.9718523025512695 or 4.9718523025512695 or 4.9718523025512695
('sentenced', 'jail', 'tax', 'evasion', 'bbc', 'news')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (24, 'news') has the max prob in order to be the next word. The prob is: 0.3703762888908386
940400 The nn predicts (tensor([24]), 'news') the next word with max prob: tensor([0.3704])
The negative loglikelihood (-log p(news)): 0.9932358264923096 or 0.9932357668876648 or 0.9932357668876648
('kong', 'investigates', 'hsbc', 'banks', 'inappropriate', 'market')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (487, 'market') has the max prob in order to be the next word. The prob is: 0.00026844587409868836
940500 The nn predicts (tensor([316]), 'data') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(market)): 8.222861289978027 or 8.222861289978027 or 8.222861289978027
('criminal', 'offense', 'recklessly', 'mismanage', 'local', 'financial')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (533, 'financial') has the max prob in order to be the next word. The prob is: 0.0006221072981134057
940600 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0639])
The negative loglikelihood (-log p(financial)): 7.3823981285095215 or 7.3823981285095215 or 7.3823981285095215
('looks', 'shared', 'values', 'iron', 'curtain', 'era')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2282, 'era') has the max prob in order to be the next word. The prob is: 7.500743231503293e-05
940700 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0137])
The negative loglikelihood (-log p(era)): 9.497922897338867 or 9.497922897338867 or 9.497922897338867
('iranians', 'close', 'new', 'president', 'rouhani', 'call')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (240, 'call') has the max prob in order to be the next word. The prob is: 0.0006518902955576777
940800 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0279])
The negative loglikelihood (-log p(call)): 7.335634231567383 or 7.335634231567383 or 7.335634231567383
('são', 'paulo', 'rio', 'de', 'janeiro', 'agreed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1180, 'agreed') has the max prob in order to be the next word. The prob is: 0.0003363991272635758
940900 The nn predicts (tensor([867]), 'park') the next word with max prob: tensor([0.0418])
The negative loglikelihood (-log p(agreed)): 7.9972124099731445 or 7.9972124099731445 or 7.9972124099731445
('draw', 'attention', 'protests', 'larger', 'issue', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.0038378050085157156
941000 The nn predicts (tensor([186]), 'public') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(police)): 5.562854766845703 or 5.562854766845703 or 5.562854766845703
('sao', 'paulo', 'rio', 'officials', 'reverse', 'subway')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4375, 'subway') has the max prob in order to be the next word. The prob is: 5.689401223207824e-05
941100 The nn predicts (tensor([57]), 'security') the next word with max prob: tensor([0.0120])
The negative loglikelihood (-log p(subway)): 9.774320602416992 or 9.774320602416992 or 9.774320602416992
('honor', 'prestigious', 'award', 'went', 'mastermind', 'behind')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (384, 'behind') has the max prob in order to be the next word. The prob is: 0.00158684141933918
941200 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0153])
The negative loglikelihood (-log p(behind)): 6.446009635925293 or 6.446009635925293 or 6.446009635925293
('police', 'death', 'son', 'stabbed', 'another', 'prisoner')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1573, 'prisoner') has the max prob in order to be the next word. The prob is: 9.774183126864955e-05
941300 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0490])
The negative loglikelihood (-log p(prisoner)): 9.23318099975586 or 9.23318099975586 or 9.23318099975586
('pirate', 'bay', 'founder', 'sentenced', 'years', 'sweden')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (618, 'sweden') has the max prob in order to be the next word. The prob is: 0.00010397667938377708
941400 The nn predicts (tensor([194]), 'prison') the next word with max prob: tensor([0.4629])
The negative loglikelihood (-log p(sweden)): 9.171343803405762 or 9.171343803405762 or 9.171343803405762
('david', 'beckham', 'appearance', 'chinese', 'university', 'triggers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4469, 'triggers') has the max prob in order to be the next word. The prob is: 3.799611658905633e-05
941500 The nn predicts (tensor([867]), 'park') the next word with max prob: tensor([0.0059])
The negative loglikelihood (-log p(triggers)): 10.17802619934082 or 10.17802619934082 or 10.17802619934082
('african', 'republic', 'might', 'another', 'country', 'heart')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1454, 'heart') has the max prob in order to be the next word. The prob is: 8.987668843474239e-05
941600 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0175])
The negative loglikelihood (-log p(heart)): 9.317071914672852 or 9.317071914672852 or 9.317071914672852
('leftist', 'ruling', 'party', 'born', 'protests', 'perplexed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15213, 'perplexed') has the max prob in order to be the next word. The prob is: 2.908286887759459e-06
941700 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(perplexed)): 12.747946739196777 or 12.747946739196777 or 12.747946739196777
('fled', 'uganda', 'alleged', 'recruitment', 'congolese', 'rebel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (459, 'rebel') has the max prob in order to be the next word. The prob is: 0.006702383980154991
941800 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0437])
The negative loglikelihood (-log p(rebel)): 5.005291938781738 or 5.005291938781738 or 5.005291938781738
('isthmus', 'ever', 'since', 'french', 'american', 'dutch')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (524, 'dutch') has the max prob in order to be the next word. The prob is: 0.00014607732009608299
941900 The nn predicts (tensor([182]), 'states') the next word with max prob: tensor([0.0396])
The negative loglikelihood (-log p(dutch)): 8.831374168395996 or 8.831374168395996 or 8.831374168395996
('people', 'expected', 'use', 'browser', 'feature', 'translate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6190, 'translate') has the max prob in order to be the next word. The prob is: 7.413861021632329e-05
942000 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0152])
The negative loglikelihood (-log p(translate)): 9.509573936462402 or 9.509573936462402 or 9.509573936462402
('warned', 'air', 'pollution', 'levels', 'reached', 'record')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (286, 'record') has the max prob in order to be the next word. The prob is: 0.004031558986753225
942100 The nn predicts (tensor([915]), 'levels') the next word with max prob: tensor([0.0411])
The negative loglikelihood (-log p(record)): 5.513602256774902 or 5.513602256774902 or 5.513602256774902
('coalition', 'greece', 'gov', 'potentially', 'changing', 'system')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (333, 'system') has the max prob in order to be the next word. The prob is: 0.0010646346490830183
942200 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0329])
The negative loglikelihood (-log p(system)): 6.845123767852783 or 6.845123767852783 or 6.845123767852783
('shows', 'direct', 'ancestor', 'george', 'george', 'bush')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1235, 'bush') has the max prob in order to be the next word. The prob is: 0.00028329703491181135
942300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0220])
The negative loglikelihood (-log p(bush)): 8.169014930725098 or 8.169014930725098 or 8.169014930725098
('wary', 'endemic', 'corruption', 'expanding', 'government', 'role')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (539, 'role') has the max prob in order to be the next word. The prob is: 0.0008699203026480973
942400 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0255])
The negative loglikelihood (-log p(role)): 7.047109127044678 or 7.047109127044678 or 7.047109127044678
('security', 'issues', 'range', 'separatist', 'groups', 'chinese')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (41, 'chinese') has the max prob in order to be the next word. The prob is: 0.0007068511331453919
942500 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0254])
The negative loglikelihood (-log p(chinese)): 7.254690647125244 or 7.254690647125244 or 7.254690647125244
('ukip', 'party', 'rise', 'great', 'britain', 'stands')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2889, 'stands') has the max prob in order to be the next word. The prob is: 8.08969052741304e-05
942600 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0126])
The negative loglikelihood (-log p(stands)): 9.422334671020508 or 9.422334671020508 or 9.422334671020508
('american', 'expats', 'clear', 'business', 'opportunities', 'abound')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12838, 'abound') has the max prob in order to be the next word. The prob is: 5.761832198913908e-06
942700 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0089])
The negative loglikelihood (-log p(abound)): 12.064254760742188 or 12.064254760742188 or 12.064254760742188
('smog', 'hit', 'levels', 'posing', 'potentially', 'deadly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (278, 'deadly') has the max prob in order to be the next word. The prob is: 0.0010614488273859024
942800 The nn predicts (tensor([915]), 'levels') the next word with max prob: tensor([0.0409])
The negative loglikelihood (-log p(deadly)): 6.84812068939209 or 6.84812068939209 or 6.84812068939209
('china', 'denounces', 'philippines', 'illegal', 'occupation', 'reef')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1799, 'reef') has the max prob in order to be the next word. The prob is: 7.251438364619389e-05
942900 The nn predicts (tensor([120]), 'thousands') the next word with max prob: tensor([0.0110])
The negative loglikelihood (-log p(reef)): 9.531725883483887 or 9.531725883483887 or 9.531725883483887
('america', 'north', 'korean', 'defector', 'tells', 'inspiring')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12318, 'inspiring') has the max prob in order to be the next word. The prob is: 2.8712070161418524e-06
943000 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0191])
The negative loglikelihood (-log p(inspiring)): 12.760778427124023 or 12.760778427124023 or 12.760778427124023
('church', 'england', 'trying', 'recruit', 'pagans', 'spiritual')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5726, 'spiritual') has the max prob in order to be the next word. The prob is: 2.411160858173389e-05
943100 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0262])
The negative loglikelihood (-log p(spiritual)): 10.632817268371582 or 10.632817268371582 or 10.632817268371582
('haze', 'leave', 'hands', 'nature', 'let', 'pray')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5647, 'pray') has the max prob in order to be the next word. The prob is: 3.298719821032137e-05
943200 The nn predicts (tensor([201]), 'get') the next word with max prob: tensor([0.0163])
The negative loglikelihood (-log p(pray)): 10.319391250610352 or 10.319391250610352 or 10.319391250610352
('bodies', 'people', 'monsoon', 'flooding', 'landslides', 'devastated')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4941, 'devastated') has the max prob in order to be the next word. The prob is: 5.6831471738405526e-05
943300 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.1479])
The negative loglikelihood (-log p(devastated)): 9.775420188903809 or 9.775420188903809 or 9.775420188903809
('<s>', 'syrian', 'rebels', 'say', 'received', 'missiles')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (582, 'missiles') has the max prob in order to be the next word. The prob is: 0.0019000448519364
943400 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0220])
The negative loglikelihood (-log p(missiles)): 6.265877723693848 or 6.265877723693848 or 6.265877723693848
('big', 'maya', 'city', 'discovered', 'remote', 'jungle')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2983, 'jungle') has the max prob in order to be the next word. The prob is: 0.00039113371167331934
943500 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0130])
The negative loglikelihood (-log p(jungle)): 7.846461296081543 or 7.846461296081543 or 7.846461296081543
('guard', 'shoots', 'jewish', 'man', 'dead', 'jerusalem')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (556, 'jerusalem') has the max prob in order to be the next word. The prob is: 0.0017317854799330235
943600 The nn predicts (tensor([56]), 'man') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(jerusalem)): 6.358602523803711 or 6.358602523803711 or 6.358602523803711
('could', 'draw', 'powerful', 'regional', 'rivals', 'iran')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9, 'iran') has the max prob in order to be the next word. The prob is: 0.0005012833862565458
943700 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0219])
The negative loglikelihood (-log p(iran)): 7.598339080810547 or 7.598339080810547 or 7.598339080810547
('chanting', 'islamic', 'revolution', 'warning', 'new', 'bloody')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2129, 'bloody') has the max prob in order to be the next word. The prob is: 6.285740528255701e-05
943800 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0465])
The negative loglikelihood (-log p(bloody)): 9.674641609191895 or 9.674641609191895 or 9.674641609191895
('flooding', 'parts', 'canada', 'people', 'evacuated', 'multiple')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2171, 'multiple') has the max prob in order to be the next word. The prob is: 0.00016102723020594567
943900 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0329])
The negative loglikelihood (-log p(multiple)): 8.73393726348877 or 8.73393726348877 or 8.73393726348877
('war', 'z', 'zombie', 'blockbuster', 'opening', 'june')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1440, 'june') has the max prob in order to be the next word. The prob is: 0.001947014476172626
944000 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0344])
The negative loglikelihood (-log p(june)): 6.241457939147949 or 6.241457939147949 or 6.241457939147949
('ripped', 'roofs', 'buildings', 'bowled', 'trees', 'damaged')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2210, 'damaged') has the max prob in order to be the next word. The prob is: 0.0006095813005231321
944100 The nn predicts (tensor([619]), 'streets') the next word with max prob: tensor([0.0146])
The negative loglikelihood (-log p(damaged)): 7.402738094329834 or 7.402738094329834 or 7.402738094329834
('percent', 'across', 'indonesia', 'early', 'saturday', 'government')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11, 'government') has the max prob in order to be the next word. The prob is: 0.004381485749036074
944200 The nn predicts (tensor([300]), 'friday') the next word with max prob: tensor([0.0201])
The negative loglikelihood (-log p(government)): 5.430367469787598 or 5.430367469787598 or 5.430367469787598
('malaysian', 'singaporean', 'firms', 'hazardous', 'index', 'smoke')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2804, 'smoke') has the max prob in order to be the next word. The prob is: 0.00011801793152699247
944300 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0284])
The negative loglikelihood (-log p(smoke)): 9.044673919677734 or 9.044673919677734 or 9.044673919677734
('fire', 'companies', 'heaviest', 'smog', 'ever', 'cover')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1535, 'cover') has the max prob in order to be the next word. The prob is: 0.00022172057651914656
944400 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0096])
The negative loglikelihood (-log p(cover)): 8.414093017578125 or 8.414093017578125 or 8.414093017578125
('five', 'killed', 'new', 'mumbai', 'building', 'collapse')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (714, 'collapse') has the max prob in order to be the next word. The prob is: 0.016093304380774498
944500 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0226])
The negative loglikelihood (-log p(collapse)): 4.12935209274292 or 4.12935209274292 or 4.12935209274292
('hacking', 'scandal', 'went', 'well', 'beyond', 'journalists')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (485, 'journalists') has the max prob in order to be the next word. The prob is: 0.0002037770755123347
944600 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0136])
The negative loglikelihood (-log p(journalists)): 8.498483657836914 or 8.498483657836914 or 8.498483657836914
('politicians', 'federal', 'ministers', 'demand', 'clarification', 'uk')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (20, 'uk') has the max prob in order to be the next word. The prob is: 0.0029305496718734503
944700 The nn predicts (tensor([316]), 'data') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(uk)): 5.8325653076171875 or 5.8325653076171875 or 5.8325653076171875
('pm', 'erdogan', 'accuses', 'protestors', 'insulting', 'islam')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (720, 'islam') has the max prob in order to be the next word. The prob is: 0.013712313026189804
944800 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0250])
The negative loglikelihood (-log p(islam)): 4.289461135864258 or 4.289461135864258 or 4.289461135864258
('maya', 'city', 'discovered', 'jungles', 'mexico', 'hidden')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1794, 'hidden') has the max prob in order to be the next word. The prob is: 8.97069985512644e-05
944900 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.1009])
The negative loglikelihood (-log p(hidden)): 9.318962097167969 or 9.318962097167969 or 9.318962097167969
('yrs', 'international', 'diplomatic', 'efforts', 'despite', 'member')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (700, 'member') has the max prob in order to be the next word. The prob is: 9.08001238713041e-05
945000 The nn predicts (tensor([127]), 'peace') the next word with max prob: tensor([0.0311])
The negative loglikelihood (-log p(member)): 9.30685043334961 or 9.306849479675293 or 9.306849479675293
('conduct', 'given', 'rising', 'concern', 'state', 'democracy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (653, 'democracy') has the max prob in order to be the next word. The prob is: 0.0004073874733876437
945100 The nn predicts (tensor([132]), 'media') the next word with max prob: tensor([0.0688])
The negative loglikelihood (-log p(democracy)): 7.805745601654053 or 7.805745601654053 or 7.805745601654053
('could', 'lead', 'changes', 'ground', 'victories', 'president')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12, 'president') has the max prob in order to be the next word. The prob is: 0.0060165049508214
945200 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0134])
The negative loglikelihood (-log p(president)): 5.113248825073242 or 5.113248825073242 or 5.113248825073242
('testing', 'chinese', 'national', 'surveillance', 'machinery', 'tibet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1842, 'tibet') has the max prob in order to be the next word. The prob is: 0.0002618353464640677
945300 The nn predicts (tensor([486]), 'companies') the next word with max prob: tensor([0.0354])
The negative loglikelihood (-log p(tibet)): 8.247795104980469 or 8.247795104980469 or 8.247795104980469
('private', 'investigators', 'break', 'law', 'commercial', 'interests')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2336, 'interests') has the max prob in order to be the next word. The prob is: 0.0002925099106505513
945400 The nn predicts (tensor([486]), 'companies') the next word with max prob: tensor([0.0282])
The negative loglikelihood (-log p(interests)): 8.137012481689453 or 8.137012481689453 or 8.137012481689453
('take', 'part', 'international', 'conference', 'syria', 'geneva')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2160, 'geneva') has the max prob in order to be the next word. The prob is: 7.033220754237846e-05
945500 The nn predicts (tensor([267]), 'conflict') the next word with max prob: tensor([0.0358])
The negative loglikelihood (-log p(geneva)): 9.562280654907227 or 9.562280654907227 or 9.562280654907227
('new', 'democracy', 'government', 'succeeds', 'annihilated', 'together')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1689, 'together') has the max prob in order to be the next word. The prob is: 0.0006423986051231623
945600 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0131])
The negative loglikelihood (-log p(together)): 7.350301742553711 or 7.350301742553711 or 7.350301742553711
('protestors', 'france', 'fined', 'thousands', 'euros', 'friday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (300, 'friday') has the max prob in order to be the next word. The prob is: 0.0007164770504459739
945700 The nn predicts (tensor([120]), 'thousands') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(friday)): 7.241164207458496 or 7.241164207458496 or 7.241164207458496
('korean', 'town', 'kaesong', 'added', 'unesco', 'world')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7, 'world') has the max prob in order to be the next word. The prob is: 0.001840880373492837
945800 The nn predicts (tensor([85]), 'international') the next word with max prob: tensor([0.0083])
The negative loglikelihood (-log p(world)): 6.297511577606201 or 6.297511577606201 or 6.297511577606201
('egyptian', 'army', 'steps', 'demand', 'political', 'truce')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1230, 'truce') has the max prob in order to be the next word. The prob is: 0.0005830671289004385
945900 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0383])
The negative loglikelihood (-log p(truce)): 7.447208404541016 or 7.447208404541016 or 7.447208404541016
('agent', 'twitter', 'campaign', 'wonder', 'provokatörmelihgökçek', 'tag')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7303, 'tag') has the max prob in order to be the next word. The prob is: 2.1332323740352876e-05
946000 The nn predicts (tensor([198]), 'internet') the next word with max prob: tensor([0.0108])
The negative loglikelihood (-log p(tag)): 10.755287170410156 or 10.755287170410156 or 10.755287170410156
('smoke', 'blow', 'fires', 'indonesia', 'air', 'pollution')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (811, 'pollution') has the max prob in order to be the next word. The prob is: 0.008825995028018951
946100 The nn predicts (tensor([253]), 'crash') the next word with max prob: tensor([0.0585])
The negative loglikelihood (-log p(pollution)): 4.730053901672363 or 4.730053901672363 or 4.730053901672363
('erdogan', 'defends', 'riot', 'police', 'tactics', 'turkey')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (49, 'turkey') has the max prob in order to be the next word. The prob is: 0.005292785819619894
946200 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0212])
The negative loglikelihood (-log p(turkey)): 5.241410732269287 or 5.241410732269287 or 5.241410732269287
('suspected', 'abu', 'sayyaf', 'gunmen', 'abduct', 'sisters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5788, 'sisters') has the max prob in order to be the next word. The prob is: 4.259399793227203e-05
946300 The nn predicts (tensor([45]), 'pakistan') the next word with max prob: tensor([0.0259])
The negative loglikelihood (-log p(sisters)): 10.063796997070312 or 10.063796997070312 or 10.063796997070312
('hackers', 'watched', 'bath', 'says', 'woman', 'bbc')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (82, 'bbc') has the max prob in order to be the next word. The prob is: 0.0010291235521435738
946400 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0099])
The negative loglikelihood (-log p(bbc)): 6.879047870635986 or 6.879047870635986 or 6.879047870635986
('says', 'pakistan', 'food', 'emergency', 'donors', 'look')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1001, 'look') has the max prob in order to be the next word. The prob is: 0.00017368493718095124
946500 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0569])
The negative loglikelihood (-log p(look)): 8.658267974853516 or 8.658267974853516 or 8.658267974853516
('orange', 'rooms', 'halved', 'light', 'dark', 'sides')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1943, 'sides') has the max prob in order to be the next word. The prob is: 6.126990774646401e-05
946600 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0112])
The negative loglikelihood (-log p(sides)): 9.70022201538086 or 9.70022201538086 or 9.70022201538086
('foreign', 'tourists', 'attack', 'pakistan', 'interior', 'minister')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (25, 'minister') has the max prob in order to be the next word. The prob is: 0.5669364929199219
946700 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.5669])
The negative loglikelihood (-log p(minister)): 0.5675079822540283 or 0.5675079822540283 or 0.5675079822540283
('pause', 'button', 'recent', 'articles', 'global', 'warming')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1002, 'warming') has the max prob in order to be the next word. The prob is: 0.012303675524890423
946800 The nn predicts (tensor([287]), 'health') the next word with max prob: tensor([0.0329])
The negative loglikelihood (-log p(warming)): 4.397857189178467 or 4.397857189178467 or 4.397857189178467
('wikileaks', 'assange', 'says', 'snowden', 'healthy', 'safe')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1178, 'safe') has the max prob in order to be the next word. The prob is: 0.0003773922799155116
946900 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0215])
The negative loglikelihood (-log p(safe)): 7.882225513458252 or 7.882225513458252 or 7.882225513458252
('search', 'terms', 'censorship', 'still', 'thrives', 'online')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (338, 'online') has the max prob in order to be the next word. The prob is: 0.0008778099436312914
947000 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0295])
The negative loglikelihood (-log p(online)): 7.03808069229126 or 7.038080215454102 or 7.038080215454102
('court', 'backs', 'tv', 'license', 'fees', 'computers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3044, 'computers') has the max prob in order to be the next word. The prob is: 0.000263760972302407
947100 The nn predicts (tensor([220]), 'sex') the next word with max prob: tensor([0.0274])
The negative loglikelihood (-log p(computers)): 8.240467071533203 or 8.240467071533203 or 8.240467071533203
('program', 'exposes', 'europe', 'parliamentarians', 'check', 'work')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (298, 'work') has the max prob in order to be the next word. The prob is: 0.0011781195644289255
947200 The nn predicts (tensor([316]), 'data') the next word with max prob: tensor([0.0196])
The negative loglikelihood (-log p(work)): 6.743835926055908 or 6.743835926055908 or 6.743835926055908
('ire', 'local', 'community', 'prompted', 'arab', 'mks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15342, 'mks') has the max prob in order to be the next word. The prob is: 9.387363775203994e-07
947300 The nn predicts (tensor([1800]), 'spring') the next word with max prob: tensor([0.0328])
The negative loglikelihood (-log p(mks)): 13.878730773925781 or 13.878730773925781 or 13.878730773925781
('west', 'bank', 'facing', 'sharp', 'criticism', 'stating')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7447, 'stating') has the max prob in order to be the next word. The prob is: 9.563283128954936e-06
947400 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0247])
The negative loglikelihood (-log p(stating)): 11.557579040527344 or 11.557579040527344 or 11.557579040527344
('mosaic', 'pavement', 'painted', 'plaster', 'window', 'glass')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5177, 'glass') has the max prob in order to be the next word. The prob is: 9.630606655264273e-05
947500 The nn predicts (tensor([111]), 'fire') the next word with max prob: tensor([0.0166])
The negative loglikelihood (-log p(glass)): 9.247979164123535 or 9.247979164123535 or 9.247979164123535
('right', 'banks', 'take', 'beating', 'stock', 'market')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (487, 'market') has the max prob in order to be the next word. The prob is: 0.001786771696060896
947600 The nn predicts (tensor([545]), 'banks') the next word with max prob: tensor([0.0079])
The negative loglikelihood (-log p(market)): 6.32734489440918 or 6.32734489440918 or 6.32734489440918
('costa', 'rica', 'signs', 'free', 'trade', 'agreement')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (543, 'agreement') has the max prob in order to be the next word. The prob is: 0.005966292694211006
947700 The nn predicts (tensor([64]), 'deal') the next word with max prob: tensor([0.0354])
The negative loglikelihood (-log p(agreement)): 5.12162971496582 or 5.12162971496582 or 5.12162971496582
('<s>', 'rolling', 'robot', 'offers', 'help', 'farmers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1196, 'farmers') has the max prob in order to be the next word. The prob is: 0.0003649444552138448
947800 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0221])
The negative loglikelihood (-log p(farmers)): 7.915765285491943 or 7.915765285491943 or 7.915765285491943
('centralized', 'monitoring', 'system', 'cms', 'cms', 'human')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (90, 'human') has the max prob in order to be the next word. The prob is: 0.0006039435975253582
947900 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0347])
The negative loglikelihood (-log p(human)): 7.41202974319458 or 7.41202974319458 or 7.41202974319458
('scientists', 'say', 'amazed', 'find', 'example', 'sophisticated')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4986, 'sophisticated') has the max prob in order to be the next word. The prob is: 2.9322658519959077e-05
948000 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0088])
The negative loglikelihood (-log p(sophisticated)): 10.437150001525879 or 10.437150001525879 or 10.437150001525879
('kabul', 'taliban', 'claims', 'responsibility', 'attack', 'presidential')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (500, 'presidential') has the max prob in order to be the next word. The prob is: 0.00014180861762724817
948100 The nn predicts (tensor([45]), 'pakistan') the next word with max prob: tensor([0.0593])
The negative loglikelihood (-log p(presidential)): 8.861032485961914 or 8.861032485961914 or 8.861032485961914
('russia', 'voices', 'alarm', 'us', 'troops', 'jordan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (832, 'jordan') has the max prob in order to be the next word. The prob is: 0.002290361560881138
948200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1145])
The negative loglikelihood (-log p(jordan)): 6.07904577255249 or 6.07904577255249 or 6.07904577255249
('aid', 'defend', 'kingdom', 'foreign', 'minister', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.04657241329550743
948300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0521])
The negative loglikelihood (-log p(said)): 3.066746950149536 or 3.066746950149536 or 3.066746950149536
('<s>', 'israel', 'reopens', 'nablus', 'checkpoints', 'shooting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (567, 'shooting') has the max prob in order to be the next word. The prob is: 0.00021155188733246177
948400 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0877])
The negative loglikelihood (-log p(shooting)): 8.461040496826172 or 8.461040496826172 or 8.461040496826172
('comedians', 'regime', 'strong', 'enough', 'handle', 'joke')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4277, 'joke') has the max prob in order to be the next word. The prob is: 1.3805171874992084e-05
948500 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.1689])
The negative loglikelihood (-log p(joke)): 11.19046688079834 or 11.19046688079834 or 11.19046688079834
('every', 'single', 'person', 'cell', 'phone', 'internet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (198, 'internet') has the max prob in order to be the next word. The prob is: 0.001445817295461893
948600 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0158])
The negative loglikelihood (-log p(internet)): 6.539080619812012 or 6.539080619812012 or 6.539080619812012
('china', 'crash', 'continues', 'shanghai', 'composite', 'enters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2825, 'enters') has the max prob in order to be the next word. The prob is: 4.1958148358389735e-05
948700 The nn predicts (tensor([424]), 'indonesia') the next word with max prob: tensor([0.0159])
The negative loglikelihood (-log p(enters)): 10.078838348388672 or 10.078838348388672 or 10.078838348388672
('must', 'significantly', 'add', 'emissions', 'win', 'approval')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1964, 'approval') has the max prob in order to be the next word. The prob is: 0.00011381797958165407
948800 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0161])
The negative loglikelihood (-log p(approval)): 9.080909729003906 or 9.080909729003906 or 9.080909729003906
('<s>', 'musharraf', 'charged', 'bhutto', 'killing', 'case')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (177, 'case') has the max prob in order to be the next word. The prob is: 0.00406584981828928
948900 The nn predicts (tensor([632]), 'son') the next word with max prob: tensor([0.0374])
The negative loglikelihood (-log p(case)): 5.505132675170898 or 5.505132675170898 or 5.505132675170898
('afghan', 'taliban', 'talks', 'despite', 'kabul', 'attack')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15, 'attack') has the max prob in order to be the next word. The prob is: 0.016153234988451004
949000 The nn predicts (tensor([60]), 'attacks') the next word with max prob: tensor([0.0668])
The negative loglikelihood (-log p(attack)): 4.125635147094727 or 4.125635147094727 or 4.125635147094727
('china', 'shenzhou', 'capsule', 'falls', 'safely', 'back')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (115, 'back') has the max prob in order to be the next word. The prob is: 0.0031596466433256865
949100 The nn predicts (tensor([377]), 'flight') the next word with max prob: tensor([0.0126])
The negative loglikelihood (-log p(back)): 5.75729513168335 or 5.75729513168335 or 5.75729513168335
('envoy', 'syria', 'warns', 'extremely', 'serious', 'spillover')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10787, 'spillover') has the max prob in order to be the next word. The prob is: 8.526140845788177e-06
949200 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.0796])
The negative loglikelihood (-log p(spillover)): 11.67237377166748 or 11.67237377166748 or 11.67237377166748
('party', 'fought', 'control', 'rostrum', 'ahead', 'scheduled')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4346, 'scheduled') has the max prob in order to be the next word. The prob is: 0.00030623568454757333
949300 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.1570])
The negative loglikelihood (-log p(scheduled)): 8.091156005859375 or 8.091156005859375 or 8.091156005859375
('warns', 'annual', 'world', 'drug', 'report', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.13001564145088196
949400 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.1300])
The negative loglikelihood (-log p(says)): 2.040100574493408 or 2.040100574493408 or 2.040100574493408
('used', 'good', 'things', 'nasty', 'things', 'make')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (215, 'make') has the max prob in order to be the next word. The prob is: 0.0019189734011888504
949500 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0820])
The negative loglikelihood (-log p(make)): 6.255964756011963 or 6.255964756011963 or 6.255964756011963
('<s>', '<s>', '<s>', 'mandela', 'life', 'support')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (189, 'support') has the max prob in order to be the next word. The prob is: 0.00023120446712709963
949600 The nn predicts (tensor([238]), 'trial') the next word with max prob: tensor([0.0150])
The negative loglikelihood (-log p(support)): 8.372208595275879 or 8.372207641601562 or 8.372207641601562
('police', 'france', 'stabbing', 'wife', 'times', 'killing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (150, 'killing') has the max prob in order to be the next word. The prob is: 0.0008902620756998658
949700 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0112])
The negative loglikelihood (-log p(killing)): 7.0239949226379395 or 7.023994445800781 or 7.023994445800781
('vast', 'majority', 'malware', 'attacks', 'spawned', 'legit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19307, 'legit') has the max prob in order to be the next word. The prob is: 3.356143679411616e-06
949800 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0240])
The negative loglikelihood (-log p(legit)): 12.604718208312988 or 12.604718208312988 or 12.604718208312988
('invalid', 'passport', 'cancelling', 'snowden', 'passport', 'bullying')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5136, 'bullying') has the max prob in order to be the next word. The prob is: 4.67500212835148e-05
949900 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0425])
The negative loglikelihood (-log p(bullying)): 9.970695495605469 or 9.970695495605469 or 9.970695495605469
('australia', 'japan', 'court', 'battle', 'whaling', 'australia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (110, 'australia') has the max prob in order to be the next word. The prob is: 0.0009975151624530554
950000 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0076])
The negative loglikelihood (-log p(australia)): 6.910243034362793 or 6.910243034362793 or 6.910243034362793
('call', 'directions', 'east', 'west', 'germany', 'monitored')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5481, 'monitored') has the max prob in order to be the next word. The prob is: 1.714721292955801e-05
950100 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0717])
The negative loglikelihood (-log p(monitored)): 10.973674774169922 or 10.973674774169922 or 10.973674774169922
('bomb', 'blast', 'targeted', 'high', 'court', 'judge')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (663, 'judge') has the max prob in order to be the next word. The prob is: 0.00016573235916439444
950200 The nn predicts (tensor([45]), 'pakistan') the next word with max prob: tensor([0.0197])
The negative loglikelihood (-log p(judge)): 8.7051362991333 or 8.7051362991333 or 8.7051362991333
('south', 'korean', 'president', 'chun', 'doo', 'hwan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (63764, 'hwan') has the max prob in order to be the next word. The prob is: 3.577031066015479e-06
950300 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0081])
The negative loglikelihood (-log p(hwan)): 12.540977478027344 or 12.540977478027344 or 12.540977478027344
('messy', 'handover', 'power', 'volatile', 'nato', 'country')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35, 'country') has the max prob in order to be the next word. The prob is: 0.01139239314943552
950400 The nn predicts (tensor([357]), 'region') the next word with max prob: tensor([0.0310])
The negative loglikelihood (-log p(country)): 4.474809646606445 or 4.474809646606445 or 4.474809646606445
('italian', 'herb', 'mix', 'leaves', 'family', 'paralysed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8128, 'paralysed') has the max prob in order to be the next word. The prob is: 4.032190190628171e-05
950500 The nn predicts (tensor([199]), 'home') the next word with max prob: tensor([0.0081])
The negative loglikelihood (-log p(paralysed)): 10.118616104125977 or 10.118616104125977 or 10.118616104125977
('among', 'palestinians', 'living', 'territories', 'due', 'stance')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2243, 'stance') has the max prob in order to be the next word. The prob is: 5.685125142917968e-05
950600 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0218])
The negative loglikelihood (-log p(stance)): 9.77507209777832 or 9.77507209777832 or 9.77507209777832
('gas', 'disperse', 'protesters', 'belo', 'horizonte', 'confederations')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17596, 'confederations') has the max prob in order to be the next word. The prob is: 5.394822437665425e-06
950700 The nn predicts (tensor([324]), 'tuesday') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(confederations)): 12.130070686340332 or 12.130070686340332 or 12.130070686340332
('fails', 'within', 'bloc', 'ministers', 'said', 'bank')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (87, 'bank') has the max prob in order to be the next word. The prob is: 0.007764151785522699
950800 The nn predicts (tensor([324]), 'tuesday') the next word with max prob: tensor([0.1129])
The negative loglikelihood (-log p(bank)): 4.858238220214844 or 4.858238220214844 or 4.858238220214844
('five', 'years', 'says', 'pm', 'ivica', 'dacic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (36502, 'dacic') has the max prob in order to be the next word. The prob is: 1.6753718909967574e-06
950900 The nn predicts (tensor([109]), 'prime') the next word with max prob: tensor([0.0422])
The negative loglikelihood (-log p(dacic)): 13.29947566986084 or 13.29947566986084 or 13.29947566986084
('europe', 'one', 'set', 'rules', 'pays', 'bill')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (312, 'bill') has the max prob in order to be the next word. The prob is: 0.0018036271212622523
951000 The nn predicts (tensor([358]), 'money') the next word with max prob: tensor([0.0202])
The negative loglikelihood (-log p(bill)): 6.317955493927002 or 6.317955493927002 or 6.317955493927002
('drugs', 'staying', 'legal', 'surged', 'worldwide', 'yrs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2832, 'yrs') has the max prob in order to be the next word. The prob is: 9.116197907133028e-05
951100 The nn predicts (tensor([287]), 'health') the next word with max prob: tensor([0.0244])
The negative loglikelihood (-log p(yrs)): 9.302872657775879 or 9.302872657775879 or 9.302872657775879
('standstill', 'depleted', 'staffing', 'levels', 'government', 'offices')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2057, 'offices') has the max prob in order to be the next word. The prob is: 0.0005603345925919712
951200 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0328])
The negative loglikelihood (-log p(offices)): 7.486976623535156 or 7.486976623535156 or 7.486976623535156
('guilty', 'displaying', 'asians', 'sign', 'local', 'guardian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1408, 'guardian') has the max prob in order to be the next word. The prob is: 0.00015415695088449866
951300 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0604])
The negative loglikelihood (-log p(guardian)): 8.777539253234863 or 8.777539253234863 or 8.777539253234863
('drunken', 'haze', 'american', 'soldiers', 'viewed', 'korea')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22, 'korea') has the max prob in order to be the next word. The prob is: 0.0005530951311811805
951400 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0149])
The negative loglikelihood (-log p(korea)): 7.499980449676514 or 7.499980449676514 or 7.499980449676514
('least', 'five', 'times', 'likely', 'occurred', 'world')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7, 'world') has the max prob in order to be the next word. The prob is: 0.002075118711218238
951500 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0107])
The negative loglikelihood (-log p(world)): 6.177736759185791 or 6.177736759185791 or 6.177736759185791
('parliament', 'members', 'basis', 'copyright', 'abuse', 'claims')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (125, 'claims') has the max prob in order to be the next word. The prob is: 0.003497363766655326
951600 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0966])
The negative loglikelihood (-log p(claims)): 5.655745983123779 or 5.655745983123779 or 5.655745983123779
('politicians', 'alleged', 'corruption', 'brutality', 'often', 'including')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (237, 'including') has the max prob in order to be the next word. The prob is: 0.0016611827304586768
951700 The nn predicts (tensor([433]), 'young') the next word with max prob: tensor([0.0116])
The negative loglikelihood (-log p(including)): 6.400225639343262 or 6.400225639343262 or 6.400225639343262
('release', 'full', 'eichmann', 'files', 'german', 'intelligence')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (343, 'intelligence') has the max prob in order to be the next word. The prob is: 0.017425019294023514
951800 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.1250])
The negative loglikelihood (-log p(intelligence)): 4.0498480796813965 or 4.0498480796813965 or 4.0498480796813965
('death', 'scores', 'others', 'injured', 'riot', 'angry')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1412, 'angry') has the max prob in order to be the next word. The prob is: 0.0005719474866054952
951900 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0366])
The negative loglikelihood (-log p(angry)): 7.466463565826416 or 7.466463565826416 or 7.466463565826416
('port', 'sudan', 'military', 'supply', 'base', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.10771005600690842
952000 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1077])
The negative loglikelihood (-log p(syria)): 2.2283122539520264 or 2.2283122539520264 or 2.2283122539520264
('renews', 'un', 'disengagement', 'observer', 'force', 'israel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10, 'israel') has the max prob in order to be the next word. The prob is: 0.01620357669889927
952100 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.4583])
The negative loglikelihood (-log p(israel)): 4.122523307800293 or 4.122523307800293 or 4.122523307800293
('us', 'army', 'reviews', 'rules', 'engagement', 'cyber')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1176, 'cyber') has the max prob in order to be the next word. The prob is: 0.0008725046063773334
952200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0699])
The negative loglikelihood (-log p(cyber)): 7.044142723083496 or 7.044142723083496 or 7.044142723083496
('journalists', 'turkish', 'journalists', 'see', 'recep', 'tayyip')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3275, 'tayyip') has the max prob in order to be the next word. The prob is: 0.00042520338320173323
952300 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0202])
The negative loglikelihood (-log p(tayyip)): 7.762942790985107 or 7.762942790985107 or 7.762942790985107
('helped', 'japan', 'dodge', 'gfc', 'deputy', 'pm')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (117, 'pm') has the max prob in order to be the next word. The prob is: 0.005237584002315998
952400 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0329])
The negative loglikelihood (-log p(pm)): 5.251894950866699 or 5.251894950866699 or 5.251894950866699
('killed', 'ahead', 'mass', 'rallies', 'aimed', 'forcing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2607, 'forcing') has the max prob in order to be the next word. The prob is: 7.873878348618746e-05
952500 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0276])
The negative loglikelihood (-log p(forcing)): 9.44937515258789 or 9.44937515258789 or 9.44937515258789
('investigation', 'allegedly', 'leaking', 'classified', 'information', 'covert')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4371, 'covert') has the max prob in order to be the next word. The prob is: 8.043225534493104e-05
952600 The nn predicts (tensor([764]), 'information') the next word with max prob: tensor([0.0409])
The negative loglikelihood (-log p(covert)): 9.428094863891602 or 9.428094863891602 or 9.428094863891602
('jerusalem', 'basketball', 'club', 'superstar', 'jewish', 'roots')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5703, 'roots') has the max prob in order to be the next word. The prob is: 3.7191417504800484e-05
952700 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.1446])
The negative loglikelihood (-log p(roots)): 10.199432373046875 or 10.199432373046875 or 10.199432373046875
('rage', 'catching', 'brazil', 'congress', 'excellent', 'background')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6019, 'background') has the max prob in order to be the next word. The prob is: 6.499775918200612e-05
952800 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0154])
The negative loglikelihood (-log p(background)): 9.641158103942871 or 9.641158103942871 or 9.641158103942871
('confederations', 'cup', 'stadium', 'around', 'protesters', 'clashes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (418, 'clashes') has the max prob in order to be the next word. The prob is: 0.000820944260340184
952900 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0280])
The negative loglikelihood (-log p(clashes)): 7.105055332183838 or 7.105055332183838 or 7.105055332183838
('us', 'senate', 'passes', 'landmark', 'immigration', 'bill')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (312, 'bill') has the max prob in order to be the next word. The prob is: 0.042020220309495926
953000 The nn predicts (tensor([312]), 'bill') the next word with max prob: tensor([0.0420])
The negative loglikelihood (-log p(bill)): 3.1696043014526367 or 3.1696043014526367 or 3.1696043014526367
('russia', 'says', 'embassy', 'naval', 'base', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.17085835337638855
953100 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1709])
The negative loglikelihood (-log p(syria)): 1.7669204473495483 or 1.7669204473495483 or 1.7669204473495483
('india', 'himalayan', 'region', 'killing', 'hundreds', 'leaving')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1065, 'leaving') has the max prob in order to be the next word. The prob is: 0.0003795241063926369
953200 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0904])
The negative loglikelihood (-log p(leaving)): 7.876592636108398 or 7.876592636108398 or 7.876592636108398
('kim', 'jong', 'un', 'initiates', 'push', 'build')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (636, 'build') has the max prob in order to be the next word. The prob is: 0.001242956263013184
953300 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.0159])
The negative loglikelihood (-log p(build)): 6.690262794494629 or 6.690262794494629 or 6.690262794494629
('campaign', 'us', 'president', 'barack', 'obama', 'visit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (303, 'visit') has the max prob in order to be the next word. The prob is: 0.035254813730716705
953400 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0564])
The negative loglikelihood (-log p(visit)): 3.345153331756592 or 3.3451530933380127 or 3.3451530933380127
('bakili', 'muluzi', 'expressed', 'interest', 'contest', 'member')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (700, 'member') has the max prob in order to be the next word. The prob is: 0.0005528107285499573
953500 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0174])
The negative loglikelihood (-log p(member)): 7.500494956970215 or 7.500494956970215 or 7.500494956970215
('contagious', 'asian', 'cousin', 'sars', 'killed', 'people')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14, 'people') has the max prob in order to be the next word. The prob is: 0.04255584254860878
953600 The nn predicts (tensor([84]), 'least') the next word with max prob: tensor([0.0659])
The negative loglikelihood (-log p(people)): 3.156938076019287 or 3.156938076019287 or 3.156938076019287
('large', 'number', 'pressure', 'industrial', 'poaching', 'wildlife')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2054, 'wildlife') has the max prob in order to be the next word. The prob is: 0.00045130852959118783
953700 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0139])
The negative loglikelihood (-log p(wildlife)): 7.703359603881836 or 7.703359127044678 or 7.703359127044678
('critical', 'zuma', 'says', 'hopes', 'leave', 'hospital')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (415, 'hospital') has the max prob in order to be the next word. The prob is: 0.001609546598047018
953800 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0525])
The negative loglikelihood (-log p(hospital)): 6.431802749633789 or 6.431802749633789 or 6.431802749633789
('brazil', 'president', 'dilma', 'rousseff', 'meets', 'youth')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1329, 'youth') has the max prob in order to be the next word. The prob is: 0.00017042120452970266
953900 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.1345])
The negative loglikelihood (-log p(youth)): 8.677237510681152 or 8.677237510681152 or 8.677237510681152
('british', 'teen', 'holiday', 'turkey', 'stripped', 'naked')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2548, 'naked') has the max prob in order to be the next word. The prob is: 0.0006556303706020117
954000 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0115])
The negative loglikelihood (-log p(naked)): 7.32991361618042 or 7.32991361618042 or 7.32991361618042
('spying', 'included', 'eu', 'offices', 'computers', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.0010122720850631595
954100 The nn predicts (tensor([316]), 'data') the next word with max prob: tensor([0.0154])
The negative loglikelihood (-log p(says)): 6.895557880401611 or 6.895557880401611 or 6.895557880401611
('environmental', 'crisis', 'well', 'underway', 'scary', 'reading')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3893, 'reading') has the max prob in order to be the next word. The prob is: 0.00016995145415421575
954200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0110])
The negative loglikelihood (-log p(reading)): 8.679997444152832 or 8.679997444152832 or 8.679997444152832
('leak', 'exposes', 'ecuador', 'surveillance', 'state', 'response')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (572, 'response') has the max prob in order to be the next word. The prob is: 0.0006991405971348286
954300 The nn predicts (tensor([132]), 'media') the next word with max prob: tensor([0.1351])
The negative loglikelihood (-log p(response)): 7.265658855438232 or 7.265658855438232 or 7.265658855438232
('ended', 'summit', 'nicaragua', 'agreements', 'promote', 'regional')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1246, 'regional') has the max prob in order to be the next word. The prob is: 0.0005927006714046001
954400 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0096])
The negative loglikelihood (-log p(regional)): 7.430820941925049 or 7.430820941925049 or 7.430820941925049
('early', 'presidential', 'election', 'mark', 'president', 'mohammed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1990, 'mohammed') has the max prob in order to be the next word. The prob is: 0.0010253210784867406
954500 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.0578])
The negative loglikelihood (-log p(mohammed)): 6.882749557495117 or 6.882749557495117 or 6.882749557495117
('magdalene', 'laundries', 'compensated', 'irish', 'govt', 'pay')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (294, 'pay') has the max prob in order to be the next word. The prob is: 0.003239372046664357
954600 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0235])
The negative loglikelihood (-log p(pay)): 5.732375621795654 or 5.732375621795654 or 5.732375621795654
('punish', 'offensive', 'facebook', 'twitter', 'comments', 'three')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (97, 'three') has the max prob in order to be the next word. The prob is: 0.00025503875804133713
954700 The nn predicts (tensor([338]), 'online') the next word with max prob: tensor([0.0193])
The negative loglikelihood (-log p(three)): 8.274094581604004 or 8.274094581604004 or 8.274094581604004
('reports', 'troops', 'kill', 'torture', 'rape', 'civilians')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (329, 'civilians') has the max prob in order to be the next word. The prob is: 0.0031643754336982965
954800 The nn predicts (tensor([141]), 'violence') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(civilians)): 5.755799770355225 or 5.755799770355225 or 5.755799770355225
('network', 'move', 'missiles', 'syria', 'ignoring', 'american')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (173, 'american') has the max prob in order to be the next word. The prob is: 0.0010202364064753056
954900 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0330])
The negative loglikelihood (-log p(american)): 6.887721061706543 or 6.887721061706543 or 6.887721061706543
('red', 'squirrels', 'seen', 'town', 'gardens', 'say')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (53, 'say') has the max prob in order to be the next word. The prob is: 0.00026563869323581457
955000 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0158])
The negative loglikelihood (-log p(say)): 8.233373641967773 or 8.233373641967773 or 8.233373641967773
('simple', 'bewitching', 'methods', 'narrating', 'universe', 'fight')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (152, 'fight') has the max prob in order to be the next word. The prob is: 6.781265983590856e-05
955100 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0073])
The negative loglikelihood (-log p(fight)): 9.598761558532715 or 9.598761558532715 or 9.598761558532715
('clashes', 'across', 'ethnically', 'divided', 'lukqun', 'township')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15272, 'township') has the max prob in order to be the next word. The prob is: 5.954319476586534e-06
955200 The nn predicts (tensor([357]), 'region') the next word with max prob: tensor([0.0380])
The negative loglikelihood (-log p(township)): 12.031394004821777 or 12.031394004821777 or 12.031394004821777
('part', 'effort', 'retake', 'city', 'activists', 'also')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (464, 'also') has the max prob in order to be the next word. The prob is: 0.0010064945090562105
955300 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.2809])
The negative loglikelihood (-log p(also)): 6.901281833648682 or 6.901281833648682 or 6.901281833648682
('france', 'brazil', 'russia', 'plunge', 'annual', 'profits')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3123, 'profits') has the max prob in order to be the next word. The prob is: 0.00028560071950778365
955400 The nn predicts (tensor([1098]), 'bailout') the next word with max prob: tensor([0.0150])
The negative loglikelihood (-log p(profits)): 8.16091537475586 or 8.16091537475586 or 8.16091537475586
('credit', 'unions', 'use', 'church', 'halls', 'effort')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1153, 'effort') has the max prob in order to be the next word. The prob is: 4.675958552979864e-05
955500 The nn predicts (tensor([201]), 'get') the next word with max prob: tensor([0.0097])
The negative loglikelihood (-log p(effort)): 9.970491409301758 or 9.970491409301758 or 9.970491409301758
('face', 'campaign', 'civil', 'disobedience', 'leave', 'power')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (106, 'power') has the max prob in order to be the next word. The prob is: 0.004034745041280985
955600 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0919])
The negative loglikelihood (-log p(power)): 5.51281213760376 or 5.51281213760376 or 5.51281213760376
('<s>', 'global', 'alcohol', 'consumption', 'drinking', 'habits')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8993, 'habits') has the max prob in order to be the next word. The prob is: 1.8474833268555813e-05
955700 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0214])
The negative loglikelihood (-log p(habits)): 10.899101257324219 or 10.899101257324219 or 10.899101257324219
('workers', 'free', 'boss', 'holding', 'captive', 'week')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (211, 'week') has the max prob in order to be the next word. The prob is: 0.0009683764073997736
955800 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0257])
The negative loglikelihood (-log p(week)): 6.939889907836914 or 6.939889907836914 or 6.939889907836914
('urged', 'european', 'union', 'arm', 'syrian', 'rebels')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (86, 'rebels') has the max prob in order to be the next word. The prob is: 0.30848047137260437
955900 The nn predicts (tensor([86]), 'rebels') the next word with max prob: tensor([0.3085])
The negative loglikelihood (-log p(rebels)): 1.176096796989441 or 1.176096796989441 or 1.176096796989441
('prohibits', 'family', 'members', 'overlooking', 'neglecting', 'elderly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2374, 'elderly') has the max prob in order to be the next word. The prob is: 0.0003063452895730734
956000 The nn predicts (tensor([256]), 'family') the next word with max prob: tensor([0.0121])
The negative loglikelihood (-log p(elderly)): 8.090797424316406 or 8.090797424316406 or 8.090797424316406
('funny', 'thing', 'happened', 'way', 'third', 'caliphate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4760, 'caliphate') has the max prob in order to be the next word. The prob is: 1.1480989314804901e-06
956100 The nn predicts (tensor([101]), 'time') the next word with max prob: tensor([0.0549])
The negative loglikelihood (-log p(caliphate)): 13.677403450012207 or 13.677403450012207 or 13.677403450012207
('protests', 'protesters', 'respond', 'green', 'laser', 'pointers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (30260, 'pointers') has the max prob in order to be the next word. The prob is: 1.6401714901803643e-06
956200 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0324])
The negative loglikelihood (-log p(pointers)): 13.320710182189941 or 13.320710182189941 or 13.320710182189941
('says', 'government', 'hours', 'meet', 'demands', 'people')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14, 'people') has the max prob in order to be the next word. The prob is: 0.009265923872590065
956300 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0229])
The negative loglikelihood (-log p(people)): 4.6814117431640625 or 4.6814117431640625 or 4.6814117431640625
('snowden', 'refuge', 'would', 'allow', 'merkel', 'demonstrate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5050, 'demonstrate') has the max prob in order to be the next word. The prob is: 2.9873106541344896e-05
956400 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0470])
The negative loglikelihood (-log p(demonstrate)): 10.41855239868164 or 10.41855239868164 or 10.41855239868164
('many', 'white', 'elephants', 'positive', 'sign', 'others')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (707, 'others') has the max prob in order to be the next word. The prob is: 0.0001533930335426703
956500 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0476])
The negative loglikelihood (-log p(others)): 8.782506942749023 or 8.782506942749023 or 8.782506942749023
('pray', 'illegal', 'migrants', 'lost', 'sea', 'trying')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (521, 'trying') has the max prob in order to be the next word. The prob is: 0.0001973428879864514
956600 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0102])
The negative loglikelihood (-log p(trying)): 8.53056812286377 or 8.53056812286377 or 8.53056812286377
('measles', 'widespread', 'mmr', 'rejection', 'caused', 'epidemic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2179, 'epidemic') has the max prob in order to be the next word. The prob is: 0.0005116869579069316
956700 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0168])
The negative loglikelihood (-log p(epidemic)): 7.5777974128723145 or 7.5777974128723145 or 7.5777974128723145
('<s>', '<s>', 'gang', 'rape', 'journalist', 'egypt')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (46, 'egypt') has the max prob in order to be the next word. The prob is: 0.01415311824530363
956800 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0222])
The negative loglikelihood (-log p(egypt)): 4.257820129394531 or 4.257820129394531 or 4.257820129394531
('president', 'retains', 'us', 'backing', 'refuses', 'bow')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8352, 'bow') has the max prob in order to be the next word. The prob is: 1.4271096006268635e-05
956900 The nn predicts (tensor([492]), 'asylum') the next word with max prob: tensor([0.0785])
The negative loglikelihood (-log p(bow)): 11.15727424621582 or 11.15727424621582 or 11.15727424621582
('say', 'rescued', 'women', 'human', 'trafficking', 'bust')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3887, 'bust') has the max prob in order to be the next word. The prob is: 2.172381937270984e-05
957000 The nn predicts (tensor([90]), 'human') the next word with max prob: tensor([0.0174])
The negative loglikelihood (-log p(bust)): 10.737101554870605 or 10.737101554870605 or 10.737101554870605
('consumer', 'goods', 'visit', 'cinema', 'go', 'holiday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2704, 'holiday') has the max prob in order to be the next word. The prob is: 2.8592781745828688e-05
957100 The nn predicts (tensor([115]), 'back') the next word with max prob: tensor([0.0239])
The negative loglikelihood (-log p(holiday)): 10.462356567382812 or 10.462356567382812 or 10.462356567382812
('crashes', 'seconds', 'launch', 'toxic', 'fuel', 'alight')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8038, 'alight') has the max prob in order to be the next word. The prob is: 1.9760773284360766e-05
957200 The nn predicts (tensor([726]), 'fuel') the next word with max prob: tensor([0.0185])
The negative loglikelihood (-log p(alight)): 10.831811904907227 or 10.831811904907227 or 10.831811904907227
('toward', 'egypt', 'troubles', 'help', 'iran', 'offered')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1809, 'offered') has the max prob in order to be the next word. The prob is: 0.00018685159739106894
957300 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0316])
The negative loglikelihood (-log p(offered)): 8.585195541381836 or 8.585195541381836 or 8.585195541381836
('chinese', 'grandmother', 'ordered', 'visit', 'least', 'every')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (652, 'every') has the max prob in order to be the next word. The prob is: 0.0004987504798918962
957400 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0826])
The negative loglikelihood (-log p(every)): 7.603404521942139 or 7.603404521942139 or 7.603404521942139
('<s>', 'egypt', 'protesters', 'mass', 'military', 'deadline')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1921, 'deadline') has the max prob in order to be the next word. The prob is: 0.00015744015399832278
957500 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0316])
The negative loglikelihood (-log p(deadline)): 8.756464958190918 or 8.756464958190918 or 8.756464958190918
('snowden', 'get', 'safe', 'europe', 'german', 'green')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1049, 'green') has the max prob in order to be the next word. The prob is: 0.00012966916256118566
957600 The nn predicts (tensor([343]), 'intelligence') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(green)): 8.95052433013916 or 8.95052433013916 or 8.95052433013916
('israeli', 'sesame', 'street', 'actors', 'protest', 'israel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10, 'israel') has the max prob in order to be the next word. The prob is: 0.0010415889555588365
957700 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0742])
The negative loglikelihood (-log p(israel)): 6.867007732391357 or 6.867007732391357 or 6.867007732391357
('growing', 'number', 'children', 'sexually', 'abused', 'via')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1716, 'via') has the max prob in order to be the next word. The prob is: 6.937584839761257e-05
957800 The nn predicts (tensor([75]), 'women') the next word with max prob: tensor([0.1150])
The negative loglikelihood (-log p(via)): 9.575971603393555 or 9.575971603393555 or 9.575971603393555
('<s>', '<s>', '<s>', 'celebrating', 'disaster', 'egypt')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (46, 'egypt') has the max prob in order to be the next word. The prob is: 0.04874439910054207
957900 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0487])
The negative loglikelihood (-log p(egypt)): 3.021164894104004 or 3.021164894104004 or 3.021164894104004
('ireland', 'moves', 'step', 'close', 'legalizing', 'abortion')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1415, 'abortion') has the max prob in order to be the next word. The prob is: 0.0005953345680609345
958000 The nn predicts (tensor([492]), 'asylum') the next word with max prob: tensor([0.0193])
The negative loglikelihood (-log p(abortion)): 7.426386833190918 or 7.426386833190918 or 7.426386833190918
('rebels', 'still', 'hold', 'positions', 'old', 'city')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (71, 'city') has the max prob in order to be the next word. The prob is: 0.001974781509488821
958100 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.0768])
The negative loglikelihood (-log p(city)): 6.227297306060791 or 6.227297306060791 or 6.227297306060791
('failure', 'spurs', 'concern', 'russia', 'space', 'program')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (596, 'program') has the max prob in order to be the next word. The prob is: 0.05095699802041054
958200 The nn predicts (tensor([596]), 'program') the next word with max prob: tensor([0.0510])
The negative loglikelihood (-log p(program)): 2.976773262023926 or 2.976773262023926 or 2.976773262023926
('loses', 'parliamentary', 'immunity', 'may', 'face', 'charges')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (296, 'charges') has the max prob in order to be the next word. The prob is: 0.01670096628367901
958300 The nn predicts (tensor([238]), 'trial') the next word with max prob: tensor([0.0813])
The negative loglikelihood (-log p(charges)): 4.092288494110107 or 4.092288494110107 or 4.092288494110107
('passos', 'coelho', 'said', 'stand', 'despite', 'resignation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2037, 'resignation') has the max prob in order to be the next word. The prob is: 0.0029520459938794374
958400 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0616])
The negative loglikelihood (-log p(resignation)): 5.825256824493408 or 5.825256824493408 or 5.825256824493408
('suspicion', 'snowden', 'board', 'plane', 'lands', 'austria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1760, 'austria') has the max prob in order to be the next word. The prob is: 0.00018171004194300622
958500 The nn predicts (tensor([112]), 'near') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(austria)): 8.61309814453125 or 8.61309814453125 or 8.61309814453125
('live', 'feed', 'public', 'hearings', 'gcsb', 'bill')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (312, 'bill') has the max prob in order to be the next word. The prob is: 0.0010380991734564304
958600 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0350])
The negative loglikelihood (-log p(bill)): 6.870364189147949 or 6.870364189147949 or 6.870364189147949
('popular', 'tourist', 'spot', 'fraser', 'island', 'part')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (262, 'part') has the max prob in order to be the next word. The prob is: 0.0006671835435554385
958700 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0148])
The negative loglikelihood (-log p(part)): 7.312445640563965 or 7.312445163726807 or 7.312445163726807
('wednesday', 'day', 'campaigning', 'starts', 'upper', 'house')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (299, 'house') has the max prob in order to be the next word. The prob is: 0.0025675820652395487
958800 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0143])
The negative loglikelihood (-log p(house)): 5.9647908210754395 or 5.9647908210754395 or 5.9647908210754395
('drone', 'attack', 'kills', 'pakistan', 'waziristan', 'region')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (357, 'region') has the max prob in order to be the next word. The prob is: 0.013659883290529251
958900 The nn predicts (tensor([45]), 'pakistan') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(region)): 4.293292045593262 or 4.293292045593262 or 4.293292045593262
('government', 'moro', 'islamic', 'liberation', 'front', 'hold')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (479, 'hold') has the max prob in order to be the next word. The prob is: 0.0003338080132380128
959000 The nn predicts (tensor([1909]), 'homs') the next word with max prob: tensor([0.0388])
The negative loglikelihood (-log p(hold)): 8.004944801330566 or 8.004944801330566 or 8.004944801330566
('coverage', 'crisis', 'egypt', 'latest', 'muslim', 'brotherhood')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1069, 'brotherhood') has the max prob in order to be the next word. The prob is: 0.3887241780757904
959100 The nn predicts (tensor([1069]), 'brotherhood') the next word with max prob: tensor([0.3887])
The negative loglikelihood (-log p(brotherhood)): 0.94488525390625 or 0.94488525390625 or 0.94488525390625
('danish', 'court', 'bans', 'kurdish', 'broadcasts', 'inciting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4190, 'inciting') has the max prob in order to be the next word. The prob is: 5.123746450408362e-05
959200 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0143])
The negative loglikelihood (-log p(inciting)): 9.879039764404297 or 9.879039764404297 or 9.879039764404297
('decade', 'century', 'shows', 'accelerated', 'warming', 'climate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (147, 'climate') has the max prob in order to be the next word. The prob is: 0.005065291654318571
959300 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0247])
The negative loglikelihood (-log p(climate)): 5.285343647003174 or 5.285343647003174 or 5.285343647003174
('spokesman', 'says', 'full', 'military', 'coup', 'way')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (302, 'way') has the max prob in order to be the next word. The prob is: 6.872825179016218e-05
959400 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0837])
The negative loglikelihood (-log p(way)): 9.585350036621094 or 9.585350036621094 or 9.585350036621094
('mursi', 'aide', 'says', 'coup', 'way', 'egypt')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (46, 'egypt') has the max prob in order to be the next word. The prob is: 0.009082590229809284
959500 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0259])
The negative loglikelihood (-log p(egypt)): 4.7013959884643555 or 4.7013959884643555 or 4.7013959884643555
('remove', 'president', 'mohamed', 'mursi', 'drew', 'millions')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (598, 'millions') has the max prob in order to be the next word. The prob is: 0.00025733665097504854
959600 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0249])
The negative loglikelihood (-log p(millions)): 8.265125274658203 or 8.265125274658203 or 8.265125274658203
('mastercard', 'international', 'reversed', 'decision', 'process', 'payments')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2674, 'payments') has the max prob in order to be the next word. The prob is: 0.0001458247861592099
959700 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0128])
The negative loglikelihood (-log p(payments)): 8.833105087280273 or 8.833105087280273 or 8.833105087280273
('constitution', 'ousted', 'morsi', 'called', 'early', 'elections')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (366, 'elections') has the max prob in order to be the next word. The prob is: 0.12963548302650452
959800 The nn predicts (tensor([366]), 'elections') the next word with max prob: tensor([0.1296])
The negative loglikelihood (-log p(elections)): 2.0430288314819336 or 2.0430288314819336 or 2.0430288314819336
('drones', 'still', 'flying', 'area', 'said', 'local')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (466, 'local') has the max prob in order to be the next word. The prob is: 0.0023853860329836607
959900 The nn predicts (tensor([362]), 'wednesday') the next word with max prob: tensor([0.0277])
The negative loglikelihood (-log p(local)): 6.038394451141357 or 6.038394451141357 or 6.038394451141357
('president', 'efforts', 'aid', 'bolivia', 'morales', 'uruguay')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2651, 'uruguay') has the max prob in order to be the next word. The prob is: 1.5215262465062551e-05
960000 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0521])
The negative loglikelihood (-log p(uruguay)): 11.09321117401123 or 11.09321117401123 or 11.09321117401123
('authority', 'back', 'democratically', 'elected', 'civilian', 'government')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11, 'government') has the max prob in order to be the next word. The prob is: 0.03519103303551674
960100 The nn predicts (tensor([18]), 'military') the next word with max prob: tensor([0.0463])
The negative loglikelihood (-log p(government)): 3.346963882446289 or 3.346963882446289 or 3.346963882446289
('korea', 'bizarre', 'social', 'media', 'strike', 'force')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (204, 'force') has the max prob in order to be the next word. The prob is: 9.893725655274466e-05
960200 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0297])
The negative loglikelihood (-log p(force)): 9.221024513244629 or 9.221024513244629 or 9.221024513244629
('horrifying', 'vigilante', 'justice', 'cameroon', 'man', 'lynched')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9620, 'lynched') has the max prob in order to be the next word. The prob is: 8.411452654399909e-06
960300 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0426])
The negative loglikelihood (-log p(lynched)): 11.68591594696045 or 11.68591594696045 or 11.68591594696045
('hardline', 'islamists', 'calling', 'holy', 'war', 'neighbour')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6835, 'neighbour') has the max prob in order to be the next word. The prob is: 3.690142330015078e-05
960400 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.1362])
The negative loglikelihood (-log p(neighbour)): 10.207260131835938 or 10.207260131835938 or 10.207260131835938
('say', 'clubbed', 'fat', 'fur', 'seal', 'pups')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (23221, 'pups') has the max prob in order to be the next word. The prob is: 3.6445890145841986e-06
960500 The nn predicts (tensor([124]), 'canada') the next word with max prob: tensor([0.0071])
The negative loglikelihood (-log p(pups)): 12.52226734161377 or 12.52226734161377 or 12.52226734161377
('islamic', 'women', 'work', 'customers', 'sellers', 'females')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9347, 'females') has the max prob in order to be the next word. The prob is: 1.812160735426005e-05
960600 The nn predicts (tensor([174]), 'use') the next word with max prob: tensor([0.0092])
The negative loglikelihood (-log p(females)): 10.918405532836914 or 10.918405532836914 or 10.918405532836914
('portugal', 'prime', 'minister', 'faces', 'day', 'talks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (83, 'talks') has the max prob in order to be the next word. The prob is: 0.002304027322679758
960700 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0486])
The negative loglikelihood (-log p(talks)): 6.073096752166748 or 6.073096752166748 or 6.073096752166748
('opponents', 'failed', 'oust', 'middle', 'east', 'claims')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (125, 'claims') has the max prob in order to be the next word. The prob is: 0.001035621389746666
960800 The nn predicts (tensor([885]), 'jazeera') the next word with max prob: tensor([0.0295])
The negative loglikelihood (-log p(claims)): 6.872753620147705 or 6.872753620147705 or 6.872753620147705
('texas', 'housemate', 'spewed', 'racist', 'homophobic', 'comments')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1174, 'comments') has the max prob in order to be the next word. The prob is: 0.002180565847083926
960900 The nn predicts (tensor([791]), 'assault') the next word with max prob: tensor([0.0177])
The negative loglikelihood (-log p(comments)): 6.128170967102051 or 6.128170967102051 or 6.128170967102051
('japan', 'said', 'grown', 'human', 'liver', 'tissue')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10632, 'tissue') has the max prob in order to be the next word. The prob is: 2.1693664166377857e-05
961000 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0219])
The negative loglikelihood (-log p(tissue)): 10.738490104675293 or 10.738490104675293 or 10.738490104675293
('microphone', 'found', 'embassy', 'julian', 'assange', 'lives')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (847, 'lives') has the max prob in order to be the next word. The prob is: 1.3011364899284672e-05
961100 The nn predicts (tensor([766]), 'assange') the next word with max prob: tensor([0.0480])
The negative loglikelihood (-log p(lives)): 11.249687194824219 or 11.249687194824219 or 11.249687194824219
('soaring', 'industry', 'look', 'like', 'behind', 'scenes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4320, 'scenes') has the max prob in order to be the next word. The prob is: 8.660797902848572e-05
961200 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0111])
The negative loglikelihood (-log p(scenes)): 9.354118347167969 or 9.354118347167969 or 9.354118347167969
('discretion', 'national', 'authorities', 'snowden', 'may', 'living')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (807, 'living') has the max prob in order to be the next word. The prob is: 0.0004014753212686628
961300 The nn predicts (tensor([187]), 'face') the next word with max prob: tensor([0.0381])
The negative loglikelihood (-log p(living)): 7.820364475250244 or 7.820364475250244 or 7.820364475250244
('overthrow', 'result', 'depending', 'usa', 'following', 'mubarak')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (975, 'mubarak') has the max prob in order to be the next word. The prob is: 4.805775824934244e-05
961400 The nn predicts (tensor([366]), 'elections') the next word with max prob: tensor([0.0136])
The negative loglikelihood (-log p(mubarak)): 9.943106651306152 or 9.943106651306152 or 9.943106651306152
('thai', 'navy', 'joined', 'search', 'crew', 'missing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (226, 'missing') has the max prob in order to be the next word. The prob is: 0.005388788878917694
961500 The nn predicts (tensor([205]), 'plane') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(missing)): 5.2234344482421875 or 5.2234344482421875 or 5.2234344482421875
('islamist', 'tv', 'stations', 'banned', 'muslim', 'brotherhood')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1069, 'brotherhood') has the max prob in order to be the next word. The prob is: 0.36258503794670105
961600 The nn predicts (tensor([1069]), 'brotherhood') the next word with max prob: tensor([0.3626])
The negative loglikelihood (-log p(brotherhood)): 1.0144963264465332 or 1.0144962072372437 or 1.0144962072372437
('country', 'president', 'evo', 'morales', 'saying', 'hand')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1165, 'hand') has the max prob in order to be the next word. The prob is: 0.00036062460276298225
961700 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0342])
The negative loglikelihood (-log p(hand)): 7.927672863006592 or 7.927672863006592 or 7.927672863006592
('state', 'tv', 'wednesday', 'attacked', 'critics', 'broadcasting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4004, 'broadcasting') has the max prob in order to be the next word. The prob is: 4.0545131923863664e-05
961800 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.1810])
The negative loglikelihood (-log p(broadcasting)): 10.1130952835083 or 10.1130952835083 or 10.1130952835083
('security', 'council', 'demand', 'syria', 'allow', 'immediate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2769, 'immediate') has the max prob in order to be the next word. The prob is: 0.0004033458244521171
961900 The nn predicts (tensor([75]), 'women') the next word with max prob: tensor([0.0553])
The negative loglikelihood (-log p(immediate)): 7.81571626663208 or 7.81571626663208 or 7.81571626663208
('two', 'years', 'old', 'nothing', 'celebrate', 'leadership')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1302, 'leadership') has the max prob in order to be the next word. The prob is: 7.149267185013741e-05
962000 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0170])
The negative loglikelihood (-log p(leadership)): 9.545915603637695 or 9.545915603637695 or 9.545915603637695
('diplomatic', 'row', 'morales', 'warned', 'would', 'close')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (428, 'close') has the max prob in order to be the next word. The prob is: 0.0007536090561188757
962100 The nn predicts (tensor([151]), 'help') the next word with max prob: tensor([0.0344])
The negative loglikelihood (-log p(close)): 7.19063663482666 or 7.19063663482666 or 7.19063663482666
('rejects', 'snowden', 'application', 'citizenship', 'snowden', 'enters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2825, 'enters') has the max prob in order to be the next word. The prob is: 4.644321234081872e-05
962200 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0991])
The negative loglikelihood (-log p(enters)): 9.977280616760254 or 9.977280616760254 or 9.977280616760254
('tech', 'firms', 'including', 'microsoft', 'said', 'voluntarily')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10037, 'voluntarily') has the max prob in order to be the next word. The prob is: 5.231702743913047e-06
962300 The nn predicts (tensor([362]), 'wednesday') the next word with max prob: tensor([0.0387])
The negative loglikelihood (-log p(voluntarily)): 12.160774230957031 or 12.160774230957031 or 12.160774230957031
('egypt', 'army', 'opens', 'fire', 'protesters', 'middle')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (344, 'middle') has the max prob in order to be the next word. The prob is: 0.0004075591859873384
962400 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0853])
The negative loglikelihood (-log p(middle)): 7.805324554443359 or 7.805324554443359 or 7.805324554443359
('billion', 'help', 'feed', 'million', 'people', 'risk')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (456, 'risk') has the max prob in order to be the next word. The prob is: 0.0028936448507010937
962500 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0155])
The negative loglikelihood (-log p(risk)): 5.845238208770752 or 5.845238208770752 or 5.845238208770752
('taking', 'food', 'woman', 'said', 'imprisoned', 'court')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (39, 'court') has the max prob in order to be the next word. The prob is: 0.0005351050058379769
962600 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0380])
The negative loglikelihood (-log p(court)): 7.533047676086426 or 7.533047676086426 or 7.533047676086426
('journalist', 'gives', 'take', 'happening', 'cairo', 'ground')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (688, 'ground') has the max prob in order to be the next word. The prob is: 0.00027685952954925597
962700 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0559])
The negative loglikelihood (-log p(ground)): 8.192000389099121 or 8.192000389099121 or 8.192000389099121
('zone', 'heavy', 'weapons', 'worse', 'seen', 'nothing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1525, 'nothing') has the max prob in order to be the next word. The prob is: 0.00035369201214052737
962800 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0189])
The negative loglikelihood (-log p(nothing)): 7.947083950042725 or 7.947083950042725 or 7.947083950042725
('brotherhood', 'claims', 'interim', 'egyptian', 'president', 'jewish')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (458, 'jewish') has the max prob in order to be the next word. The prob is: 0.00010516057955101132
962900 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.1091])
The negative loglikelihood (-log p(jewish)): 9.160021781921387 or 9.160021781921387 or 9.160021781921387
('francis', 'clears', 'john', 'paul', 'ii', 'sainthood')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15041, 'sainthood') has the max prob in order to be the next word. The prob is: 9.797385246201884e-06
963000 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0365])
The negative loglikelihood (-log p(sainthood)): 11.533394813537598 or 11.533394813537598 or 11.533394813537598
('bloom', 'engulfs', 'china', 'coast', 'like', 'bright')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7169, 'bright') has the max prob in order to be the next word. The prob is: 1.5926818377920426e-05
963100 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0292])
The negative loglikelihood (-log p(bright)): 11.047506332397461 or 11.047506332397461 or 11.047506332397461
('threatens', 'pull', 'zimbabwe', 'southern', 'african', 'development')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1103, 'development') has the max prob in order to be the next word. The prob is: 9.551027324050665e-05
963200 The nn predicts (tensor([525]), 'republic') the next word with max prob: tensor([0.1495])
The negative loglikelihood (-log p(development)): 9.256277084350586 or 9.256277084350586 or 9.256277084350586
('could', 'offer', 'political', 'asylum', 'us', 'fugitive')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3101, 'fugitive') has the max prob in order to be the next word. The prob is: 0.0001736655831336975
963300 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0553])
The negative loglikelihood (-log p(fugitive)): 8.658378601074219 or 8.658378601074219 or 8.658378601074219
('faces', 'booze', 'sale', 'ban', 'fpi', 'court')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (39, 'court') has the max prob in order to be the next word. The prob is: 0.03620119392871857
963400 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0362])
The negative loglikelihood (-log p(court)): 3.3186631202697754 or 3.3186631202697754 or 3.3186631202697754
('wet', 'nurses', 'adults', 'xinxinyu', 'domestic', 'staff')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (793, 'staff') has the max prob in order to be the next word. The prob is: 0.00011514588550198823
963500 The nn predicts (tensor([222]), 'food') the next word with max prob: tensor([0.0182])
The negative loglikelihood (-log p(staff)): 9.069311141967773 or 9.069310188293457 or 9.069310188293457
('west', 'bank', 'muslim', 'family', 'within', 'israel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10, 'israel') has the max prob in order to be the next word. The prob is: 0.0009143758798018098
963600 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0545])
The negative loglikelihood (-log p(israel)): 6.9972686767578125 or 6.9972686767578125 or 6.9972686767578125
('names', 'carmen', 'melendez', 'woman', 'defense', 'minister')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (25, 'minister') has the max prob in order to be the next word. The prob is: 0.0739358514547348
963700 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0739])
The negative loglikelihood (-log p(minister)): 2.604557514190674 or 2.604557514190674 or 2.604557514190674
('thrown', 'building', 'alexandria', 'islamist', 'supporters', 'ousted')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2437, 'ousted') has the max prob in order to be the next word. The prob is: 0.0009933997644111514
963800 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.0454])
The negative loglikelihood (-log p(ousted)): 6.914377212524414 or 6.914377212524414 or 6.914377212524414
('station', 'near', 'sydney', 'bioreactors', 'grow', 'algae')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10585, 'algae') has the max prob in order to be the next word. The prob is: 1.07357036540634e-05
963900 The nn predicts (tensor([111]), 'fire') the next word with max prob: tensor([0.0164])
The negative loglikelihood (-log p(algae)): 11.441935539245605 or 11.441935539245605 or 11.441935539245605
('egypt', 'violence', 'army', 'struggling', 'maintain', 'order')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (639, 'order') has the max prob in order to be the next word. The prob is: 0.00017545190348755568
964000 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(order)): 8.64814567565918 or 8.64814567565918 or 8.64814567565918
('bill', 'allows', 'one', 'parent', 'give', 'consent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4634, 'consent') has the max prob in order to be the next word. The prob is: 0.0003496737335808575
964100 The nn predicts (tensor([492]), 'asylum') the next word with max prob: tensor([0.0152])
The negative loglikelihood (-log p(consent)): 7.958509922027588 or 7.958509922027588 or 7.958509922027588
('qatada', 'deported', 'uk', 'stand', 'trial', 'jordan')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (832, 'jordan') has the max prob in order to be the next word. The prob is: 0.0003111939877271652
964200 The nn predicts (tensor([238]), 'trial') the next word with max prob: tensor([0.0420])
The negative loglikelihood (-log p(jordan)): 8.075094223022461 or 8.075094223022461 or 8.075094223022461
('venezuela', 'best', 'solution', 'snowden', 'new', 'york')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1053, 'york') has the max prob in order to be the next word. The prob is: 0.011115435510873795
964300 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0565])
The negative loglikelihood (-log p(york)): 4.499420642852783 or 4.499420642852783 or 4.499420642852783
('john', 'paul', 'sainthood', 'timing', 'connected', 'recent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (594, 'recent') has the max prob in order to be the next word. The prob is: 0.0002529590274207294
964400 The nn predicts (tensor([173]), 'american') the next word with max prob: tensor([0.0189])
The negative loglikelihood (-log p(recent)): 8.282282829284668 or 8.282282829284668 or 8.282282829284668
('wiped', 'amid', 'budget', 'crisis', 'latest', 'spain')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (277, 'spain') has the max prob in order to be the next word. The prob is: 0.00018873598310165107
964500 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.0295])
The negative loglikelihood (-log p(spain)): 8.57516098022461 or 8.57516098022461 or 8.57516098022461
('says', 'west', 'must', 'step', 'arms', 'rebels')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (86, 'rebels') has the max prob in order to be the next word. The prob is: 0.007430498022586107
964600 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0228])
The negative loglikelihood (-log p(rebels)): 4.902162551879883 or 4.902162551879883 or 4.902162551879883
('gargantuan', 'algae', 'bloom', 'fouls', 'beaches', 'chinese')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (41, 'chinese') has the max prob in order to be the next word. The prob is: 0.0021158901508897543
964700 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0140])
The negative loglikelihood (-log p(chinese)): 6.158279895782471 or 6.158279895782471 or 6.158279895782471
('preacher', 'hate', 'incited', 'murder', 'live', 'air')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (80, 'air') has the max prob in order to be the next word. The prob is: 0.00018195938901044428
964800 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0216])
The negative loglikelihood (-log p(air)): 8.611726760864258 or 8.611726760864258 or 8.611726760864258
('blasts', 'indian', 'buddhist', 'complex', 'muslims', 'suspected')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (241, 'suspected') has the max prob in order to be the next word. The prob is: 0.0007149599841795862
964900 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0471])
The negative loglikelihood (-log p(suspected)): 7.243284225463867 or 7.243283748626709 or 7.243283748626709
('military', 'dictatorship', 'military', 'domination', 'politics', 'civil')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (452, 'civil') has the max prob in order to be the next word. The prob is: 0.001441697240807116
965000 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0076])
The negative loglikelihood (-log p(civil)): 6.541934013366699 or 6.541934013366699 or 6.541934013366699
('former', 'president', 'mohammed', 'morsi', 'marking', 'possible')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (453, 'possible') has the max prob in order to be the next word. The prob is: 0.0003991163393948227
965100 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.0474])
The negative loglikelihood (-log p(possible)): 7.826257705688477 or 7.826257705688477 or 7.826257705688477
('releases', 'men', 'machetes', 'attacked', 'gezi', 'protesters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (119, 'protesters') has the max prob in order to be the next word. The prob is: 0.010944297537207603
965200 The nn predicts (tensor([867]), 'park') the next word with max prob: tensor([0.0363])
The negative loglikelihood (-log p(protesters)): 4.514936923980713 or 4.514936923980713 or 4.514936923980713
('canadian', 'train', 'explosion', 'least', 'five', 'died')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (425, 'died') has the max prob in order to be the next word. The prob is: 0.0006289302255026996
965300 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.1450])
The negative loglikelihood (-log p(died)): 7.371490478515625 or 7.371490001678467 or 7.371490001678467
('gradually', 'end', 'contentious', 'system', 'granted', 'automatic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6719, 'automatic') has the max prob in order to be the next word. The prob is: 6.741596280335216e-06
965400 The nn predicts (tensor([492]), 'asylum') the next word with max prob: tensor([0.0517])
The negative loglikelihood (-log p(automatic)): 11.907214164733887 or 11.907214164733887 or 11.907214164733887
('along', 'cows', 'stomachs', 'throats', 'animal', 'organs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4278, 'organs') has the max prob in order to be the next word. The prob is: 0.0003310106694698334
965500 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0202])
The negative loglikelihood (-log p(organs)): 8.013360023498535 or 8.013360023498535 or 8.013360023498535
('tahrir', 'square', 'women', 'fear', 'gang', 'rape')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (420, 'rape') has the max prob in order to be the next word. The prob is: 0.0151671776548028
965600 The nn predicts (tensor([141]), 'violence') the next word with max prob: tensor([0.1049])
The negative loglikelihood (-log p(rape)): 4.188621520996094 or 4.188621520996094 or 4.188621520996094
('swimming', 'pool', 'caught', 'refusing', 'entry', 'arab')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (271, 'arab') has the max prob in order to be the next word. The prob is: 9.077781578525901e-05
965700 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0159])
The negative loglikelihood (-log p(arab)): 9.307095527648926 or 9.307095527648926 or 9.307095527648926
('officer', 'says', 'gunmen', 'kill', 'morsi', 'supporters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (988, 'supporters') has the max prob in order to be the next word. The prob is: 0.01020137406885624
965800 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0214])
The negative loglikelihood (-log p(supporters)): 4.585232734680176 or 4.585232734680176 or 4.585232734680176
('kenya', 'teachers', 'strike', 'enters', 'third', 'week')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (211, 'week') has the max prob in order to be the next word. The prob is: 0.020259274169802666
965900 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.1584])
The negative loglikelihood (-log p(week)): 3.8991427421569824 or 3.8991425037384033 or 3.8991425037384033
('even', 'proper', 'way', 'use', 'data', 'must')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (254, 'must') has the max prob in order to be the next word. The prob is: 0.0005015954375267029
966000 The nn predicts (tensor([316]), 'data') the next word with max prob: tensor([0.0438])
The negative loglikelihood (-log p(must)): 7.597716808319092 or 7.597716808319092 or 7.597716808319092
('protected', 'scale', 'ban', 'fishing', 'sq', 'kms')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (23926, 'kms') has the max prob in order to be the next word. The prob is: 6.4512682911299635e-06
966100 The nn predicts (tensor([695]), 'industry') the next word with max prob: tensor([0.0061])
The negative loglikelihood (-log p(kms)): 11.951233863830566 or 11.951233863830566 or 11.951233863830566
('sending', 'sons', 'die', 'syria', 'asharq', 'alawsat')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44086, 'alawsat') has the max prob in order to be the next word. The prob is: 3.2268292216031114e-06
966200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0607])
The negative loglikelihood (-log p(alawsat)): 12.644010543823242 or 12.644010543823242 or 12.644010543823242
('heatwaves', 'five', 'times', 'likely', 'due', 'global')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (95, 'global') has the max prob in order to be the next word. The prob is: 0.004832425154745579
966300 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0122])
The negative loglikelihood (-log p(global)): 5.332406997680664 or 5.332406997680664 or 5.332406997680664
('new', 'command', 'amid', 'war', 'vp', 'among')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (414, 'among') has the max prob in order to be the next word. The prob is: 0.0004595540522132069
966400 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.0182])
The negative loglikelihood (-log p(among)): 7.685254096984863 or 7.685254096984863 or 7.685254096984863
('fire', 'department', 'said', 'one', 'may', 'run')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (587, 'run') has the max prob in order to be the next word. The prob is: 0.003942244686186314
966500 The nn predicts (tensor([187]), 'face') the next word with max prob: tensor([0.0261])
The negative loglikelihood (-log p(run)): 5.536005020141602 or 5.536005020141602 or 5.536005020141602
('suspending', 'participation', 'efforts', 'form', 'interim', 'government')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11, 'government') has the max prob in order to be the next word. The prob is: 0.2352503091096878
966600 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.2353])
The negative loglikelihood (-log p(government)): 1.4471051692962646 or 1.4471051692962646 or 1.4471051692962646
('run', 'one', 'chinese', 'students', 'boeing', 'wreck')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5103, 'wreck') has the max prob in order to be the next word. The prob is: 4.185699071967974e-05
966700 The nn predicts (tensor([363]), 'airport') the next word with max prob: tensor([0.0150])
The negative loglikelihood (-log p(wreck)): 10.081252098083496 or 10.081252098083496 or 10.081252098083496
('gezi', 'park', 'focus', 'last', 'month', 'protests')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (96, 'protests') has the max prob in order to be the next word. The prob is: 0.00500264298170805
966800 The nn predicts (tensor([18]), 'military') the next word with max prob: tensor([0.0280])
The negative loglikelihood (-log p(protests)): 5.297789096832275 or 5.297789096832275 or 5.297789096832275
('drop', 'life', 'span', 'northern', 'china', 'study')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (306, 'study') has the max prob in order to be the next word. The prob is: 0.0007294996757991612
966900 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0746])
The negative loglikelihood (-log p(study)): 7.223151683807373 or 7.223151683807373 or 7.223151683807373
('contested', 'race', 'governor', 'key', 'mexican', 'border')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (103, 'border') has the max prob in order to be the next word. The prob is: 0.002904376247897744
967000 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.1631])
The negative loglikelihood (-log p(border)): 5.841536521911621 or 5.841536521911621 or 5.841536521911621
('starting', 'talks', 'biggest', 'bilateral', 'trade', 'deal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (64, 'deal') has the max prob in order to be the next word. The prob is: 0.028622671961784363
967100 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0308])
The negative loglikelihood (-log p(deal)): 3.553556203842163 or 3.553556203842163 or 3.553556203842163
('femen', 'political', 'asylum', 'leader', 'inna', 'shevchenko')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (23686, 'shevchenko') has the max prob in order to be the next word. The prob is: 4.300492946640588e-06
967200 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.0426])
The negative loglikelihood (-log p(shevchenko)): 12.356781005859375 or 12.356781005859375 or 12.356781005859375
('train', 'ran', 'away', 'devastated', 'canadian', 'town')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (242, 'town') has the max prob in order to be the next word. The prob is: 0.03364512324333191
967300 The nn predicts (tensor([242]), 'town') the next word with max prob: tensor([0.0336])
The negative loglikelihood (-log p(town)): 3.3918871879577637 or 3.3918871879577637 or 3.3918871879577637
('became', 'brothers', 'crowded', 'tonight', 'missed', 'brotherhood')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1069, 'brotherhood') has the max prob in order to be the next word. The prob is: 2.0781290004379116e-06
967400 The nn predicts (tensor([37]), 'one') the next word with max prob: tensor([0.0089])
The negative loglikelihood (-log p(brotherhood)): 13.0840425491333 or 13.0840425491333 or 13.0840425491333
('<s>', '<s>', 'second', 'boeing', 'suffers', 'incident')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1414, 'incident') has the max prob in order to be the next word. The prob is: 0.001085096737369895
967500 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0338])
The negative loglikelihood (-log p(incident)): 6.826086044311523 or 6.826086044311523 or 6.826086044311523
('join', 'fight', 'save', 'gran', 'chaco', 'wilderness')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10611, 'wilderness') has the max prob in order to be the next word. The prob is: 5.251814855000703e-06
967600 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0095])
The negative loglikelihood (-log p(wilderness)): 12.156936645507812 or 12.156936645507812 or 12.156936645507812
('caucasus', 'prominent', 'journalist', 'killed', 'dagestan', 'russia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6, 'russia') has the max prob in order to be the next word. The prob is: 0.0004844200157094747
967700 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0164])
The negative loglikelihood (-log p(russia)): 7.632558345794678 or 7.632558345794678 or 7.632558345794678
('longer', 'continues', 'power', 'islamist', 'groups', 'gain')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2487, 'gain') has the max prob in order to be the next word. The prob is: 0.0002253136335639283
967800 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0487])
The negative loglikelihood (-log p(gain)): 8.398016929626465 or 8.398016929626465 or 8.398016929626465
('chairman', 'company', 'whose', 'train', 'barreled', 'small')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (958, 'small') has the max prob in order to be the next word. The prob is: 0.0011186731280758977
967900 The nn predicts (tensor([31]), 'two') the next word with max prob: tensor([0.0095])
The negative loglikelihood (-log p(small)): 6.79561185836792 or 6.79561185836792 or 6.79561185836792
('muslim', 'brotherhood', 'rejects', 'timetable', 'egypt', 'reform')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1068, 'reform') has the max prob in order to be the next word. The prob is: 0.00021527147328015417
968000 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0383])
The negative loglikelihood (-log p(reform)): 8.443611145019531 or 8.443610191345215 or 8.443610191345215
('quebec', 'oil', 'train', 'explosion', 'expected', 'rise')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (337, 'rise') has the max prob in order to be the next word. The prob is: 0.0013464391231536865
968100 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0125])
The negative loglikelihood (-log p(rise)): 6.610291957855225 or 6.610291957855225 or 6.610291957855225
('shooting', 'primary', 'school', 'jailed', 'two', 'years')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (33, 'years') has the max prob in order to be the next word. The prob is: 0.21751874685287476
968200 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.2175])
The negative loglikelihood (-log p(years)): 1.52547025680542 or 1.52547025680542 or 1.52547025680542
('base', 'blast', 'points', 'israeli', 'raid', 'foreign')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (88, 'foreign') has the max prob in order to be the next word. The prob is: 0.002653212519362569
968300 The nn predicts (tensor([435]), 'embassy') the next word with max prob: tensor([0.0357])
The negative loglikelihood (-log p(foreign)): 5.931983947753906 or 5.931983947753906 or 5.931983947753906
('report', 'scathing', 'holding', 'government', 'military', 'responsible')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1305, 'responsible') has the max prob in order to be the next word. The prob is: 0.00016597188368905336
968400 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.0311])
The negative loglikelihood (-log p(responsible)): 8.703692436218262 or 8.703692436218262 or 8.703692436218262
('contents', 'letters', 'prince', 'charles', 'wrote', 'government')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11, 'government') has the max prob in order to be the next word. The prob is: 0.001360663678497076
968500 The nn predicts (tensor([820]), 'bin') the next word with max prob: tensor([0.0167])
The negative loglikelihood (-log p(government)): 6.599782943725586 or 6.599782466888428 or 6.599782466888428
('<s>', 'timeline', 'egypt', 'since', 'morsy', 'ouster')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4665, 'ouster') has the max prob in order to be the next word. The prob is: 0.003717323997989297
968600 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.0361])
The negative loglikelihood (-log p(ouster)): 5.594751358032227 or 5.594751358032227 or 5.594751358032227
('risk', 'global', 'economy', 'imf', 'imf', 'trims')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19771, 'trims') has the max prob in order to be the next word. The prob is: 8.384416787521332e-07
968700 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0346])
The negative loglikelihood (-log p(trims)): 13.991721153259277 or 13.991721153259277 or 13.991721153259277
('man', 'made', 'threat', 'kill', 'us', 'schoolchildren')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5954, 'schoolchildren') has the max prob in order to be the next word. The prob is: 3.742260378203355e-05
968800 The nn predicts (tensor([435]), 'embassy') the next word with max prob: tensor([0.0483])
The negative loglikelihood (-log p(schoolchildren)): 10.193235397338867 or 10.193235397338867 or 10.193235397338867
('still', 'working', 'greece', 'despite', 'drastic', 'austerity')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1172, 'austerity') has the max prob in order to be the next word. The prob is: 0.0020079088862985373
968900 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.0980])
The negative loglikelihood (-log p(austerity)): 6.2106614112854 or 6.2106614112854 or 6.2106614112854
('detailed', 'briefing', 'saturday', 'crash', 'ntsb', 'chairwoman')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18743, 'chairwoman') has the max prob in order to be the next word. The prob is: 4.0955801523523405e-06
969000 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0457])
The negative loglikelihood (-log p(chairwoman)): 12.40560245513916 or 12.40560245513916 or 12.40560245513916
('communist', 'party', 'struggle', 'dalai', 'lama', 'country')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35, 'country') has the max prob in order to be the next word. The prob is: 0.0014310291735455394
969100 The nn predicts (tensor([1896]), 'lama') the next word with max prob: tensor([0.0360])
The negative loglikelihood (-log p(country)): 6.549361228942871 or 6.549361228942871 or 6.549361228942871
('rifles', 'security', 'post', 'manipur', 'state', 'along')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (999, 'along') has the max prob in order to be the next word. The prob is: 0.00024751643650233746
969200 The nn predicts (tensor([132]), 'media') the next word with max prob: tensor([0.0791])
The negative loglikelihood (-log p(along)): 8.304033279418945 or 8.304033279418945 or 8.304033279418945
('family', 'converted', 'islam', 'order', 'marry', 'christian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (694, 'christian') has the max prob in order to be the next word. The prob is: 0.000745953933801502
969300 The nn predicts (tensor([75]), 'women') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(christian)): 7.2008466720581055 or 7.2008466720581055 or 7.2008466720581055
('access', 'us', 'programme', 'uk', 'authorities', 'able')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1725, 'able') has the max prob in order to be the next word. The prob is: 0.00030567889916710556
969400 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0879])
The negative loglikelihood (-log p(able)): 8.092975616455078 or 8.092975616455078 or 8.092975616455078
('called', 'toppling', 'country', 'president', 'morsi', 'part')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (262, 'part') has the max prob in order to be the next word. The prob is: 0.00026861956575885415
969500 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.0775])
The negative loglikelihood (-log p(part)): 8.222214698791504 or 8.222214698791504 or 8.222214698791504
('lost', 'much', 'economic', 'vigor', 'beijing', 'seeks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (484, 'seeks') has the max prob in order to be the next word. The prob is: 0.00014760231715627015
969600 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0309])
The negative loglikelihood (-log p(seeks)): 8.820988655090332 or 8.820988655090332 or 8.820988655090332
('imminent', 'fall', 'neighborhood', 'live', 'thread', 'comments')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1174, 'comments') has the max prob in order to be the next word. The prob is: 0.001529371365904808
969700 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0382])
The negative loglikelihood (-log p(comments)): 6.482898712158203 or 6.482898712158203 or 6.482898712158203
('curbs', 'power', 'architect', 'group', 'involved', 'protests')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (96, 'protests') has the max prob in order to be the next word. The prob is: 0.0013282343279570341
969800 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0381])
The negative loglikelihood (-log p(protests)): 6.623904705047607 or 6.623904705047607 or 6.623904705047607
('america', 'plan', 'b', 'egypt', 'bring', 'back')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (115, 'back') has the max prob in order to be the next word. The prob is: 0.006368627306073904
969900 The nn predicts (tensor([127]), 'peace') the next word with max prob: tensor([0.0156])
The negative loglikelihood (-log p(back)): 5.056371212005615 or 5.056371212005615 or 5.056371212005615
('china', 'vows', 'step', 'fight', 'dalai', 'lama')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1896, 'lama') has the max prob in order to be the next word. The prob is: 0.017771193757653236
970000 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.0265])
The negative loglikelihood (-log p(lama)): 4.030176639556885 or 4.030176639556885 or 4.030176639556885
('committee', 'annual', 'report', 'said', 'spy', 'chiefs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2861, 'chiefs') has the max prob in order to be the next word. The prob is: 0.00030482339207082987
970100 The nn predicts (tensor([51]), 'report') the next word with max prob: tensor([0.0317])
The negative loglikelihood (-log p(chiefs)): 8.095778465270996 or 8.095778465270996 or 8.095778465270996
('page', 'post', 'congress', 'derails', 'obama', 'plans')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (167, 'plans') has the max prob in order to be the next word. The prob is: 0.0008010872406885028
970200 The nn predicts (tensor([303]), 'visit') the next word with max prob: tensor([0.0131])
The negative loglikelihood (-log p(plans)): 7.129540920257568 or 7.129540920257568 or 7.129540920257568
('staff', 'plant', 'operator', 'knew', 'exactly', 'leaks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1641, 'leaks') has the max prob in order to be the next word. The prob is: 0.0006327293231151998
970300 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0127])
The negative loglikelihood (-log p(leaks)): 7.3654680252075195 or 7.3654680252075195 or 7.3654680252075195
('plant', 'getting', 'closer', 'pacific', 'two', 'years')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (33, 'years') has the max prob in order to be the next word. The prob is: 0.14249324798583984
970400 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.1425])
The negative loglikelihood (-log p(years)): 1.9484606981277466 or 1.9484606981277466 or 1.9484606981277466
('missiles', 'poised', 'pointed', 'israel', 'iran', 'satellite')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (926, 'satellite') has the max prob in order to be the next word. The prob is: 0.00022821519814897329
970500 The nn predicts (tensor([32]), 'nuclear') the next word with max prob: tensor([0.0540])
The negative loglikelihood (-log p(satellite)): 8.385221481323242 or 8.385221481323242 or 8.385221481323242
('also', 'dogged', 'scientists', 'claims', 'construction', 'polluting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7207, 'polluting') has the max prob in order to be the next word. The prob is: 3.103853669017553e-05
970600 The nn predicts (tensor([347]), 'plant') the next word with max prob: tensor([0.1152])
The negative loglikelihood (-log p(polluting)): 10.380281448364258 or 10.380281448364258 or 10.380281448364258
('chinese', 'police', 'say', 'employees', 'drug', 'maker')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3313, 'maker') has the max prob in order to be the next word. The prob is: 0.00015603787323925644
970700 The nn predicts (tensor([1379]), 'trafficking') the next word with max prob: tensor([0.0512])
The negative loglikelihood (-log p(maker)): 8.765411376953125 or 8.765411376953125 or 8.765411376953125
('today', 'implicated', 'security', 'service', 'spying', 'probe')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (431, 'probe') has the max prob in order to be the next word. The prob is: 0.0012662197696045041
970800 The nn predicts (tensor([671]), 'spying') the next word with max prob: tensor([0.0439])
The negative loglikelihood (-log p(probe)): 6.671719551086426 or 6.671719551086426 or 6.671719551086426
('university', 'shows', 'water', 'times', 'radiation', 'safe')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1178, 'safe') has the max prob in order to be the next word. The prob is: 0.00028970694984309375
970900 The nn predicts (tensor([416]), 'fukushima') the next word with max prob: tensor([0.0223])
The negative loglikelihood (-log p(safe)): 8.14664077758789 or 8.14664077758789 or 8.14664077758789
('labor', 'organizer', 'nazma', 'akhter', 'argues', 'plants')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1501, 'plants') has the max prob in order to be the next word. The prob is: 2.4955828848760575e-05
971000 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0117])
The negative loglikelihood (-log p(plants)): 10.598402976989746 or 10.598402976989746 or 10.598402976989746
('zaelke', 'said', 'hard', 'work', 'technocrats', 'hard')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1365, 'hard') has the max prob in order to be the next word. The prob is: 0.0006040993030183017
971100 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0909])
The negative loglikelihood (-log p(hard)): 7.411771774291992 or 7.411771774291992 or 7.411771774291992
('lucrative', 'job', 'advising', 'transnational', 'corporations', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.001036715810187161
971200 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0140])
The negative loglikelihood (-log p(says)): 6.871697425842285 or 6.871697425842285 or 6.871697425842285
('design', 'vacuum', 'cleaner', 'detention', 'keep', 'sane')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11599, 'sane') has the max prob in order to be the next word. The prob is: 9.031020454131067e-06
971300 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.0240])
The negative loglikelihood (-log p(sane)): 11.614845275878906 or 11.614845275878906 or 11.614845275878906
('today', 'flight', 'taking', 'unusual', 'detour', 'avoids')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6850, 'avoids') has the max prob in order to be the next word. The prob is: 1.2329624951235019e-05
971400 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0107])
The negative loglikelihood (-log p(avoids)): 11.303505897521973 or 11.303505897521973 or 11.303505897521973
('europe', 'perfect', 'hunting', 'ground', 'wolves', 'moving')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1724, 'moving') has the max prob in order to be the next word. The prob is: 0.00024308444699272513
971500 The nn predicts (tensor([122]), 'afghanistan') the next word with max prob: tensor([0.0083])
The negative loglikelihood (-log p(moving)): 8.322101593017578 or 8.322101593017578 or 8.322101593017578
('described', 'western', 'nations', 'small', 'propaganda', 'storm')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (623, 'storm') has the max prob in order to be the next word. The prob is: 7.774361438350752e-05
971600 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0169])
The negative loglikelihood (-log p(storm)): 9.4620943069458 or 9.4620943069458 or 9.4620943069458
('thursday', 'effort', 'discourage', 'trying', 'return', 'mexican')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (439, 'mexican') has the max prob in order to be the next word. The prob is: 0.000299644423648715
971700 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0447])
The negative loglikelihood (-log p(mexican)): 8.112914085388184 or 8.112914085388184 or 8.112914085388184
('refrain', 'binds', 'protest', 'movements', 'brazil', 'turkey')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (49, 'turkey') has the max prob in order to be the next word. The prob is: 0.0004155481292400509
971800 The nn predicts (tensor([91]), 'protest') the next word with max prob: tensor([0.0399])
The negative loglikelihood (-log p(turkey)): 7.785912036895752 or 7.785912036895752 or 7.785912036895752
('perhaps', 'surprise', 'floating', 'around', 'isles', 'abdication')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14981, 'abdication') has the max prob in order to be the next word. The prob is: 2.093934881486348e-06
971900 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(abdication)): 13.076465606689453 or 13.076465606689453 or 13.076465606689453
('lawyer', 'india', 'rape', 'verdict', 'given', 'july')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1391, 'july') has the max prob in order to be the next word. The prob is: 0.0006327906739898026
972000 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0407])
The negative loglikelihood (-log p(july)): 7.365370750427246 or 7.365370750427246 or 7.365370750427246
('resident', 'shaker', 'aamer', 'claims', 'guards', 'slamming')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12742, 'slamming') has the max prob in order to be the next word. The prob is: 3.379169129402726e-06
972100 The nn predicts (tensor([194]), 'prison') the next word with max prob: tensor([0.0079])
The negative loglikelihood (-log p(slamming)): 12.597880363464355 or 12.597880363464355 or 12.597880363464355
('including', 'hamas', 'members', 'arrested', 'others', 'past')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (560, 'past') has the max prob in order to be the next word. The prob is: 0.0003450953809078783
972200 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0217])
The negative loglikelihood (-log p(past)): 7.971689701080322 or 7.971689701080322 or 7.971689701080322
('rights', 'watch', 'sheremetyevo', 'airport', 'us', 'officials')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (79, 'officials') has the max prob in order to be the next word. The prob is: 0.003647896461188793
972300 The nn predicts (tensor([435]), 'embassy') the next word with max prob: tensor([0.0853])
The negative loglikelihood (-log p(officials)): 5.613604545593262 or 5.613604545593262 or 5.613604545593262
('uranium', 'processing', 'plant', 'would', 'nation', 'largest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (276, 'largest') has the max prob in order to be the next word. The prob is: 0.006603536196053028
972400 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0200])
The negative loglikelihood (-log p(largest)): 5.020150184631348 or 5.020150184631348 or 5.020150184631348
('client', 'list', 'worsening', 'public', 'health', 'network')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (918, 'network') has the max prob in order to be the next word. The prob is: 0.0009997328743338585
972500 The nn predicts (tensor([333]), 'system') the next word with max prob: tensor([0.0638])
The negative loglikelihood (-log p(network)): 6.908022403717041 or 6.908022403717041 or 6.908022403717041
('son', 'boasted', 'law', 'accused', 'rape', 'plays')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3975, 'plays') has the max prob in order to be the next word. The prob is: 1.705963586573489e-05
972600 The nn predicts (tensor([296]), 'charges') the next word with max prob: tensor([0.0412])
The negative loglikelihood (-log p(plays)): 10.978795051574707 or 10.978795051574707 or 10.978795051574707
('time', 'digital', 'citizens', 'happiness', 'barometer', 'city')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (71, 'city') has the max prob in order to be the next word. The prob is: 0.0003438246203586459
972700 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0293])
The negative loglikelihood (-log p(city)): 7.97537899017334 or 7.97537899017334 or 7.97537899017334
('rebel', 'commander', 'free', 'syrian', 'army', 'shot')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (232, 'shot') has the max prob in order to be the next word. The prob is: 0.0025485383812338114
972800 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.0598])
The negative loglikelihood (-log p(shot)): 5.972235202789307 or 5.972235202789307 or 5.972235202789307
('worked', 'seven', 'years', 'pages', 'project', 'new')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4, 'new') has the max prob in order to be the next word. The prob is: 0.006522185634821653
972900 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0085])
The negative loglikelihood (-log p(new)): 5.032545566558838 or 5.032545566558838 or 5.032545566558838
('three', 'men', 'brutally', 'gored', 'bloodiest', 'day')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (121, 'day') has the max prob in order to be the next word. The prob is: 0.04876355826854706
973000 The nn predicts (tensor([375]), 'month') the next word with max prob: tensor([0.0942])
The negative loglikelihood (-log p(day)): 3.0207719802856445 or 3.0207719802856445 or 3.0207719802856445
('government', 'corruption', 'unprecedented', 'modern', 'times', 'even')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (334, 'even') has the max prob in order to be the next word. The prob is: 0.0019420821918174624
973100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0315])
The negative loglikelihood (-log p(even)): 6.24399471282959 or 6.24399471282959 or 6.24399471282959
('egypt', 'second', 'christian', 'killed', 'sinai', 'suspected')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (241, 'suspected') has the max prob in order to be the next word. The prob is: 0.0013951938599348068
973200 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0451])
The negative loglikelihood (-log p(suspected)): 6.574721813201904 or 6.574721813201904 or 6.574721813201904
('former', 'member', 'bank', 'england', 'financial', 'policy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (368, 'policy') has the max prob in order to be the next word. The prob is: 0.0016141365049406886
973300 The nn predicts (tensor([65]), 'crisis') the next word with max prob: tensor([0.0392])
The negative loglikelihood (-log p(policy)): 6.428955078125 or 6.428955078125 or 6.428955078125
('back', 'home', 'many', 'afghan', 'envoys', 'decline')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2172, 'decline') has the max prob in order to be the next word. The prob is: 4.487682963372208e-05
973400 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0261])
The negative loglikelihood (-log p(decline)): 10.011589050292969 or 10.011589050292969 or 10.011589050292969
('probe', 'morsi', 'jailbreak', 'egypt', 'new', 'rulers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4194, 'rulers') has the max prob in order to be the next word. The prob is: 7.303222082555294e-05
973500 The nn predicts (tensor([1392]), 'constitution') the next word with max prob: tensor([0.0366])
The negative loglikelihood (-log p(rulers)): 9.524609565734863 or 9.524609565734863 or 9.524609565734863
('set', 'fire', 'india', 'shockingly', 'police', 'refused')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1635, 'refused') has the max prob in order to be the next word. The prob is: 0.0006367124151438475
973600 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0357])
The negative loglikelihood (-log p(refused)): 7.359192371368408 or 7.359192371368408 or 7.359192371368408
('footage', 'shot', 'activists', 'showed', 'saturday', 'video')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (100, 'video') has the max prob in order to be the next word. The prob is: 0.0006581400521099567
973700 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0175])
The negative loglikelihood (-log p(video)): 7.326092720031738 or 7.326092720031738 or 7.326092720031738
('islamist', 'rallies', 'protest', 'military', 'coup', 'egypt')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (46, 'egypt') has the max prob in order to be the next word. The prob is: 0.00847723986953497
973800 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.1055])
The negative loglikelihood (-log p(egypt)): 4.7703704833984375 or 4.7703704833984375 or 4.7703704833984375
('kent', 'destroyed', 'quarry', 'site', 'hermitage', 'quarry')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13978, 'quarry') has the max prob in order to be the next word. The prob is: 7.2189554884971585e-06
973900 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0132])
The negative loglikelihood (-log p(quarry)): 11.838800430297852 or 11.838800430297852 or 11.838800430297852
('group', 'northern', 'syrian', 'city', 'aleppo', 'similar')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2454, 'similar') has the max prob in order to be the next word. The prob is: 5.8002602600026876e-05
974000 The nn predicts (tensor([150]), 'killing') the next word with max prob: tensor([0.0445])
The negative loglikelihood (-log p(similar)): 9.755023002624512 or 9.755023002624512 or 9.755023002624512
('every', 'move', 'facebook', 'twitter', 'instagram', 'whatsapp')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4602, 'whatsapp') has the max prob in order to be the next word. The prob is: 6.8436729634413496e-06
974100 The nn predicts (tensor([490]), 'facebook') the next word with max prob: tensor([0.0172])
The negative loglikelihood (-log p(whatsapp)): 11.892186164855957 or 11.892186164855957 or 11.892186164855957
('ruling', 'twitter', 'agreed', 'hand', 'information', 'regarding')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3321, 'regarding') has the max prob in order to be the next word. The prob is: 0.00010567490971880034
974200 The nn predicts (tensor([79]), 'officials') the next word with max prob: tensor([0.0354])
The negative loglikelihood (-log p(regarding)): 9.155142784118652 or 9.155142784118652 or 9.155142784118652
('least', 'people', 'killed', 'moscow', 'crash', 'passenger')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1565, 'passenger') has the max prob in order to be the next word. The prob is: 0.0005547114997170866
974300 The nn predicts (tensor([150]), 'killing') the next word with max prob: tensor([0.0184])
The negative loglikelihood (-log p(passenger)): 7.4970622062683105 or 7.4970622062683105 or 7.4970622062683105
('fourth', 'attack', 'syria', 'since', 'beginning', 'civil')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (452, 'civil') has the max prob in order to be the next word. The prob is: 0.008972362615168095
974400 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0592])
The negative loglikelihood (-log p(civil)): 4.713606357574463 or 4.713606357574463 or 4.713606357574463
('ramadan', 'violence', 'rips', 'iraq', 'mosques', 'cafes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10052, 'cafes') has the max prob in order to be the next word. The prob is: 9.928399776981678e-06
974500 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0918])
The negative loglikelihood (-log p(cafes)): 11.520111083984375 or 11.520111083984375 or 11.520111083984375
('police', 'officers', 'lawmaker', 'hurt', 'belfast', 'riots')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1042, 'riots') has the max prob in order to be the next word. The prob is: 0.005513160023838282
974600 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0672])
The negative loglikelihood (-log p(riots)): 5.20061731338501 or 5.20061731338501 or 5.20061731338501
('star', 'television', 'show', 'glee', 'corey', 'monteith')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (27419, 'monteith') has the max prob in order to be the next word. The prob is: 4.365464519651141e-06
974700 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0450])
The negative loglikelihood (-log p(monteith)): 12.34178638458252 or 12.34178638458252 or 12.34178638458252
('scraps', 'plans', 'uranium', 'plant', 'protest', 'chinese')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (41, 'chinese') has the max prob in order to be the next word. The prob is: 0.00318698538467288
974800 The nn predicts (tensor([119]), 'protesters') the next word with max prob: tensor([0.0205])
The negative loglikelihood (-log p(chinese)): 5.7486796379089355 or 5.7486796379089355 or 5.7486796379089355
('news', 'snowden', 'enough', 'information', 'cause', 'damage')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1193, 'damage') has the max prob in order to be the next word. The prob is: 0.0016555875772610307
974900 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(damage)): 6.403599262237549 or 6.403599262237549 or 6.403599262237549
('compliance', 'using', 'combination', 'military', 'might', 'economic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (272, 'economic') has the max prob in order to be the next word. The prob is: 0.00018189920228905976
975000 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0245])
The negative loglikelihood (-log p(economic)): 8.61205768585205 or 8.61205768585205 or 8.61205768585205
('black', 'cabinet', 'minister', 'orangutan', 'latest', 'racist')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1684, 'racist') has the max prob in order to be the next word. The prob is: 0.00014006567653268576
975100 The nn predicts (tensor([141]), 'violence') the next word with max prob: tensor([0.0186])
The negative loglikelihood (-log p(racist)): 8.873398780822754 or 8.873398780822754 or 8.873398780822754
('knocked', 'ranting', 'racist', 'drinker', 'claiming', 'goldman')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4047, 'goldman') has the max prob in order to be the next word. The prob is: 8.792616426944733e-06
975200 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0257])
The negative loglikelihood (-log p(goldman)): 11.64159870147705 or 11.64159870147705 or 11.64159870147705
('according', 'government', 'figures', 'compared', 'one', 'per')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (909, 'per') has the max prob in order to be the next word. The prob is: 0.0006324503337964416
975300 The nn predicts (tensor([276]), 'largest') the next word with max prob: tensor([0.0297])
The negative loglikelihood (-log p(per)): 7.365909099578857 or 7.365909099578857 or 7.365909099578857
('motivations', 'take', 'snowden', 'wall', 'street', 'journal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3900, 'journal') has the max prob in order to be the next word. The prob is: 0.000156043388415128
975400 The nn predicts (tensor([91]), 'protest') the next word with max prob: tensor([0.0139])
The negative loglikelihood (-log p(journal)): 8.765376091003418 or 8.765376091003418 or 8.765376091003418
('glee', 'actor', 'found', 'dead', 'hotel', 'room')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2397, 'room') has the max prob in order to be the next word. The prob is: 0.0013696696842089295
975500 The nn predicts (tensor([253]), 'crash') the next word with max prob: tensor([0.0383])
The negative loglikelihood (-log p(room)): 6.593185901641846 or 6.593185901641846 or 6.593185901641846
('officials', 'say', 'israel', 'targeted', 'advanced', 'cruise')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1846, 'cruise') has the max prob in order to be the next word. The prob is: 7.19400923117064e-05
975600 The nn predicts (tensor([582]), 'missiles') the next word with max prob: tensor([0.1136])
The negative loglikelihood (-log p(cruise)): 9.539676666259766 or 9.539676666259766 or 9.539676666259766
('language', 'called', 'warlpiri', 'rampaku', 'created', 'children')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (93, 'children') has the max prob in order to be the next word. The prob is: 0.0008153288508765399
975700 The nn predicts (tensor([37]), 'one') the next word with max prob: tensor([0.0195])
The negative loglikelihood (-log p(children)): 7.111918926239014 or 7.111918926239014 or 7.111918926239014
('calls', 'quit', 'messages', 'treasurer', 'socialists', 'also')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (464, 'also') has the max prob in order to be the next word. The prob is: 0.000919008394703269
975800 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0183])
The negative loglikelihood (-log p(also)): 6.992215156555176 or 6.992215156555176 or 6.992215156555176
('say', 'glaxosmithkline', 'broke', 'law', 'boost', 'sales')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1186, 'sales') has the max prob in order to be the next word. The prob is: 0.0006323035340756178
975900 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0250])
The negative loglikelihood (-log p(sales)): 7.366140842437744 or 7.366140842437744 or 7.366140842437744
('japan', 'still', 'giving', 'tens', 'millions', 'dollars')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1390, 'dollars') has the max prob in order to be the next word. The prob is: 0.029672756791114807
976000 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0386])
The negative loglikelihood (-log p(dollars)): 3.5175259113311768 or 3.5175259113311768 or 3.5175259113311768
('rakyat', 'filed', 'suit', 'malaysia', 'election', 'commission')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (906, 'commission') has the max prob in order to be the next word. The prob is: 0.00095115922158584
976100 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0184])
The negative loglikelihood (-log p(commission)): 6.957828998565674 or 6.957828998565674 or 6.957828998565674
('say', 'glaxosmithkline', 'broke', 'laws', 'sales', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.0023156118113547564
976200 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(police)): 6.068081378936768 or 6.068081378936768 or 6.068081378936768
('activists', 'climbed', 'fences', 'break', 'edf', 'nuclear')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (32, 'nuclear') has the max prob in order to be the next word. The prob is: 0.0010257979156449437
976300 The nn predicts (tensor([347]), 'plant') the next word with max prob: tensor([0.0590])
The negative loglikelihood (-log p(nuclear)): 6.882284641265869 or 6.882284641265869 or 6.882284641265869
('china', 'aims', 'quadruple', 'solar', 'power', 'generating')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9886, 'generating') has the max prob in order to be the next word. The prob is: 6.8013650889042765e-06
976400 The nn predicts (tensor([347]), 'plant') the next word with max prob: tensor([0.2955])
The negative loglikelihood (-log p(generating)): 11.89838695526123 or 11.89838695526123 or 11.89838695526123
('sign', 'new', 'deal', 'reducing', 'nuclear', 'threat')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (202, 'threat') has the max prob in order to be the next word. The prob is: 0.00424022926017642
976500 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.1882])
The negative loglikelihood (-log p(threat)): 5.463138103485107 or 5.463138103485107 or 5.463138103485107
('boost', 'sales', 'illegally', 'raise', 'price', 'medicines')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5818, 'medicines') has the max prob in order to be the next word. The prob is: 0.00012124292697990313
976600 The nn predicts (tensor([154]), 'billion') the next word with max prob: tensor([0.0137])
The negative loglikelihood (-log p(medicines)): 9.017714500427246 or 9.017714500427246 or 9.017714500427246
('secret', 'service', 'men', 'door', 'checking', 'political')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (178, 'political') has the max prob in order to be the next word. The prob is: 0.0006484078476205468
976700 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0226])
The negative loglikelihood (-log p(political)): 7.3409905433654785 or 7.3409905433654785 or 7.3409905433654785
('mainstream', 'institutions', 'taught', 'untrained', 'teacher', 'according')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (291, 'according') has the max prob in order to be the next word. The prob is: 0.0006969718378968537
976800 The nn predicts (tensor([220]), 'sex') the next word with max prob: tensor([0.0189])
The negative loglikelihood (-log p(according)): 7.268765449523926 or 7.268765449523926 or 7.268765449523926
('uk', 'house', 'lords', 'approves', 'equal', 'marriage')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (606, 'marriage') has the max prob in order to be the next word. The prob is: 0.07068627327680588
976900 The nn predicts (tensor([606]), 'marriage') the next word with max prob: tensor([0.0707])
The negative loglikelihood (-log p(marriage)): 2.6495039463043213 or 2.6495039463043213 or 2.6495039463043213
('german', 'chancellor', 'angela', 'merkel', 'given', 'backing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2185, 'backing') has the max prob in order to be the next word. The prob is: 0.00022067139798309654
977000 The nn predicts (tensor([492]), 'asylum') the next word with max prob: tensor([0.0208])
The negative loglikelihood (-log p(backing)): 8.418835639953613 or 8.418835639953613 or 8.418835639953613
('criminalizing', 'open', 'homosexuality', 'support', 'homosexuality', 'including')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (237, 'including') has the max prob in order to be the next word. The prob is: 0.00046127050882205367
977100 The nn predicts (tensor([606]), 'marriage') the next word with max prob: tensor([0.0538])
The negative loglikelihood (-log p(including)): 7.681525707244873 or 7.681525707244873 or 7.681525707244873
('syrian', 'rebels', 'part', 'strategy', 'cement', 'ties')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (467, 'ties') has the max prob in order to be the next word. The prob is: 0.0050395941361784935
977200 The nn predicts (tensor([314]), 'arms') the next word with max prob: tensor([0.0847])
The negative loglikelihood (-log p(ties)): 5.290429592132568 or 5.290429592132568 or 5.290429592132568
('reportedly', 'captures', 'kingpin', 'dreaded', 'zetas', 'cartel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1785, 'cartel') has the max prob in order to be the next word. The prob is: 0.0032876087352633476
977300 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0186])
The negative loglikelihood (-log p(cartel)): 5.717594623565674 or 5.717594623565674 or 5.717594623565674
('spain', 'premier', 'refuses', 'resign', 'fraud', 'scandal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (436, 'scandal') has the max prob in order to be the next word. The prob is: 0.08120840787887573
977400 The nn predicts (tensor([436]), 'scandal') the next word with max prob: tensor([0.0812])
The negative loglikelihood (-log p(scandal)): 2.5107364654541016 or 2.5107364654541016 or 2.5107364654541016
('vulcan', 'rejected', 'astronomy', 'international', 'naming', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.005158186890184879
977500 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(police)): 5.267169952392578 or 5.267169952392578 or 5.267169952392578
('since', 'soviet', 'times', 'involving', 'troops', 'tanks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1499, 'tanks') has the max prob in order to be the next word. The prob is: 0.000637367891613394
977600 The nn predicts (tensor([122]), 'afghanistan') the next word with max prob: tensor([0.0171])
The negative loglikelihood (-log p(tanks)): 7.358163356781006 or 7.358163356781006 or 7.358163356781006
('first', 'small', 'step', 'toward', 'holding', 'account')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1321, 'account') has the max prob in order to be the next word. The prob is: 5.0750157242873684e-05
977700 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0227])
The negative loglikelihood (-log p(account)): 9.888595581054688 or 9.888595581054688 or 9.888595581054688
('struck', 'rioters', 'back', 'belfast', 'streets', 'hurling')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13399, 'hurling') has the max prob in order to be the next word. The prob is: 1.7254761814911035e-06
977800 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0518])
The negative loglikelihood (-log p(hurling)): 13.270007133483887 or 13.270007133483887 or 13.270007133483887
('amanda', 'palmer', 'attacks', 'daily', 'mail', 'nude')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4953, 'nude') has the max prob in order to be the next word. The prob is: 4.764355253428221e-05
977900 The nn predicts (tensor([338]), 'online') the next word with max prob: tensor([0.0202])
The negative loglikelihood (-log p(nude)): 9.951763153076172 or 9.951763153076172 or 9.951763153076172
('korea', 'ship', 'president', 'says', 'vessel', 'en')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3623, 'en') has the max prob in order to be the next word. The prob is: 5.9095644246554e-05
978000 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0094])
The negative loglikelihood (-log p(en)): 9.736352920532227 or 9.736352920532227 or 9.736352920532227
('create', 'huge', 'antarctic', 'marine', 'sanctuaries', 'missed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5443, 'missed') has the max prob in order to be the next word. The prob is: 1.6132446035044268e-05
978100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0073])
The negative loglikelihood (-log p(missed)): 11.03467845916748 or 11.03467845916748 or 11.03467845916748
('found', 'swiss', 'lake', 'near', 'nuclear', 'plant')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (347, 'plant') has the max prob in order to be the next word. The prob is: 0.5774612426757812
978200 The nn predicts (tensor([347]), 'plant') the next word with max prob: tensor([0.5775])
The negative loglikelihood (-log p(plant)): 0.5491139888763428 or 0.549113929271698 or 0.549113929271698
('writing', 'book', 'exposing', 'chinese', 'atrocities', 'tibet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1842, 'tibet') has the max prob in order to be the next word. The prob is: 6.188891711644828e-05
978300 The nn predicts (tensor([331]), 'crimes') the next word with max prob: tensor([0.0093])
The negative loglikelihood (-log p(tibet)): 9.690169334411621 or 9.690169334411621 or 9.690169334411621
('flight', 'refugees', 'worst', 'since', 'rwanda', 'genocide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (875, 'genocide') has the max prob in order to be the next word. The prob is: 0.0004660790436901152
978400 The nn predicts (tensor([153]), 'since') the next word with max prob: tensor([0.0447])
The negative loglikelihood (-log p(genocide)): 7.6711554527282715 or 7.6711554527282715 or 7.6711554527282715
('fattah', 'led', 'ousting', 'mohammed', 'morsi', 'becoming')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1595, 'becoming') has the max prob in order to be the next word. The prob is: 8.94534241524525e-05
978500 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.0628])
The negative loglikelihood (-log p(becoming)): 9.321792602539062 or 9.321792602539062 or 9.321792602539062
('relations', 'bad', 'place', 'long', 'time', 'come')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (613, 'come') has the max prob in order to be the next word. The prob is: 0.002973119029775262
978600 The nn predicts (tensor([153]), 'since') the next word with max prob: tensor([0.0286])
The negative loglikelihood (-log p(come)): 5.818143844604492 or 5.818143844604492 or 5.818143844604492
('<s>', '<s>', 'george', 'zimmerman', 'prelude', 'shooting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (567, 'shooting') has the max prob in order to be the next word. The prob is: 0.00040550759877078235
978700 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0319])
The negative loglikelihood (-log p(shooting)): 7.810370922088623 or 7.810370922088623 or 7.810370922088623
('used', 'test', 'subjects', 'malnutrition', 'experiments', 'without')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (259, 'without') has the max prob in order to be the next word. The prob is: 0.0015219602501019835
978800 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(without)): 6.487756252288818 or 6.487756252288818 or 6.487756252288818
('posing', 'nude', 'body', 'issue', 'espn', 'magazine')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1923, 'magazine') has the max prob in order to be the next word. The prob is: 0.0010610308963805437
978900 The nn predicts (tensor([808]), 'debate') the next word with max prob: tensor([0.0077])
The negative loglikelihood (-log p(magazine)): 6.848514080047607 or 6.848514080047607 or 6.848514080047607
('anger', 'erupts', 'canadian', 'prime', 'minister', 'stephen')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2496, 'stephen') has the max prob in order to be the next word. The prob is: 0.00014135829405859113
979000 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0208])
The negative loglikelihood (-log p(stephen)): 8.864212989807129 or 8.864212989807129 or 8.864212989807129
('pollution', 'may', 'increased', 'ferocity', 'frequency', 'hurricanes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11410, 'hurricanes') has the max prob in order to be the next word. The prob is: 4.3277245822537225e-06
979100 The nn predicts (tensor([95]), 'global') the next word with max prob: tensor([0.0171])
The negative loglikelihood (-log p(hurricanes)): 12.350468635559082 or 12.350468635559082 or 12.350468635559082
('claim', 'death', 'weapons', 'expert', 'dr', 'david')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (498, 'david') has the max prob in order to be the next word. The prob is: 0.00036831205943599343
979200 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0308])
The negative loglikelihood (-log p(david)): 7.906579971313477 or 7.906579971313477 or 7.906579971313477
('armed', 'protesters', 'raid', 'eastern', 'zueitina', 'terminal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4648, 'terminal') has the max prob in order to be the next word. The prob is: 1.266660365217831e-05
979300 The nn predicts (tensor([242]), 'town') the next word with max prob: tensor([0.0729])
The negative loglikelihood (-log p(terminal)): 11.276541709899902 or 11.276541709899902 or 11.276541709899902
('use', 'teargas', 'hrw', 'documented', 'cases', 'people')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14, 'people') has the max prob in order to be the next word. The prob is: 0.01273373607546091
979400 The nn predicts (tensor([237]), 'including') the next word with max prob: tensor([0.0243])
The negative loglikelihood (-log p(people)): 4.363500595092773 or 4.363500595092773 or 4.363500595092773
('<s>', 'eu', 'ban', 'fipronil', 'protect', 'honeybees')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15122, 'honeybees') has the max prob in order to be the next word. The prob is: 3.011117087226012e-06
979500 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0312])
The negative loglikelihood (-log p(honeybees)): 12.713199615478516 or 12.713199615478516 or 12.713199615478516
('cloud', 'stretched', 'like', 'spaghetti', 'giant', 'gas')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (180, 'gas') has the max prob in order to be the next word. The prob is: 0.0013033399591222405
979600 The nn predicts (tensor([288]), 'around') the next word with max prob: tensor([0.0040])
The negative loglikelihood (-log p(gas)): 6.642825126647949 or 6.642825126647949 or 6.642825126647949
('femen', 'one', 'inspirations', 'new', 'stamp', 'depicting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6083, 'depicting') has the max prob in order to be the next word. The prob is: 3.697624561027624e-05
979700 The nn predicts (tensor([165]), 'national') the next word with max prob: tensor([0.0069])
The negative loglikelihood (-log p(depicting)): 10.20523452758789 or 10.20523452758789 or 10.20523452758789
('children', 'kanakir', 'syria', 'south', 'small', 'town')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (242, 'town') has the max prob in order to be the next word. The prob is: 0.07346485555171967
979800 The nn predicts (tensor([242]), 'town') the next word with max prob: tensor([0.0735])
The negative loglikelihood (-log p(town)): 2.610948085784912 or 2.610948085784912 or 2.610948085784912
('<s>', 'kuwait', 'jails', 'woman', 'months', 'tweets')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2787, 'tweets') has the max prob in order to be the next word. The prob is: 0.0006477631395682693
979900 The nn predicts (tensor([194]), 'prison') the next word with max prob: tensor([0.2215])
The negative loglikelihood (-log p(tweets)): 7.341985702514648 or 7.34198522567749 or 7.34198522567749
('content', 'preferential', 'treatment', 'video', 'article', 'web')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1292, 'web') has the max prob in order to be the next word. The prob is: 0.0006768584717065096
980000 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0181])
The negative loglikelihood (-log p(web)): 7.298048496246338 or 7.298048496246338 or 7.298048496246338
('spanish', 'prime', 'minister', 'refuses', 'resign', 'corruption')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (411, 'corruption') has the max prob in order to be the next word. The prob is: 0.003744707442820072
980100 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0135])
The negative loglikelihood (-log p(corruption)): 5.587411880493164 or 5.587411880493164 or 5.587411880493164
('angry', 'rcmp', 'seize', 'guns', 'high', 'river')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1400, 'river') has the max prob in order to be the next word. The prob is: 0.0013516799081116915
980200 The nn predicts (tensor([297]), 'school') the next word with max prob: tensor([0.0159])
The negative loglikelihood (-log p(river)): 6.606407165527344 or 6.606407165527344 or 6.606407165527344
('officials', 'north', 'korea', 'behind', 'recent', 'cyberattacks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6691, 'cyberattacks') has the max prob in order to be the next word. The prob is: 5.730973862227984e-05
980300 The nn predicts (tensor([60]), 'attacks') the next word with max prob: tensor([0.0216])
The negative loglikelihood (-log p(cyberattacks)): 9.767040252685547 or 9.767040252685547 or 9.767040252685547
('last', 'year', 'provoking', 'attack', 'smearing', 'islam')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (720, 'islam') has the max prob in order to be the next word. The prob is: 0.002889208495616913
980400 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0115])
The negative loglikelihood (-log p(islam)): 5.84677267074585 or 5.84677267074585 or 5.84677267074585
('murder', 'dismemberment', 'video', 'website', 'police', 'ignored')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3187, 'ignored') has the max prob in order to be the next word. The prob is: 7.321336306631565e-05
980500 The nn predicts (tensor([769]), 'officer') the next word with max prob: tensor([0.0349])
The negative loglikelihood (-log p(ignored)): 9.522132873535156 or 9.522132873535156 or 9.522132873535156
('guinea', 'deploys', 'troops', 'killed', 'ethnic', 'clashes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (418, 'clashes') has the max prob in order to be the next word. The prob is: 0.11887584626674652
980600 The nn predicts (tensor([418]), 'clashes') the next word with max prob: tensor([0.1189])
The negative loglikelihood (-log p(clashes)): 2.1296756267547607 or 2.1296756267547607 or 2.1296756267547607
('indian', 'school', 'lunch', 'poisoning', 'doctors', 'race')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1179, 'race') has the max prob in order to be the next word. The prob is: 7.804931556165684e-06
980700 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0528])
The negative loglikelihood (-log p(race)): 11.760754585266113 or 11.760754585266113 or 11.760754585266113
('glamorising', 'terrorism', 'featuring', 'accused', 'boston', 'bomber')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (654, 'bomber') has the max prob in order to be the next word. The prob is: 0.004050905816257
980800 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0486])
The negative loglikelihood (-log p(bomber)): 5.508814811706543 or 5.508814811706543 or 5.508814811706543
('alas', 'nobody', 'lives', 'anymore', 'bassem', 'youssef')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11579, 'youssef') has the max prob in order to be the next word. The prob is: 1.7341040802421048e-05
980900 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0094])
The negative loglikelihood (-log p(youssef)): 10.962434768676758 or 10.962434768676758 or 10.962434768676758
('leader', 'alexei', 'navalny', 'putin', 'biggest', 'political')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (178, 'political') has the max prob in order to be the next word. The prob is: 0.019080670550465584
981000 The nn predicts (tensor([178]), 'political') the next word with max prob: tensor([0.0191])
The negative loglikelihood (-log p(political)): 3.9590795040130615 or 3.9590795040130615 or 3.9590795040130615
('police', 'officers', 'two', 'seriously', 'attack', 'wednesday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (362, 'wednesday') has the max prob in order to be the next word. The prob is: 0.0014699656749144197
981100 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0263])
The negative loglikelihood (-log p(wednesday)): 6.522516250610352 or 6.522516250610352 or 6.522516250610352
('gas', 'exploration', 'disputed', 'waters', 'east', 'china')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1, 'china') has the max prob in order to be the next word. The prob is: 0.017338436096906662
981200 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0504])
The negative loglikelihood (-log p(china)): 4.0548295974731445 or 4.0548295974731445 or 4.0548295974731445
('government', 'intentionally', 'withheld', 'rations', 'vitamin', 'supplements')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18510, 'supplements') has the max prob in order to be the next word. The prob is: 3.2283271593769314e-06
981300 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0135])
The negative loglikelihood (-log p(supplements)): 12.643546104431152 or 12.643546104431152 or 12.643546104431152
('century', 'warmest', 'decade', 'recorded', 'land', 'ocean')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1078, 'ocean') has the max prob in order to be the next word. The prob is: 0.0009885696927085519
981400 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0185])
The negative loglikelihood (-log p(ocean)): 6.919251441955566 or 6.919251441955566 or 6.919251441955566
('made', 'clear', 'father', 'making', 'remarkable', 'progress')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1686, 'progress') has the max prob in order to be the next word. The prob is: 0.0007042610086500645
981500 The nn predicts (tensor([101]), 'time') the next word with max prob: tensor([0.0107])
The negative loglikelihood (-log p(progress)): 7.25836181640625 or 7.258361339569092 or 7.258361339569092
('insurgencies', 'tapping', 'illegal', 'ivory', 'source', 'funds')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1324, 'funds') has the max prob in order to be the next word. The prob is: 0.00014173903036862612
981600 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0581])
The negative loglikelihood (-log p(funds)): 8.861522674560547 or 8.861522674560547 or 8.861522674560547
('police', 'deployed', 'streets', 'athens', 'german', 'finance')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1093, 'finance') has the max prob in order to be the next word. The prob is: 0.0002787546836771071
981700 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0657])
The negative loglikelihood (-log p(finance)): 8.185178756713867 or 8.185178756713867 or 8.185178756713867
('sanctions', 'israel', 'started', 'official', 'bds', 'movement')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (899, 'movement') has the max prob in order to be the next word. The prob is: 0.0007004549843259156
981800 The nn predicts (tensor([1562]), 'settlements') the next word with max prob: tensor([0.0173])
The negative loglikelihood (-log p(movement)): 7.26378059387207 or 7.26378059387207 or 7.26378059387207
('chiefs', 'staff', 'general', 'martin', 'dempsey', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.036911576986312866
981900 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0369])
The negative loglikelihood (-log p(says)): 3.2992300987243652 or 3.2992300987243652 or 3.2992300987243652
('oil', 'used', 'food', 'headmistress', 'insisted', 'use')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (174, 'use') has the max prob in order to be the next word. The prob is: 0.0011064157588407397
982000 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0101])
The negative loglikelihood (-log p(use)): 6.806629657745361 or 6.806629657745361 or 6.806629657745361
('video', 'crackstarter', 'campaign', 'go', 'towards', 'local')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (466, 'local') has the max prob in order to be the next word. The prob is: 0.0006683190003968775
982100 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0156])
The negative loglikelihood (-log p(local)): 7.310744762420654 or 7.310744762420654 or 7.310744762420654
('authorities', 'must', 'regulate', 'sale', 'acid', 'used')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (218, 'used') has the max prob in order to be the next word. The prob is: 0.0012522845063358545
982200 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0143])
The negative loglikelihood (-log p(used)): 6.682785987854004 or 6.682785987854004 or 6.682785987854004
('nationals', 'indicted', 'federal', 'grand', 'jury', 'thursday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (385, 'thursday') has the max prob in order to be the next word. The prob is: 0.0006503656622953713
982300 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0363])
The negative loglikelihood (-log p(thursday)): 7.337975978851318 or 7.337975978851318 or 7.337975978851318
('wild', 'animals', 'die', 'german', 'roads', 'scientists')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (305, 'scientists') has the max prob in order to be the next word. The prob is: 0.0004976911004632711
982400 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0093])
The negative loglikelihood (-log p(scientists)): 7.605530738830566 or 7.605530738830566 or 7.605530738830566
('rendition', 'case', 'given', 'prison', 'sentence', 'absentia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10134, 'absentia') has the max prob in order to be the next word. The prob is: 4.5398403017316014e-05
982500 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0500])
The negative loglikelihood (-log p(absentia)): 10.000033378601074 or 10.000033378601074 or 10.000033378601074
('leader', 'sam', 'rainsy', 'returned', 'cambodia', 'days')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (210, 'days') has the max prob in order to be the next word. The prob is: 0.0013718911213800311
982600 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0456])
The negative loglikelihood (-log p(days)): 6.591565132141113 or 6.591565132141113 or 6.591565132141113
('new', 'guinea', 'asylum', 'seeker', 'arrives', 'australia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (110, 'australia') has the max prob in order to be the next word. The prob is: 0.004487801808863878
982700 The nn predicts (tensor([115]), 'back') the next word with max prob: tensor([0.0265])
The negative loglikelihood (-log p(australia)): 5.4063920974731445 or 5.4063920974731445 or 5.4063920974731445
('companies', 'apple', 'google', 'avoid', 'billions', 'dollars')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1390, 'dollars') has the max prob in order to be the next word. The prob is: 0.013607703149318695
982800 The nn predicts (tensor([1390]), 'dollars') the next word with max prob: tensor([0.0136])
The negative loglikelihood (-log p(dollars)): 4.297119140625 or 4.297119140625 or 4.297119140625
('hikes', 'austerity', 'measures', 'include', 'include', 'tax')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (274, 'tax') has the max prob in order to be the next word. The prob is: 0.002438838593661785
982900 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(tax)): 6.016233444213867 or 6.016233444213867 or 6.016233444213867
('<s>', 'venezuela', 'slams', 'repressive', 'regimes', 'remarks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2453, 'remarks') has the max prob in order to be the next word. The prob is: 0.00016771917580626905
983000 The nn predicts (tensor([6]), 'russia') the next word with max prob: tensor([0.0133])
The negative loglikelihood (-log p(remarks)): 8.693219184875488 or 8.693219184875488 or 8.693219184875488
('cia', 'station', 'chief', 'sought', 'italy', 'rendition')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4455, 'rendition') has the max prob in order to be the next word. The prob is: 0.00022638925292994827
983100 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0257])
The negative loglikelihood (-log p(rendition)): 8.393254280090332 or 8.393254280090332 or 8.393254280090332
('court', 'finds', 'berlusconi', 'associates', 'guilty', 'sex')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (220, 'sex') has the max prob in order to be the next word. The prob is: 0.02520388551056385
983200 The nn predicts (tensor([220]), 'sex') the next word with max prob: tensor([0.0252])
The negative loglikelihood (-log p(sex)): 3.6807570457458496 or 3.6807570457458496 or 3.6807570457458496
('others', 'son', 'arrested', 'allegedly', 'stealing', 'october')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2081, 'october') has the max prob in order to be the next word. The prob is: 9.182693247566931e-06
983300 The nn predicts (tensor([220]), 'sex') the next word with max prob: tensor([0.0227])
The negative loglikelihood (-log p(october)): 11.598190307617188 or 11.598190307617188 or 11.598190307617188
('brazen', 'costa', 'drug', 'lord', 'caught', 'spain')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (277, 'spain') has the max prob in order to be the next word. The prob is: 0.00047953694593161345
983400 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0164])
The negative loglikelihood (-log p(spain)): 7.6426897048950195 or 7.6426897048950195 or 7.6426897048950195
('seizes', 'baby', 'elephant', 'tusks', 'major', 'ivory')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1248, 'ivory') has the max prob in order to be the next word. The prob is: 0.0002867510193027556
983500 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0106])
The negative loglikelihood (-log p(ivory)): 8.156896591186523 or 8.156896591186523 or 8.156896591186523
('entering', 'australia', 'instead', 'sending', 'papua', 'new')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4, 'new') has the max prob in order to be the next word. The prob is: 0.02502039633691311
983600 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0250])
The negative loglikelihood (-log p(new)): 3.688063859939575 or 3.688063859939575 or 3.688063859939575
('facing', 'months', 'jail', 'reporting', 'rape', 'dubai')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1115, 'dubai') has the max prob in order to be the next word. The prob is: 0.0001557048235554248
983700 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0898])
The negative loglikelihood (-log p(dubai)): 8.767548561096191 or 8.767548561096191 or 8.767548561096191
('rare', 'protest', 'china', 'prompts', 'government', 'scrap')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3715, 'scrap') has the max prob in order to be the next word. The prob is: 4.6197365008993074e-05
983800 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0191])
The negative loglikelihood (-log p(scrap)): 9.982587814331055 or 9.982587814331055 or 9.982587814331055
('slogans', 'tossing', 'headscarves', 'colorful', 'fabrics', 'fancy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12377, 'fancy') has the max prob in order to be the next word. The prob is: 1.697558764135465e-05
983900 The nn predicts (tensor([100]), 'video') the next word with max prob: tensor([0.0049])
The negative loglikelihood (-log p(fancy)): 10.983734130859375 or 10.983734130859375 or 10.983734130859375
('although', 'attacks', 'taking', 'major', 'toll', 'global')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (95, 'global') has the max prob in order to be the next word. The prob is: 0.0026995670050382614
984000 The nn predicts (tensor([153]), 'since') the next word with max prob: tensor([0.0236])
The negative loglikelihood (-log p(global)): 5.914663791656494 or 5.914663791656494 or 5.914663791656494
('vaccine', 'measles', 'mumps', 'rubella', 'bill', 'come')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (613, 'come') has the max prob in order to be the next word. The prob is: 0.0021997380536049604
984100 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0201])
The negative loglikelihood (-log p(come)): 6.119417190551758 or 6.119417190551758 or 6.119417190551758
('media', 'pirate', 'tv', 'channel', 'called', 'tv')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (374, 'tv') has the max prob in order to be the next word. The prob is: 0.002115547191351652
984200 The nn predicts (tensor([300]), 'friday') the next word with max prob: tensor([0.0208])
The negative loglikelihood (-log p(tv)): 6.15844202041626 or 6.158441543579102 or 6.158441543579102
('death', 'watermelon', 'vendor', 'sparked', 'online', 'fury')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3449, 'fury') has the max prob in order to be the next word. The prob is: 8.218669972848147e-05
984300 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0478])
The negative loglikelihood (-log p(fury)): 9.406517028808594 or 9.406517028808594 or 9.406517028808594
('aimed', 'keeping', 'men', 'distracted', 'holy', 'month')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (375, 'month') has the max prob in order to be the next word. The prob is: 0.0044050742872059345
984400 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0480])
The negative loglikelihood (-log p(month)): 5.4249982833862305 or 5.4249982833862305 or 5.4249982833862305
('government', 'relax', 'interpretation', 'privacy', 'laws', 'provide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1372, 'provide') has the max prob in order to be the next word. The prob is: 0.0012768080923706293
984500 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0155])
The negative loglikelihood (-log p(provide)): 6.663392066955566 or 6.663392066955566 or 6.663392066955566
('first', 'comments', 'foreign', 'minister', 'nabil', 'fahmy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10374, 'fahmy') has the max prob in order to be the next word. The prob is: 2.8683336950052762e-06
984600 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0144])
The negative loglikelihood (-log p(fahmy)): 12.76177978515625 or 12.761778831481934 or 12.761778831481934
('pesticide', 'cooking', 'oil', 'chef', 'told', 'principal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7116, 'principal') has the max prob in order to be the next word. The prob is: 2.2850408640806563e-05
984700 The nn predicts (tensor([679]), 'guilty') the next word with max prob: tensor([0.0142])
The negative loglikelihood (-log p(principal)): 10.686541557312012 or 10.686541557312012 or 10.686541557312012
('hold', 'onto', 'power', 'years', 'even', 'face')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (187, 'face') has the max prob in order to be the next word. The prob is: 0.0020679032895714045
984800 The nn predicts (tensor([1424]), 'worse') the next word with max prob: tensor([0.0119])
The negative loglikelihood (-log p(face)): 6.181220054626465 or 6.181220054626465 or 6.181220054626465
('undergo', 'unsafe', 'abortions', 'conducted', 'unsanitary', 'conditions')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1279, 'conditions') has the max prob in order to be the next word. The prob is: 0.001926081022247672
984900 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0280])
The negative loglikelihood (-log p(conditions)): 6.252267837524414 or 6.252267837524414 or 6.252267837524414
('marriage', 'reported', 'alleged', 'rape', 'vowed', 'friday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (300, 'friday') has the max prob in order to be the next word. The prob is: 0.005296085495501757
985000 The nn predicts (tensor([420]), 'rape') the next word with max prob: tensor([0.0173])
The negative loglikelihood (-log p(friday)): 5.240787506103516 or 5.240787506103516 or 5.240787506103516
('ignored', 'massive', 'misconduct', 'corruption', 'decade', 'private')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (879, 'private') has the max prob in order to be the next word. The prob is: 0.0001575734931975603
985100 The nn predicts (tensor([608]), 'ago') the next word with max prob: tensor([0.0161])
The negative loglikelihood (-log p(private)): 8.755619049072266 or 8.755619049072266 or 8.755619049072266
('held', 'captive', 'house', 'border', 'city', 'reynosa')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (22467, 'reynosa') has the max prob in order to be the next word. The prob is: 2.059625558104017e-06
985200 The nn predicts (tensor([112]), 'near') the next word with max prob: tensor([0.0442])
The negative loglikelihood (-log p(reynosa)): 13.092986106872559 or 13.092986106872559 or 13.092986106872559
('fighter', 'jets', 'accidentally', 'bomb', 'great', 'barrier')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2074, 'barrier') has the max prob in order to be the next word. The prob is: 0.0006906726630404592
985300 The nn predicts (tensor([100]), 'video') the next word with max prob: tensor([0.0177])
The negative loglikelihood (-log p(barrier)): 7.277844429016113 or 7.277844429016113 or 7.277844429016113
('spy', 'threatens', 'expose', 'dodgy', 'dossier', 'used')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (218, 'used') has the max prob in order to be the next word. The prob is: 0.010197055526077747
985400 The nn predicts (tensor([331]), 'crimes') the next word with max prob: tensor([0.0195])
The negative loglikelihood (-log p(used)): 4.58565616607666 or 4.58565616607666 or 4.58565616607666
('sim', 'cards', 'finally', 'hacked', 'flaw', 'could')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (50, 'could') has the max prob in order to be the next word. The prob is: 0.004466686397790909
985500 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0195])
The negative loglikelihood (-log p(could)): 5.411108493804932 or 5.411108493804932 or 5.411108493804932
('stronger', 'says', 'david', 'cameron', 'prime', 'minister')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (25, 'minister') has the max prob in order to be the next word. The prob is: 0.8816027641296387
985600 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.8816])
The negative loglikelihood (-log p(minister)): 0.12601371109485626 or 0.12601371109485626 or 0.12601371109485626
('wraps', 'edition', 'ticket', 'sales', 'almost', 'percent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (497, 'percent') has the max prob in order to be the next word. The prob is: 0.009327586740255356
985700 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.0667])
The negative loglikelihood (-log p(percent)): 4.674778938293457 or 4.674778938293457 or 4.674778938293457
('hiring', 'aide', 'links', 'healthcare', 'firms', 'lobbyist')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13333, 'lobbyist') has the max prob in order to be the next word. The prob is: 3.1548004244541517e-06
985800 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0156])
The negative loglikelihood (-log p(lobbyist)): 12.666584968566895 or 12.666584968566895 or 12.666584968566895
('chance', 'show', 'true', 'colors', 'big', 'election')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (158, 'election') has the max prob in order to be the next word. The prob is: 0.00012247791164554656
985900 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.0069])
The negative loglikelihood (-log p(election)): 9.007579803466797 or 9.007579803466797 or 9.007579803466797
('every', 'uk', 'household', 'isps', 'contact', 'users')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1247, 'users') has the max prob in order to be the next word. The prob is: 0.0008705636137165129
986000 The nn predicts (tensor([652]), 'every') the next word with max prob: tensor([0.0182])
The negative loglikelihood (-log p(users)): 7.046369552612305 or 7.046369552612305 or 7.046369552612305
('japan', 'nationalistic', 'prime', 'minister', 'expected', 'expand')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2216, 'expand') has the max prob in order to be the next word. The prob is: 0.00023124586732592434
986100 The nn predicts (tensor([137]), 'end') the next word with max prob: tensor([0.0250])
The negative loglikelihood (-log p(expand)): 8.372029304504395 or 8.372029304504395 or 8.372029304504395
('bengal', 'villagers', 'boycott', 'polls', 'protest', 'rape')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (420, 'rape') has the max prob in order to be the next word. The prob is: 0.00048692806740291417
986200 The nn predicts (tensor([91]), 'protest') the next word with max prob: tensor([0.0201])
The negative loglikelihood (-log p(rape)): 7.627394199371338 or 7.627394199371338 or 7.627394199371338
('rejected', 'calls', 'snap', 'elections', 'resolve', 'political')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (178, 'political') has the max prob in order to be the next word. The prob is: 0.02336025983095169
986300 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0311])
The negative loglikelihood (-log p(political)): 3.7567191123962402 or 3.7567191123962402 or 3.7567191123962402
('governance', 'election', 'win', 'ruling', 'party', 'signals')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2445, 'signals') has the max prob in order to be the next word. The prob is: 4.720097786048427e-05
986400 The nn predicts (tensor([52]), 'leader') the next word with max prob: tensor([0.0733])
The negative loglikelihood (-log p(signals)): 9.961095809936523 or 9.961095809936523 or 9.961095809936523
('cambridge', 'carries', 'torch', 'british', 'monarchy', 'style')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3700, 'style') has the max prob in order to be the next word. The prob is: 0.0002541587164159864
986500 The nn predicts (tensor([91]), 'protest') the next word with max prob: tensor([0.0076])
The negative loglikelihood (-log p(style)): 8.277551651000977 or 8.277551651000977 or 8.277551651000977
('copyright', 'infringement', 'complaints', 'legislation', 'scaring', 'google')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (336, 'google') has the max prob in order to be the next word. The prob is: 0.0035865127574652433
986600 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0131])
The negative loglikelihood (-log p(google)): 5.630575180053711 or 5.630574703216553 or 5.630574703216553
('brings', 'high', 'expectations', 'unease', 'homecoming', 'south')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (23, 'south') has the max prob in order to be the next word. The prob is: 0.00539029436185956
986700 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.0199])
The negative loglikelihood (-log p(south)): 5.223155498504639 or 5.223155498504639 or 5.223155498504639
('arabia', 'allocates', 'bn', 'renewable', 'energy', 'shift')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2173, 'shift') has the max prob in order to be the next word. The prob is: 0.00024706244585104287
986800 The nn predicts (tensor([307]), 'energy') the next word with max prob: tensor([0.0607])
The negative loglikelihood (-log p(shift)): 8.305869102478027 or 8.305869102478027 or 8.305869102478027
('military', 'drone', 'surveillance', 'expanding', 'hot', 'spots')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5351, 'spots') has the max prob in order to be the next word. The prob is: 1.7320509869023226e-05
986900 The nn predicts (tensor([21]), 'north') the next word with max prob: tensor([0.0460])
The negative loglikelihood (-log p(spots)): 10.963619232177734 or 10.963619232177734 or 10.963619232177734
('outdated', 'coding', 'system', 'called', 'digital', 'encryption')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4551, 'encryption') has the max prob in order to be the next word. The prob is: 4.979378900316078e-06
987000 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0186])
The negative loglikelihood (-log p(encryption)): 12.210205078125 or 12.210205078125 or 12.210205078125
('marte', 'deborah', 'dalelv', 'norwegian', 'woman', 'centre')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1146, 'centre') has the max prob in order to be the next word. The prob is: 0.00025872301193885505
987100 The nn predicts (tensor([232]), 'shot') the next word with max prob: tensor([0.0266])
The negative loglikelihood (-log p(centre)): 8.25975227355957 or 8.25975227355957 or 8.25975227355957
('power', 'plant', 'admits', 'leaks', 'may', 'becontaminating')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (64237, 'becontaminating') has the max prob in order to be the next word. The prob is: 9.859700185188558e-07
987200 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0130])
The negative loglikelihood (-log p(becontaminating)): 13.82964038848877 or 13.829639434814453 or 13.829639434814453
('broken', 'abu', 'ghraib', 'taji', 'prisons', 'iraq')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44, 'iraq') has the max prob in order to be the next word. The prob is: 0.002249755198135972
987300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0166])
The negative loglikelihood (-log p(iraq)): 6.096933841705322 or 6.096933841705322 or 6.096933841705322
('forced', 'borrow', 'social', 'security', 'fund', 'pay')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (294, 'pay') has the max prob in order to be the next word. The prob is: 0.003211133647710085
987400 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0234])
The negative loglikelihood (-log p(pay)): 5.74113130569458 or 5.74113130569458 or 5.74113130569458
('conditions', 'ancient', 'melting', 'antarctic', 'ice', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.0014712440315634012
987500 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0345])
The negative loglikelihood (-log p(said)): 6.521646976470947 or 6.521646976470947 or 6.521646976470947
('rebels', 'attack', 'shiite', 'shrines', 'threaten', 'christian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (694, 'christian') has the max prob in order to be the next word. The prob is: 0.00024707126431167126
987600 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.0282])
The negative loglikelihood (-log p(christian)): 8.30583381652832 or 8.30583381652832 or 8.30583381652832
('gear', 'southwest', 'jet', 'collapses', 'laguardia', 'airport')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (363, 'airport') has the max prob in order to be the next word. The prob is: 0.32917723059654236
987700 The nn predicts (tensor([363]), 'airport') the next word with max prob: tensor([0.3292])
The negative loglikelihood (-log p(airport)): 1.1111589670181274 or 1.1111589670181274 or 1.1111589670181274
('letter', 'warning', 'country', 'constitutional', 'order', 'threat')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (202, 'threat') has the max prob in order to be the next word. The prob is: 7.177888619480655e-05
987800 The nn predicts (tensor([130]), 'ban') the next word with max prob: tensor([0.0224])
The negative loglikelihood (-log p(threat)): 9.54192066192627 or 9.54192066192627 or 9.54192066192627
('wiesenthal', 'center', 'launches', 'campaign', 'germany', 'find')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (478, 'find') has the max prob in order to be the next word. The prob is: 0.0002962784783449024
987900 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0063])
The negative loglikelihood (-log p(find)): 8.124210357666016 or 8.124210357666016 or 8.124210357666016
('amid', 'political', 'crisis', 'christians', 'attacked', 'several')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (573, 'several') has the max prob in order to be the next word. The prob is: 0.001202172483317554
988000 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0542])
The negative loglikelihood (-log p(several)): 6.723625183105469 or 6.723625183105469 or 6.723625183105469
('repression', 'protesters', 'journalists', 'rio', 'police', 'beats')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4288, 'beats') has the max prob in order to be the next word. The prob is: 1.1125999662908725e-05
988100 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0625])
The negative loglikelihood (-log p(beats)): 11.40622615814209 or 11.40622615814209 or 11.40622615814209
('obama', 'move', 'forward', 'plan', 'arm', 'syrian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (30, 'syrian') has the max prob in order to be the next word. The prob is: 0.2168678641319275
988200 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.2169])
The negative loglikelihood (-log p(syrian)): 1.528467059135437 or 1.528467059135437 or 1.528467059135437
('bloggers', 'charged', 'imprisoned', 'without', 'bail', 'posted')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2886, 'posted') has the max prob in order to be the next word. The prob is: 6.007571937516332e-05
988300 The nn predicts (tensor([914]), 'sentence') the next word with max prob: tensor([0.0333])
The negative loglikelihood (-log p(posted)): 9.719904899597168 or 9.719904899597168 or 9.719904899597168
('world', 'bank', 'africa', 'held', 'back', 'land')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (406, 'land') has the max prob in order to be the next word. The prob is: 0.000658366538118571
988400 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0121])
The negative loglikelihood (-log p(land)): 7.325748920440674 or 7.325748920440674 or 7.325748920440674
('critics', 'bill', 'including', 'many', 'catholics', 'berlusconi')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1304, 'berlusconi') has the max prob in order to be the next word. The prob is: 8.435437484877184e-05
988500 The nn predicts (tensor([237]), 'including') the next word with max prob: tensor([0.0112])
The negative loglikelihood (-log p(berlusconi)): 9.380483627319336 or 9.380483627319336 or 9.380483627319336
('reported', 'near', 'canadian', 'british', 'embassies', 'tripoli')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1479, 'tripoli') has the max prob in order to be the next word. The prob is: 0.00021993290283717215
988600 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0219])
The negative loglikelihood (-log p(tripoli)): 8.422187805175781 or 8.422187805175781 or 8.422187805175781
('qaeda', 'group', 'claims', 'abu', 'ghraib', 'prison')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (194, 'prison') has the max prob in order to be the next word. The prob is: 0.004902407992631197
988700 The nn predicts (tensor([44]), 'iraq') the next word with max prob: tensor([0.0451])
The negative loglikelihood (-log p(prison)): 5.318028926849365 or 5.318028926849365 or 5.318028926849365
('oil', 'production', 'next', 'week', 'ahead', 'deadline')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1921, 'deadline') has the max prob in order to be the next word. The prob is: 0.0006101133185438812
988800 The nn predicts (tensor([158]), 'election') the next word with max prob: tensor([0.0435])
The negative loglikelihood (-log p(deadline)): 7.4018659591674805 or 7.4018659591674805 or 7.4018659591674805
('killer', 'japan', 'leaves', 'haiku', 'clue', 'setting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2371, 'setting') has the max prob in order to be the next word. The prob is: 0.00013531326840166003
988900 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0316])
The negative loglikelihood (-log p(setting)): 8.907917976379395 or 8.907917976379395 or 8.907917976379395
('part', 'major', 'tournament', 'next', 'week', 'country')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35, 'country') has the max prob in order to be the next word. The prob is: 0.005641252733767033
989000 The nn predicts (tensor([105]), 'last') the next word with max prob: tensor([0.0135])
The negative loglikelihood (-log p(country)): 5.177649021148682 or 5.177649021148682 or 5.177649021148682
('attention', 'mubarak', 'fell', 'country', 'bright', 'optimism')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7964, 'optimism') has the max prob in order to be the next word. The prob is: 2.9410644856397994e-05
989100 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0196])
The negative loglikelihood (-log p(optimism)): 10.43415355682373 or 10.43415355682373 or 10.43415355682373
('süddeutsche', 'zeitung', 'demons', 'dispatches', 'us', 'diplomacy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2291, 'diplomacy') has the max prob in order to be the next word. The prob is: 0.00018814958457369357
989200 The nn predicts (tensor([365]), 'drone') the next word with max prob: tensor([0.0587])
The negative loglikelihood (-log p(diplomacy)): 8.578272819519043 or 8.578272819519043 or 8.578272819519043
('mps', 'trapped', 'inside', 'parliament', 'protesters', 'riot')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (738, 'riot') has the max prob in order to be the next word. The prob is: 0.004104213789105415
989300 The nn predicts (tensor([752]), 'cairo') the next word with max prob: tensor([0.0153])
The negative loglikelihood (-log p(riot)): 5.49574089050293 or 5.49574089050293 or 5.49574089050293
('sources', 'report', 'comes', 'country', 'new', 'leadership')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1302, 'leadership') has the max prob in order to be the next word. The prob is: 0.001559830503538251
989400 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0671])
The negative loglikelihood (-log p(leadership)): 6.4631781578063965 or 6.4631781578063965 or 6.4631781578063965
('principal', 'responsible', 'death', 'children', 'surrenders', 'topolice')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (64258, 'topolice') has the max prob in order to be the next word. The prob is: 1.272149347641971e-06
989500 The nn predicts (tensor([420]), 'rape') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(topolice)): 13.57480239868164 or 13.57480239868164 or 13.57480239868164
('arrest', 'school', 'principal', 'food', 'poisoning', 'case')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (177, 'case') has the max prob in order to be the next word. The prob is: 0.001248362474143505
989600 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0226])
The negative loglikelihood (-log p(case)): 6.685922622680664 or 6.685922622680664 or 6.685922622680664
('snowden', 'permitted', 'leave', 'airport', 'transit', 'zone')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (629, 'zone') has the max prob in order to be the next word. The prob is: 0.003925447352230549
989700 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0318])
The negative loglikelihood (-log p(zone)): 5.5402750968933105 or 5.5402750968933105 or 5.5402750968933105
('island', 'detention', 'centre', 'papua', 'new', 'guinea')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1583, 'guinea') has the max prob in order to be the next word. The prob is: 0.14105241000652313
989800 The nn predicts (tensor([1583]), 'guinea') the next word with max prob: tensor([0.1411])
The negative loglikelihood (-log p(guinea)): 1.9586237668991089 or 1.9586237668991089 or 1.9586237668991089
('fully', 'executing', 'nixon', 'strategy', 'greater', 'success')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2135, 'success') has the max prob in order to be the next word. The prob is: 9.95836453512311e-05
989900 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0107])
The negative loglikelihood (-log p(success)): 9.214512825012207 or 9.214512825012207 or 9.214512825012207
('new', 'picture', 'curiosity', 'rover', 'taken', 'mars')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1357, 'mars') has the max prob in order to be the next word. The prob is: 0.0005072539788670838
990000 The nn predicts (tensor([736]), 'place') the next word with max prob: tensor([0.0230])
The negative loglikelihood (-log p(mars)): 7.586498737335205 or 7.586498737335205 or 7.586498737335205
('watch', 'trailer', 'half', 'yellow', 'sun', 'starring')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19505, 'starring') has the max prob in order to be the next word. The prob is: 4.6616919462394435e-06
990100 The nn predicts (tensor([1009]), 'square') the next word with max prob: tensor([0.0116])
The negative loglikelihood (-log p(starring)): 12.276132583618164 or 12.276131629943848 or 12.276131629943848
('threatened', 'sue', 'claire', 'perry', 'mp', 'alleged')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (390, 'alleged') has the max prob in order to be the next word. The prob is: 0.00021577328152488917
990200 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0337])
The negative loglikelihood (-log p(alleged)): 8.441282272338867 or 8.441282272338867 or 8.441282272338867
('military', 'analyst', 'told', 'reuters', 'command', 'eroded')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17518, 'eroded') has the max prob in order to be the next word. The prob is: 1.4403751720237778e-06
990300 The nn predicts (tensor([18]), 'military') the next word with max prob: tensor([0.0289])
The negative loglikelihood (-log p(eroded)): 13.450607299804688 or 13.450607299804688 or 13.450607299804688
('xilai', 'corruption', 'case', 'trial', 'may', 'start')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (579, 'start') has the max prob in order to be the next word. The prob is: 0.0005265802028588951
990400 The nn predicts (tensor([128]), 'kill') the next word with max prob: tensor([0.0140])
The negative loglikelihood (-log p(start)): 7.549107074737549 or 7.549107074737549 or 7.549107074737549
('war', 'zone', 'key', 'elections', 'scheduled', 'less')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (862, 'less') has the max prob in order to be the next word. The prob is: 0.00023577891988679767
990500 The nn predicts (tensor([375]), 'month') the next word with max prob: tensor([0.0142])
The negative loglikelihood (-log p(less)): 8.352616310119629 or 8.352616310119629 or 8.352616310119629
('life', 'saving', 'drugs', 'act', 'control', 'group')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (62, 'group') has the max prob in order to be the next word. The prob is: 0.0007351802196353674
990600 The nn predicts (tensor([184]), 'drug') the next word with max prob: tensor([0.0188])
The negative loglikelihood (-log p(group)): 7.215394973754883 or 7.215394973754883 or 7.215394973754883
('snowden', 'returned', 'extradited', 'ambassador', 'says', 'ambassador')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (589, 'ambassador') has the max prob in order to be the next word. The prob is: 0.0002581189328338951
990700 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0596])
The negative loglikelihood (-log p(ambassador)): 8.262089729309082 or 8.262089729309082 or 8.262089729309082
('permafrost', 'could', 'trigger', 'catastrophic', 'climate', 'change')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (162, 'change') has the max prob in order to be the next word. The prob is: 0.31875133514404297
990800 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.3188])
The negative loglikelihood (-log p(change)): 1.1433440446853638 or 1.1433440446853638 or 1.1433440446853638
('phones', 'however', 'billion', 'access', 'toilets', 'latrines')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (27802, 'latrines') has the max prob in order to be the next word. The prob is: 1.9271410565124825e-06
990900 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0177])
The negative loglikelihood (-log p(latrines)): 13.159473419189453 or 13.159473419189453 or 13.159473419189453
('feet', 'rescued', 'mexico', 'national', 'migration', 'institute')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3241, 'institute') has the max prob in order to be the next word. The prob is: 7.289991481229663e-05
991000 The nn predicts (tensor([333]), 'system') the next word with max prob: tensor([0.0293])
The negative loglikelihood (-log p(institute)): 9.526423454284668 or 9.526423454284668 or 9.526423454284668
('localized', 'reports', 'competition', 'jobs', 'high', 'applications')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5642, 'applications') has the max prob in order to be the next word. The prob is: 4.9936621508095413e-05
991100 The nn predicts (tensor([915]), 'levels') the next word with max prob: tensor([0.0109])
The negative loglikelihood (-log p(applications)): 9.904755592346191 or 9.904755592346191 or 9.904755592346191
('axe', 'big', 'brother', 'road', 'cameras', 'town')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (242, 'town') has the max prob in order to be the next word. The prob is: 0.0007225348381325603
991200 The nn predicts (tensor([494]), 'train') the next word with max prob: tensor([0.0113])
The negative loglikelihood (-log p(town)): 7.232745170593262 or 7.2327446937561035 or 7.2327446937561035
('war', 'grim', 'milestone', 'dead', 'sign', 'talks')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (83, 'talks') has the max prob in order to be the next word. The prob is: 0.005340833682566881
991300 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0452])
The negative loglikelihood (-log p(talks)): 5.2323737144470215 or 5.2323737144470215 or 5.2323737144470215
('loan', 'company', 'finds', 'cofe', 'invests', 'company')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (353, 'company') has the max prob in order to be the next word. The prob is: 0.0025722237769514322
991400 The nn predicts (tensor([154]), 'billion') the next word with max prob: tensor([0.0538])
The negative loglikelihood (-log p(company)): 5.962984561920166 or 5.962984561920166 or 5.962984561920166
('even', 'hearing', 'name', 'kim', 'dot', 'com')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (25355, 'com') has the max prob in order to be the next word. The prob is: 3.4708664315985516e-06
991500 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0097])
The negative loglikelihood (-log p(com)): 12.57110595703125 or 12.57110595703125 or 12.57110595703125
('operator', 'makes', 'first', 'public', 'admission', 'severe')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1816, 'severe') has the max prob in order to be the next word. The prob is: 5.64533329452388e-05
991600 The nn predicts (tensor([177]), 'case') the next word with max prob: tensor([0.0110])
The negative loglikelihood (-log p(severe)): 9.782095909118652 or 9.782095909118652 or 9.782095909118652
('fragile', 'truce', 'el', 'salvador', 'gangs', 'despite')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (243, 'despite') has the max prob in order to be the next word. The prob is: 0.0007314838585443795
991700 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0282])
The negative loglikelihood (-log p(despite)): 7.22043514251709 or 7.220435619354248 or 7.220435619354248
('letter', 'chancellor', 'merkel', 'germany', 'surveillance', 'society')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1523, 'society') has the max prob in order to be the next word. The prob is: 7.540947262896225e-05
991800 The nn predicts (tensor([368]), 'policy') the next word with max prob: tensor([0.0252])
The negative loglikelihood (-log p(society)): 9.49257755279541 or 9.49257755279541 or 9.49257755279541
('big', 'operation', 'coastal', 'region', 'near', 'rome')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1853, 'rome') has the max prob in order to be the next word. The prob is: 3.8591973861912265e-05
991900 The nn predicts (tensor([242]), 'town') the next word with max prob: tensor([0.0796])
The negative loglikelihood (-log p(rome)): 10.162466049194336 or 10.162466049194336 or 10.162466049194336
('<s>', 'turkey', 'captured', 'bird', 'spy', 'israel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10, 'israel') has the max prob in order to be the next word. The prob is: 0.002135668182745576
992000 The nn predicts (tensor([365]), 'drone') the next word with max prob: tensor([0.0233])
The negative loglikelihood (-log p(israel)): 6.148975849151611 or 6.148975849151611 or 6.148975849151611
('mohamed', 'brahmi', 'killed', 'gun', 'used', 'assassinate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5306, 'assassinate') has the max prob in order to be the next word. The prob is: 8.128838089760393e-05
992100 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0338])
The negative loglikelihood (-log p(assassinate)): 9.41750717163086 or 9.41750717163086 or 9.41750717163086
('russian', 'traitors', 'place', 'russian', 'friends', 'give')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (516, 'give') has the max prob in order to be the next word. The prob is: 0.0014167786575853825
992200 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(give)): 6.5593695640563965 or 6.5593695640563965 or 6.5593695640563965
('concerned', 'detention', 'syrian', 'refugees', 'amid', 'sentiment')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5965, 'sentiment') has the max prob in order to be the next word. The prob is: 4.2477309762034565e-05
992300 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0988])
The negative loglikelihood (-log p(sentiment)): 10.066540718078613 or 10.066540718078613 or 10.066540718078613
('garcía', 'fraile', 'general', 'secretary', 'semaf', 'train')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (494, 'train') has the max prob in order to be the next word. The prob is: 0.0019359717844054103
992400 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0133])
The negative loglikelihood (-log p(train)): 6.247145652770996 or 6.247145652770996 or 6.247145652770996
('iran', 'described', 'online', 'censorship', 'futile', 'views')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2920, 'views') has the max prob in order to be the next word. The prob is: 0.00015421003627125174
992500 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0583])
The negative loglikelihood (-log p(views)): 8.77719497680664 or 8.77719497680664 or 8.77719497680664
('staff', 'entering', 'gaza', 'western', 'diplomat', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.0972759798169136
992600 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0973])
The negative loglikelihood (-log p(says)): 2.3302032947540283 or 2.3302032947540283 or 2.3302032947540283
('war', 'destabilizing', 'region', 'expert', 'syria', 'like')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (236, 'like') has the max prob in order to be the next word. The prob is: 7.061148062348366e-05
992700 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0736])
The negative loglikelihood (-log p(like)): 9.558318138122559 or 9.558318138122559 or 9.558318138122559
('<s>', 'twin', 'suicide', 'blasts', 'kill', 'parachinar')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (42754, 'parachinar') has the max prob in order to be the next word. The prob is: 5.123589517097571e-07
992800 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0989])
The negative loglikelihood (-log p(parachinar)): 14.484240531921387 or 14.484240531921387 or 14.484240531921387
('fisk', 'egypt', 'impoverished', 'crowds', 'gather', 'support')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (189, 'support') has the max prob in order to be the next word. The prob is: 0.0025447094812989235
992900 The nn predicts (tensor([619]), 'streets') the next word with max prob: tensor([0.0531])
The negative loglikelihood (-log p(support)): 5.973738670349121 or 5.973738670349121 or 5.973738670349121
('prosecution', 'perpetrators', 'crimes', 'humanity', 'unit', 'arab')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (271, 'arab') has the max prob in order to be the next word. The prob is: 0.00036241955240257084
993000 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(arab)): 7.922708034515381 or 7.922708034515381 or 7.922708034515381
('army', 'rebels', 'sudan', 'main', 'oil', 'state')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16, 'state') has the max prob in order to be the next word. The prob is: 0.001908356905914843
993100 The nn predicts (tensor([180]), 'gas') the next word with max prob: tensor([0.0371])
The negative loglikelihood (-log p(state)): 6.261512756347656 or 6.261512756347656 or 6.261512756347656
('clashes', 'least', 'mohamed', 'morsi', 'supporters', 'killed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13, 'killed') has the max prob in order to be the next word. The prob is: 0.0340021476149559
993200 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0340])
The negative loglikelihood (-log p(killed)): 3.3813316822052 or 3.3813316822052 or 3.3813316822052
('putin', 'crackdown', 'keep', 'people', 'winter', 'olympics')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (646, 'olympics') has the max prob in order to be the next word. The prob is: 0.000838712730910629
993300 The nn predicts (tensor([91]), 'protest') the next word with max prob: tensor([0.0196])
The negative loglikelihood (-log p(olympics)): 7.083642482757568 or 7.083642482757568 or 7.083642482757568
('insidious', 'corrective', 'rape', 'rape', 'gay', 'men')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (197, 'men') has the max prob in order to be the next word. The prob is: 0.008413510397076607
993400 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.1688])
The negative loglikelihood (-log p(men)): 4.777916431427002 or 4.777916431427002 or 4.777916431427002
('news', 'website', 'report', 'iaf', 'activity', 'baalbek')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (29731, 'baalbek') has the max prob in order to be the next word. The prob is: 1.283579081245989e-06
993500 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0281])
The negative loglikelihood (-log p(baalbek)): 13.565857887268066 or 13.565857887268066 or 13.565857887268066
('us', 'pens', 'letter', 'russia', 'promising', 'protections')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7965, 'protections') has the max prob in order to be the next word. The prob is: 9.862489605438896e-06
993600 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.1830])
The negative loglikelihood (-log p(protections)): 11.526771545410156 or 11.526771545410156 or 11.526771545410156
('calls', 'respect', 'protection', 'environment', 'end', 'exploitation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4849, 'exploitation') has the max prob in order to be the next word. The prob is: 3.883251338265836e-05
993700 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0285])
The negative loglikelihood (-log p(exploitation)): 10.15625286102295 or 10.15625286102295 or 10.15625286102295
('eight', 'kurdish', 'police', 'northern', 'iraq', 'targeting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1490, 'targeting') has the max prob in order to be the next word. The prob is: 0.00019706960301846266
993800 The nn predicts (tensor([150]), 'killing') the next word with max prob: tensor([0.0380])
The negative loglikelihood (-log p(targeting)): 8.531953811645508 or 8.531953811645508 or 8.531953811645508
('costs', 'delays', 'team', 'consultants', 'drafted', 'accountants')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (25766, 'accountants') has the max prob in order to be the next word. The prob is: 9.578511708241422e-07
993900 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0143])
The negative loglikelihood (-log p(accountants)): 13.858573913574219 or 13.858572959899902 or 13.858572959899902
('fake', 'israeli', 'passports', 'vancouver', 'international', 'airport')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (363, 'airport') has the max prob in order to be the next word. The prob is: 0.020347412675619125
994000 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0464])
The negative loglikelihood (-log p(airport)): 3.894801616668701 or 3.894801616668701 or 3.894801616668701
('russia', 'largest', 'search', 'engine', 'yandex', 'announced')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (770, 'announced') has the max prob in order to be the next word. The prob is: 0.0006524825002998114
994100 The nn predicts (tensor([40]), 'japan') the next word with max prob: tensor([0.0089])
The negative loglikelihood (-log p(announced)): 7.334726333618164 or 7.334726333618164 or 7.334726333618164
('eu', 'un', 'egypt', 'v', 'turkey', 'unrest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (940, 'unrest') has the max prob in order to be the next word. The prob is: 0.0005224340129643679
994200 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.0167])
The negative loglikelihood (-log p(unrest)): 7.55701208114624 or 7.55701208114624 or 7.55701208114624
('bbc', 'news', 'libyan', 'city', 'benghazi', 'hit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (171, 'hit') has the max prob in order to be the next word. The prob is: 0.0008815077599138021
994300 The nn predicts (tensor([363]), 'airport') the next word with max prob: tensor([0.0592])
The negative loglikelihood (-log p(hit)): 7.033876895904541 or 7.033876895904541 or 7.033876895904541
('lists', 'investigative', 'journalists', 'alongside', 'spies', 'terrorists')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (472, 'terrorists') has the max prob in order to be the next word. The prob is: 0.0008154521347023547
994400 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0221])
The negative loglikelihood (-log p(terrorists)): 7.111767768859863 or 7.111767768859863 or 7.111767768859863
('searching', 'indonesian', 'housemaid', 'accused', 'employer', 'casting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10763, 'casting') has the max prob in order to be the next word. The prob is: 5.6973990467668045e-06
994500 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0053])
The negative loglikelihood (-log p(casting)): 12.07550048828125 or 12.07550048828125 or 12.07550048828125
('uk', 'evangelical', 'alliance', 'exposed', 'online', 'campaign')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (330, 'campaign') has the max prob in order to be the next word. The prob is: 0.007215060759335756
994600 The nn predicts (tensor([386]), 'abuse') the next word with max prob: tensor([0.0301])
The negative loglikelihood (-log p(campaign)): 4.93158483505249 or 4.93158483505249 or 4.93158483505249
('tourist', 'resort', 'black', 'waves', 'crude', 'oil')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (66, 'oil') has the max prob in order to be the next word. The prob is: 0.006276527885347605
994700 The nn predicts (tensor([494]), 'train') the next word with max prob: tensor([0.0109])
The negative loglikelihood (-log p(oil)): 5.0709381103515625 or 5.0709381103515625 or 5.0709381103515625
('prepared', 'attend', 'peace', 'conference', 'geneva', 'representatives')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4236, 'representatives') has the max prob in order to be the next word. The prob is: 0.0001612364430911839
994800 The nn predicts (tensor([83]), 'talks') the next word with max prob: tensor([0.1783])
The negative loglikelihood (-log p(representatives)): 8.732638359069824 or 8.732638359069824 or 8.732638359069824
('saudi', 'prince', 'releases', 'report', 'claiming', 'fracking')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2970, 'fracking') has the max prob in order to be the next word. The prob is: 1.1170090147061273e-05
994900 The nn predicts (tensor([420]), 'rape') the next word with max prob: tensor([0.0090])
The negative loglikelihood (-log p(fracking)): 11.402271270751953 or 11.402271270751953 or 11.402271270751953
('women', 'rights', 'risk', 'peace', 'process', 'un')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (34, 'un') has the max prob in order to be the next word. The prob is: 0.006373999640345573
995000 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(un)): 5.055528163909912 or 5.055528163909912 or 5.055528163909912
('teenagers', 'face', 'life', 'prison', 'attempted', 'murder')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (269, 'murder') has the max prob in order to be the next word. The prob is: 0.0678364485502243
995100 The nn predicts (tensor([420]), 'rape') the next word with max prob: tensor([0.1102])
The negative loglikelihood (-log p(murder)): 2.6906557083129883 or 2.6906557083129883 or 2.6906557083129883
('officials', 'say', 'whistleblower', 'lying', 'journalist', 'brought')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1554, 'brought') has the max prob in order to be the next word. The prob is: 0.00020659378787968308
995200 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0263])
The negative loglikelihood (-log p(brought)): 8.484756469726562 or 8.484756469726562 or 8.484756469726562
('public', 'debate', 'report', 'published', 'last', 'week')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (211, 'week') has the max prob in order to be the next word. The prob is: 0.1983683854341507
995300 The nn predicts (tensor([211]), 'week') the next word with max prob: tensor([0.1984])
The negative loglikelihood (-log p(week)): 1.6176294088363647 or 1.6176294088363647 or 1.6176294088363647
('facility', 'defiance', 'army', 'warning', 'turned', 'back')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (115, 'back') has the max prob in order to be the next word. The prob is: 0.020783687010407448
995400 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0277])
The negative loglikelihood (-log p(back)): 3.873586893081665 or 3.873586893081665 or 3.873586893081665
('public', 'transport', 'france', 'spain', 'italy', 'recent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (594, 'recent') has the max prob in order to be the next word. The prob is: 9.790530020836741e-05
995500 The nn predicts (tensor([326]), 'biggest') the next word with max prob: tensor([0.0202])
The negative loglikelihood (-log p(recent)): 9.231510162353516 or 9.231510162353516 or 9.231510162353516
('interview', 'lifts', 'biography', 'jesus', 'sales', 'book')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1539, 'book') has the max prob in order to be the next word. The prob is: 0.0003484167391434312
995600 The nn predicts (tensor([37]), 'one') the next word with max prob: tensor([0.0086])
The negative loglikelihood (-log p(book)): 7.962111473083496 or 7.962111473083496 or 7.962111473083496
('president', 'robert', 'mugabe', 'closest', 'allies', 'accepted')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4270, 'accepted') has the max prob in order to be the next word. The prob is: 0.0002399279474047944
995700 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0380])
The negative loglikelihood (-log p(accepted)): 8.335171699523926 or 8.335171699523926 or 8.335171699523926
('oil', 'prices', 'unusually', 'high', 'tax', 'rate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (897, 'rate') has the max prob in order to be the next word. The prob is: 0.002801566617563367
995800 The nn predicts (tensor([1763]), 'sector') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(rate)): 5.8775763511657715 or 5.8775763511657715 or 5.8775763511657715
('next', 'government', 'much', 'stake', 'future', 'generations')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7047, 'generations') has the max prob in order to be the next word. The prob is: 5.015034548705444e-05
995900 The nn predicts (tensor([307]), 'energy') the next word with max prob: tensor([0.0293])
The negative loglikelihood (-log p(generations)): 9.900485038757324 or 9.900485038757324 or 9.900485038757324
('use', 'upcoming', 'copyright', 'infringement', 'trial', 'court')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (39, 'court') has the max prob in order to be the next word. The prob is: 0.0018396979430690408
996000 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(court)): 6.298153877258301 or 6.298153877258301 or 6.298153877258301
('court', 'indict', 'musharraf', 'benazir', 'bhutto', 'murder')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (269, 'murder') has the max prob in order to be the next word. The prob is: 0.15354275703430176
996100 The nn predicts (tensor([269]), 'murder') the next word with max prob: tensor([0.1535])
The negative loglikelihood (-log p(murder)): 1.8737761974334717 or 1.8737761974334717 or 1.8737761974334717
('muslim', 'brotherhood', 'overthrow', 'egypt', 'tunisia', 'opposition')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (138, 'opposition') has the max prob in order to be the next word. The prob is: 0.00702812522649765
996200 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0701])
The negative loglikelihood (-log p(opposition)): 4.9578351974487305 or 4.9578351974487305 or 4.9578351974487305
('statements', 'propaganda', 'laws', 'exempt', 'olympic', 'visitors')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3163, 'visitors') has the max prob in order to be the next word. The prob is: 0.0002599392319098115
996300 The nn predicts (tensor([529]), 'laws') the next word with max prob: tensor([0.0054])
The negative loglikelihood (-log p(visitors)): 8.2550630569458 or 8.2550630569458 or 8.2550630569458
('moment', 'sammy', 'yatim', 'shot', 'toronto', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.04507092759013176
996400 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0451])
The negative loglikelihood (-log p(police)): 3.099517822265625 or 3.099517822265625 or 3.099517822265625
('entrepreneur', 'reveals', 'route', 'nicaraguan', 'canal', 'hundreds')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (255, 'hundreds') has the max prob in order to be the next word. The prob is: 0.000410211127018556
996500 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0131])
The negative loglikelihood (-log p(hundreds)): 7.7988386154174805 or 7.7988386154174805 or 7.7988386154174805
('iceberg', 'calving', 'accounts', 'roughly', 'half', 'mass')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (289, 'mass') has the max prob in order to be the next word. The prob is: 0.0002302749635418877
996600 The nn predicts (tensor([74]), 'million') the next word with max prob: tensor([0.1316])
The negative loglikelihood (-log p(mass)): 8.376236915588379 or 8.376236915588379 or 8.376236915588379
('spies', 'opposition', 'emails', 'shows', 'national', 'assembly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1721, 'assembly') has the max prob in order to be the next word. The prob is: 0.00029791361885145307
996700 The nn predicts (tensor([57]), 'security') the next word with max prob: tensor([0.0855])
The negative loglikelihood (-log p(assembly)): 8.118706703186035 or 8.118706703186035 or 8.118706703186035
('robert', 'mugabe', 'vowed', 'step', 'defeated', 'polls')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1487, 'polls') has the max prob in order to be the next word. The prob is: 0.002095101634040475
996800 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0865])
The negative loglikelihood (-log p(polls)): 6.168153285980225 or 6.168153285980225 or 6.168153285980225
('polls', 'elect', 'president', 'parliament', 'members', 'local')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (466, 'local') has the max prob in order to be the next word. The prob is: 0.0014491442125290632
996900 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0151])
The negative loglikelihood (-log p(local)): 6.536782264709473 or 6.536782264709473 or 6.536782264709473
('news', 'spain', 'train', 'crash', 'driver', 'francisco')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6539, 'francisco') has the max prob in order to be the next word. The prob is: 0.00012257456546649337
997000 The nn predicts (tensor([253]), 'crash') the next word with max prob: tensor([0.0569])
The negative loglikelihood (-log p(francisco)): 9.006791114807129 or 9.006791114807129 or 9.006791114807129
('numerous', 'syrian', 'rebel', 'groups', 'al', 'nusrah')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (24032, 'nusrah') has the max prob in order to be the next word. The prob is: 8.442812600151228e-07
997100 The nn predicts (tensor([659]), 'qaeda') the next word with max prob: tensor([0.2459])
The negative loglikelihood (-log p(nusrah)): 13.984780311584473 or 13.984780311584473 or 13.984780311584473
('armed', 'taking', 'advantage', 'political', 'chaos', 'plunder')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13121, 'plunder') has the max prob in order to be the next word. The prob is: 3.868404746754095e-06
997200 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0153])
The negative loglikelihood (-log p(plunder)): 12.462668418884277 or 12.462668418884277 or 12.462668418884277
('highly', 'leveraged', 'acquisition', 'interests', 'could', 'damaged')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2210, 'damaged') has the max prob in order to be the next word. The prob is: 0.00016986430273391306
997300 The nn predicts (tensor([224]), 'stop') the next word with max prob: tensor([0.0116])
The negative loglikelihood (-log p(damaged)): 8.680510520935059 or 8.680510520935059 or 8.680510520935059
('train', 'disaster', 'que', 'company', 'able', 'afford')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4863, 'afford') has the max prob in order to be the next word. The prob is: 0.00010284002200933173
997400 The nn predicts (tensor([151]), 'help') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(afford)): 9.18233585357666 or 9.18233585357666 or 9.18233585357666
('jumped', 'death', 'gun', 'battle', 'police', 'china')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1, 'china') has the max prob in order to be the next word. The prob is: 0.0008699483587406576
997500 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0359])
The negative loglikelihood (-log p(china)): 7.04707670211792 or 7.04707670211792 or 7.04707670211792
('leaders', 'trading', 'fishing', 'stories', 'belarus', 'leader')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (52, 'leader') has the max prob in order to be the next word. The prob is: 0.004546341951936483
997600 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0122])
The negative loglikelihood (-log p(leader)): 5.393432140350342 or 5.393432140350342 or 5.393432140350342
('accepted', 'fantasies', 'police', 'sture', 'bergwall', 'served')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4554, 'served') has the max prob in order to be the next word. The prob is: 0.00015993876149877906
997700 The nn predicts (tensor([269]), 'murder') the next word with max prob: tensor([0.0644])
The negative loglikelihood (-log p(served)): 8.74071979522705 or 8.74071979522705 or 8.74071979522705
('total', 'syrian', 'population', 'displaced', 'conflict', 'since')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (153, 'since') has the max prob in order to be the next word. The prob is: 0.007166203577071428
997800 The nn predicts (tensor([34]), 'un') the next word with max prob: tensor([0.0350])
The negative loglikelihood (-log p(since)): 4.938379287719727 or 4.938379287719727 or 4.938379287719727
('un', 'investigators', 'visit', 'three', 'sites', 'chemical')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (313, 'chemical') has the max prob in order to be the next word. The prob is: 0.00019442405027803034
997900 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0327])
The negative loglikelihood (-log p(chemical)): 8.545469284057617 or 8.545469284057617 or 8.545469284057617
('also', 'used', 'medical', 'test', 'subjects', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.0006314859492704272
998000 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0078])
The negative loglikelihood (-log p(says)): 7.367434978485107 or 7.367434978485107 or 7.367434978485107
('brown', 'eyes', 'visible', 'fights', 'corrupt', 'politicians')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (908, 'politicians') has the max prob in order to be the next word. The prob is: 0.001020571100525558
998100 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0218])
The negative loglikelihood (-log p(politicians)): 6.887392997741699 or 6.887392997741699 or 6.887392997741699
('open', 'way', 'authorities', 'create', 'one', 'latin')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1471, 'latin') has the max prob in order to be the next word. The prob is: 0.0001542107929708436
998200 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0188])
The negative loglikelihood (-log p(latin)): 8.777190208435059 or 8.777190208435059 or 8.777190208435059
('problems', 'remain', 'many', 'questioning', 'old', 'elite')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1917, 'elite') has the max prob in order to be the next word. The prob is: 6.775334622943774e-05
998300 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.0746])
The negative loglikelihood (-log p(elite)): 9.599637031555176 or 9.599637031555176 or 9.599637031555176
('bill', 'would', 'give', 'new', 'zealand', 'government')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11, 'government') has the max prob in order to be the next word. The prob is: 0.021090615540742874
998400 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0211])
The negative loglikelihood (-log p(government)): 3.858927011489868 or 3.858927011489868 or 3.858927011489868
('years', 'tells', 'brutal', 'daily', 'cell', 'searches')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4982, 'searches') has the max prob in order to be the next word. The prob is: 3.208962516509928e-05
998500 The nn predicts (tensor([269]), 'murder') the next word with max prob: tensor([0.0074])
The negative loglikelihood (-log p(searches)): 10.346978187561035 or 10.346978187561035 or 10.346978187561035
('vowed', 'fight', 'expulsion', 'said', 'politically', 'motivated')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5748, 'motivated') has the max prob in order to be the next word. The prob is: 0.00024152903642971069
998600 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0154])
The negative loglikelihood (-log p(motivated)): 8.328520774841309 or 8.328520774841309 or 8.328520774841309
('drug', 'legalisation', 'uruguay', 'experiment', 'another', 'blow')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1368, 'blow') has the max prob in order to be the next word. The prob is: 0.00022451872064266354
998700 The nn predicts (tensor([101]), 'time') the next word with max prob: tensor([0.0211])
The negative loglikelihood (-log p(blow)): 8.401551246643066 or 8.401551246643066 or 8.401551246643066
('age', 'means', 'pregnancy', 'childbirth', 'kill', 'girls')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (648, 'girls') has the max prob in order to be the next word. The prob is: 0.006856984458863735
998800 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0570])
The negative loglikelihood (-log p(girls)): 4.982487678527832 or 4.982487678527832 or 4.982487678527832
('country', 'struggles', 'make', 'planes', 'fly', 'schedule')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6477, 'schedule') has the max prob in order to be the next word. The prob is: 3.234258110751398e-05
998900 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(schedule)): 10.339125633239746 or 10.339125633239746 or 10.339125633239746
('first', 'meeting', 'us', 'secretary', 'state', 'john')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (685, 'john') has the max prob in order to be the next word. The prob is: 0.0010799348820000887
999000 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0207])
The negative loglikelihood (-log p(john)): 6.830854415893555 or 6.830854415893555 or 6.830854415893555
('second', 'year', 'first', 'increase', 'since', 'least')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (84, 'least') has the max prob in order to be the next word. The prob is: 0.00023579895787406713
999100 The nn predicts (tensor([153]), 'since') the next word with max prob: tensor([0.0801])
The negative loglikelihood (-log p(least)): 8.352531433105469 or 8.352531433105469 or 8.352531433105469
('said', 'bringing', 'end', 'drone', 'strikes', 'would')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (104, 'would') has the max prob in order to be the next word. The prob is: 0.0019003920024260879
999200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0586])
The negative loglikelihood (-log p(would)): 6.265695095062256 or 6.265695095062256 or 6.265695095062256
('interior', 'ministry', 'said', 'begun', 'investigation', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.011790284886956215
999300 The nn predicts (tensor([390]), 'alleged') the next word with max prob: tensor([0.0171])
The negative loglikelihood (-log p(police)): 4.440479278564453 or 4.440479278564453 or 4.440479278564453
('arrest', 'ban', 'public', 'office', 'referred', 'another')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (270, 'another') has the max prob in order to be the next word. The prob is: 0.0007150911842472851
999400 The nn predicts (tensor([251]), 'gay') the next word with max prob: tensor([0.0228])
The negative loglikelihood (-log p(another)): 7.243100643157959 or 7.243100643157959 or 7.243100643157959
('department', 'thursday', 'ordered', 'temporary', 'closures', 'key')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (455, 'key') has the max prob in order to be the next word. The prob is: 0.00028611120069399476
999500 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0111])
The negative loglikelihood (-log p(key)): 8.159130096435547 or 8.159130096435547 or 8.159130096435547
('peru', 'culture', 'ministry', 'blocks', 'expansion', 'camisea')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (64393, 'camisea') has the max prob in order to be the next word. The prob is: 2.6042459921882255e-06
999600 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0184])
The negative loglikelihood (-log p(camisea)): 12.858366966247559 or 12.858366966247559 or 12.858366966247559
('divisions', 'residents', 'northeast', 'calling', 'secession', 'following')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (398, 'following') has the max prob in order to be the next word. The prob is: 0.0002950320194941014
999700 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0127])
The negative loglikelihood (-log p(following)): 8.128426551818848 or 8.128426551818848 or 8.128426551818848
('beitz', 'leading', 'german', 'industrialist', 'credited', 'saving')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3217, 'saving') has the max prob in order to be the next word. The prob is: 7.271102367667481e-05
999800 The nn predicts (tensor([274]), 'tax') the next word with max prob: tensor([0.0183])
The negative loglikelihood (-log p(saving)): 9.529017448425293 or 9.529017448425293 or 9.529017448425293
('invoking', 'family', 'god', 'patriotism', 'moving', 'tones')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19437, 'tones') has the max prob in order to be the next word. The prob is: 2.488254494892317e-06
999900 The nn predicts (tensor([115]), 'back') the next word with max prob: tensor([0.0292])
The negative loglikelihood (-log p(tones)): 12.903928756713867 or 12.903928756713867 or 12.903928756713867
('desecrating', 'aboriginal', 'site', 'australia', 'northern', 'territory')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1032, 'territory') has the max prob in order to be the next word. The prob is: 0.0012418624246492982
1000000 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0458])
The negative loglikelihood (-log p(territory)): 6.691143035888672 or 6.691143035888672 or 6.691143035888672
('reports', 'find', 'number', 'recent', 'studies', 'shown')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3201, 'shown') has the max prob in order to be the next word. The prob is: 8.069390605669469e-05
1000100 The nn predicts (tensor([260]), 'shows') the next word with max prob: tensor([0.0186])
The negative loglikelihood (-log p(shown)): 9.424847602844238 or 9.424847602844238 or 9.424847602844238
('away', 'little', 'equipment', 'braving', 'sandstorms', 'river')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1400, 'river') has the max prob in order to be the next word. The prob is: 0.0008037957013584673
1000200 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(river)): 7.126165390014648 or 7.126165390014648 or 7.126165390014648
('us', 'issues', 'worldwide', 'travel', 'alert', 'al')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (213, 'al') has the max prob in order to be the next word. The prob is: 0.0005111314239911735
1000300 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0436])
The negative loglikelihood (-log p(al)): 7.578883647918701 or 7.578883647918701 or 7.578883647918701
('administration', 'perceive', 'nazi', 'germany', 'positive', 'light')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1158, 'light') has the max prob in order to be the next word. The prob is: 0.0005571970832534134
1000400 The nn predicts (tensor([202]), 'threat') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(light)): 7.492591381072998 or 7.492591381072998 or 7.492591381072998
('mummy', 'inside', 'sarcophagus', 'complete', 'hieroglyphic', 'adornments')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44875, 'adornments') has the max prob in order to be the next word. The prob is: 2.7218370632908773e-06
1000500 The nn predicts (tensor([383]), 'site') the next word with max prob: tensor([0.0203])
The negative loglikelihood (-log p(adornments)): 12.814203262329102 or 12.814203262329102 or 12.814203262329102
('cairo', 'orphanges', 'dressed', 'dead', 'martyrs', 'attire')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17087, 'attire') has the max prob in order to be the next word. The prob is: 5.799292466690531e-06
1000600 The nn predicts (tensor([56]), 'man') the next word with max prob: tensor([0.0204])
The negative loglikelihood (-log p(attire)): 12.057774543762207 or 12.057774543762207 or 12.057774543762207
('made', 'hasan', 'rouhani', 'misquoted', 'comments', 'sparked')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2344, 'sparked') has the max prob in order to be the next word. The prob is: 0.00020836478506680578
1000700 The nn predicts (tensor([1214]), 'morsi') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(sparked)): 8.47622013092041 or 8.47622013092041 or 8.47622013092041
('come', 'home', 'fukushima', 'district', 'first', 'time')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (101, 'time') has the max prob in order to be the next word. The prob is: 0.29203271865844727
1000800 The nn predicts (tensor([101]), 'time') the next word with max prob: tensor([0.2920])
The negative loglikelihood (-log p(time)): 1.2308894395828247 or 1.2308894395828247 or 1.2308894395828247
('virus', 'two', 'saudi', 'health', 'workers', 'contact')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2339, 'contact') has the max prob in order to be the next word. The prob is: 7.774685946060345e-05
1000900 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0479])
The negative loglikelihood (-log p(contact)): 9.462052345275879 or 9.462052345275879 or 9.462052345275879
('hassan', 'rouhani', 'officially', 'takes', 'iran', 'president')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12, 'president') has the max prob in order to be the next word. The prob is: 0.009008431807160378
1001000 The nn predicts (tensor([32]), 'nuclear') the next word with max prob: tensor([0.0655])
The negative loglikelihood (-log p(president)): 4.709594249725342 or 4.709594249725342 or 4.709594249725342
('understand', 'spooked', 'group', 'baboons', 'monkeys', 'huddled')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (21450, 'huddled') has the max prob in order to be the next word. The prob is: 9.420435162610374e-06
1001100 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0120])
The negative loglikelihood (-log p(huddled)): 11.57262897491455 or 11.57262897491455 or 11.57262897491455
('angela', 'merkel', 'government', 'faces', 'mounting', 'criticism')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1195, 'criticism') has the max prob in order to be the next word. The prob is: 0.00296920258551836
1001200 The nn predicts (tensor([411]), 'corruption') the next word with max prob: tensor([0.0398])
The negative loglikelihood (-log p(criticism)): 5.819461822509766 or 5.819461822509766 or 5.819461822509766
('uk', 'gov', 'approves', 'autonomous', 'cars', 'public')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (186, 'public') has the max prob in order to be the next word. The prob is: 0.00147869938518852
1001300 The nn predicts (tensor([353]), 'company') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(public)): 6.516592502593994 or 6.516592502593994 or 6.516592502593994
('vodafone', 'cable', 'global', 'crossing', 'level', 'viatel')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44877, 'viatel') has the max prob in order to be the next word. The prob is: 6.145011752778373e-07
1001400 The nn predicts (tensor([153]), 'since') the next word with max prob: tensor([0.0206])
The negative loglikelihood (-log p(viatel)): 14.302454948425293 or 14.302454948425293 or 14.302454948425293
('without', 'food', 'water', 'hundreds', 'men', 'died')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (425, 'died') has the max prob in order to be the next word. The prob is: 0.012164284475147724
1001500 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0346])
The negative loglikelihood (-log p(died)): 4.4092512130737305 or 4.4092512130737305 or 4.4092512130737305
('court', 'decide', 'sterilisation', 'man', 'learning', 'difficulties')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9017, 'difficulties') has the max prob in order to be the next word. The prob is: 1.0116727025888395e-05
1001600 The nn predicts (tensor([201]), 'get') the next word with max prob: tensor([0.0125])
The negative loglikelihood (-log p(difficulties)): 11.501320838928223 or 11.501320838928223 or 11.501320838928223
('news', 'thursday', 'received', 'treatment', 'stress', 'disorder')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5992, 'disorder') has the max prob in order to be the next word. The prob is: 6.059592851670459e-05
1001700 The nn predicts (tensor([287]), 'health') the next word with max prob: tensor([0.0135])
The negative loglikelihood (-log p(disorder)): 9.711282730102539 or 9.711282730102539 or 9.711282730102539
('told', 'ignore', 'alternative', 'methods', 'energy', 'generation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2099, 'generation') has the max prob in order to be the next word. The prob is: 0.00014605984324589372
1001800 The nn predicts (tensor([695]), 'industry') the next word with max prob: tensor([0.0279])
The negative loglikelihood (-log p(generation)): 8.831494331359863 or 8.831494331359863 or 8.831494331359863
('scandal', 'close', 'home', 'abc', 'news', 'australian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (144, 'australian') has the max prob in order to be the next word. The prob is: 0.00861778762191534
1001900 The nn predicts (tensor([304]), 'agency') the next word with max prob: tensor([0.0370])
The negative loglikelihood (-log p(australian)): 4.753926753997803 or 4.753926753997803 or 4.753926753997803
('pardoning', 'king', 'says', 'know', 'seriousness', 'spanish')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (562, 'spanish') has the max prob in order to be the next word. The prob is: 0.0008150122594088316
1002000 The nn predicts (tensor([202]), 'threat') the next word with max prob: tensor([0.0092])
The negative loglikelihood (-log p(spanish)): 7.112307548522949 or 7.112307548522949 or 7.112307548522949
('shore', 'sendai', 'japan', 'depth', 'km', 'tsunami')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1241, 'tsunami') has the max prob in order to be the next word. The prob is: 0.001746786991134286
1002100 The nn predicts (tensor([40]), 'japan') the next word with max prob: tensor([0.0505])
The negative loglikelihood (-log p(tsunami)): 6.349977016448975 or 6.349977016448975 or 6.349977016448975
('ordered', 'militarization', 'country', 'main', 'prison', 'saturday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (493, 'saturday') has the max prob in order to be the next word. The prob is: 0.0057899923995137215
1002200 The nn predicts (tensor([79]), 'officials') the next word with max prob: tensor([0.0177])
The negative loglikelihood (-log p(saturday)): 5.1516242027282715 or 5.1516242027282715 or 5.1516242027282715
('slamming', 'government', 'dysfunctional', 'unable', 'end', 'wave')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1117, 'wave') has the max prob in order to be the next word. The prob is: 0.000453367450973019
1002300 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0678])
The negative loglikelihood (-log p(wave)): 7.698807716369629 or 7.698807716369629 or 7.698807716369629
('ravaged', 'areas', 'around', 'afghanistan', 'capital', 'killing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (150, 'killing') has the max prob in order to be the next word. The prob is: 0.01793934404850006
1002400 The nn predicts (tensor([150]), 'killing') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(killing)): 4.020759105682373 or 4.020759105682373 or 4.020759105682373
('cannabis', 'law', 'first', 'country', 'set', 'legalise')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6529, 'legalise') has the max prob in order to be the next word. The prob is: 7.17071452527307e-05
1002500 The nn predicts (tensor([137]), 'end') the next word with max prob: tensor([0.0341])
The negative loglikelihood (-log p(legalise)): 9.542920112609863 or 9.542920112609863 or 9.542920112609863
('bank', 'settlement', 'subsidies', 'cabinet', 'approves', 'funding')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1286, 'funding') has the max prob in order to be the next word. The prob is: 0.001303992816247046
1002600 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0282])
The negative loglikelihood (-log p(funding)): 6.642324447631836 or 6.642324447631836 or 6.642324447631836
('family', 'near', 'damascus', 'five', 'members', 'one')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (37, 'one') has the max prob in order to be the next word. The prob is: 0.0023161200806498528
1002700 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0449])
The negative loglikelihood (-log p(one)): 6.067862033843994 or 6.067862033843994 or 6.067862033843994
('dept', 'overseas', 'posts', 'remain', 'closed', 'response')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (572, 'response') has the max prob in order to be the next word. The prob is: 0.0004461746721062809
1002800 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0330])
The negative loglikelihood (-log p(response)): 7.714799880981445 or 7.714799880981445 or 7.714799880981445
('worldwide', 'travel', 'alert', 'closes', 'facilities', 'across')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (266, 'across') has the max prob in order to be the next word. The prob is: 0.0015577728627249599
1002900 The nn predicts (tensor([32]), 'nuclear') the next word with max prob: tensor([0.0093])
The negative loglikelihood (-log p(across)): 6.464498043060303 or 6.464498043060303 or 6.464498043060303
('temple', 'indonesia', 'one', 'device', 'explodes', 'two')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (31, 'two') has the max prob in order to be the next word. The prob is: 0.005543471314013004
1003000 The nn predicts (tensor([37]), 'one') the next word with max prob: tensor([0.0098])
The negative loglikelihood (-log p(two)): 5.195134162902832 or 5.195134162902832 or 5.195134162902832
('bank', 'told', 'dozens', 'foreign', 'missions', 'london')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (207, 'london') has the max prob in order to be the next word. The prob is: 0.0007588511798530817
1003100 The nn predicts (tensor([122]), 'afghanistan') the next word with max prob: tensor([0.0312])
The negative loglikelihood (-log p(london)): 7.183704853057861 or 7.183704853057861 or 7.183704853057861
('<s>', '<s>', 'italian', 'robbers', 'bulldoze', 'way')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (302, 'way') has the max prob in order to be the next word. The prob is: 0.0003971936530433595
1003200 The nn predicts (tensor([145]), 'mexico') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(way)): 7.8310866355896 or 7.8310866355896 or 7.8310866355896
('culture', 'german', 'labor', 'unions', 'say', 'amazon')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1332, 'amazon') has the max prob in order to be the next word. The prob is: 0.00017455419583711773
1003300 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0100])
The negative loglikelihood (-log p(amazon)): 8.653275489807129 or 8.653275489807129 or 8.653275489807129
('rebels', 'make', 'new', 'push', 'assad', 'alawite')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9417, 'alawite') has the max prob in order to be the next word. The prob is: 3.389116682228632e-05
1003400 The nn predicts (tensor([273]), 'assad') the next word with max prob: tensor([0.0865])
The negative loglikelihood (-log p(alawite)): 10.292356491088867 or 10.292356491088867 or 10.292356491088867
('undeclared', 'war', 'pakistan', 'fresh', 'evidence', 'cia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (667, 'cia') has the max prob in order to be the next word. The prob is: 0.0008931579068303108
1003500 The nn predicts (tensor([125]), 'claims') the next word with max prob: tensor([0.0238])
The negative loglikelihood (-log p(cia)): 7.020747184753418 or 7.020747184753418 or 7.020747184753418
('costa', 'rica', 'away', 'caged', 'animals', 'zoos')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10201, 'zoos') has the max prob in order to be the next word. The prob is: 1.2949690244568046e-05
1003600 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0070])
The negative loglikelihood (-log p(zoos)): 11.254438400268555 or 11.254438400268555 or 11.254438400268555
('ireland', 'heaviest', 'amount', 'precious', 'metal', 'ever')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (509, 'ever') has the max prob in order to be the next word. The prob is: 0.00036783661926165223
1003700 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0094])
The negative loglikelihood (-log p(ever)): 7.907871723175049 or 7.907871723175049 or 7.907871723175049
('secret', 'talks', 'representatives', 'afghan', 'president', 'hamid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3997, 'hamid') has the max prob in order to be the next word. The prob is: 0.0015076935524120927
1003800 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0604])
The negative loglikelihood (-log p(hamid)): 6.497174263000488 or 6.497174263000488 or 6.497174263000488
('woman', 'could', 'wake', 'blood', 'semen', 'stains')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (21185, 'stains') has the max prob in order to be the next word. The prob is: 3.2834320791153004e-06
1003900 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0084])
The negative loglikelihood (-log p(stains)): 12.62662124633789 or 12.62662124633789 or 12.62662124633789
('menagh', 'air', 'base', 'hours', 'suicide', 'bomber')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (654, 'bomber') has the max prob in order to be the next word. The prob is: 0.0032431769650429487
1004000 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0752])
The negative loglikelihood (-log p(bomber)): 5.731202125549316 or 5.731201648712158 or 5.731201648712158
('military', 'ambulance', 'hospital', 'calls', 'army', 'come')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (613, 'come') has the max prob in order to be the next word. The prob is: 7.536120392614976e-05
1004100 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.0270])
The negative loglikelihood (-log p(come)): 9.493218421936035 or 9.493218421936035 or 9.493218421936035
('pirates', 'form', 'church', 'battle', 'copyright', 'law')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (102, 'law') has the max prob in order to be the next word. The prob is: 0.08946636319160461
1004200 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0895])
The negative loglikelihood (-log p(law)): 2.4138925075531006 or 2.4138925075531006 or 2.4138925075531006
('racist', 'roma', 'killings', 'court', 'hungary', 'finds')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (446, 'finds') has the max prob in order to be the next word. The prob is: 7.395097782136872e-05
1004300 The nn predicts (tensor([102]), 'law') the next word with max prob: tensor([0.0280])
The negative loglikelihood (-log p(finds)): 9.512107849121094 or 9.512107849121094 or 9.512107849121094
('edging', 'closer', 'scientists', 'philippines', 'weeks', 'submitting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14810, 'submitting') has the max prob in order to be the next word. The prob is: 6.439471690100618e-06
1004400 The nn predicts (tensor([608]), 'ago') the next word with max prob: tensor([0.0640])
The negative loglikelihood (-log p(submitting)): 11.95306396484375 or 11.95306396484375 or 11.95306396484375
('jesuit', 'priest', 'missing', 'syria', 'apparently', 'kidnapped')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (872, 'kidnapped') has the max prob in order to be the next word. The prob is: 0.0009678883361630142
1004500 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0467])
The negative loglikelihood (-log p(kidnapped)): 6.940393924713135 or 6.940393924713135 or 6.940393924713135
('silicon', 'valley', 'startups', 'founded', 'indian', 'immigrant')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4774, 'immigrant') has the max prob in order to be the next word. The prob is: 2.7184602004126646e-05
1004600 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0752])
The negative loglikelihood (-log p(immigrant)): 10.512860298156738 or 10.512860298156738 or 10.512860298156738
('feed', 'billion', 'skip', 'meat', 'milk', 'eggs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4524, 'eggs') has the max prob in order to be the next word. The prob is: 4.702372461906634e-05
1004700 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0185])
The negative loglikelihood (-log p(eggs)): 9.964858055114746 or 9.964858055114746 or 9.964858055114746
('infosys', 'employees', 'us', 'south', 'asian', 'descent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6899, 'descent') has the max prob in order to be the next word. The prob is: 2.1838952307007276e-05
1004800 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0426])
The negative loglikelihood (-log p(descent)): 10.731815338134766 or 10.731815338134766 or 10.731815338134766
('turkey', 'ankara', 'softening', 'position', 'increasingly', 'autonomous')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6165, 'autonomous') has the max prob in order to be the next word. The prob is: 6.525288881675806e-06
1004900 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0309])
The negative loglikelihood (-log p(autonomous)): 11.939825057983398 or 11.939825057983398 or 11.939825057983398
('wearing', 'mankinis', 'sponsored', 'walk', 'stones', 'accuses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (708, 'accuses') has the max prob in order to be the next word. The prob is: 4.501150397118181e-05
1005000 The nn predicts (tensor([75]), 'women') the next word with max prob: tensor([0.0182])
The negative loglikelihood (-log p(accuses)): 10.00859260559082 or 10.00859260559082 or 10.00859260559082
('chiefs', 'discussing', 'imminent', 'major', 'terror', 'attack')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15, 'attack') has the max prob in order to be the next word. The prob is: 0.26905369758605957
1005100 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.2691])
The negative loglikelihood (-log p(attack)): 1.3128442764282227 or 1.3128442764282227 or 1.3128442764282227
('manager', 'warns', 'dangers', 'fatbergs', 'biggest', 'lump')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18423, 'lump') has the max prob in order to be the next word. The prob is: 3.7505158161366126e-06
1005200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0371])
The negative loglikelihood (-log p(lump)): 12.493617057800293 or 12.493617057800293 or 12.493617057800293
('jewish', 'character', 'town', 'banning', 'christmas', 'trees')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3250, 'trees') has the max prob in order to be the next word. The prob is: 0.00015052437083795667
1005300 The nn predicts (tensor([121]), 'day') the next word with max prob: tensor([0.0410])
The negative loglikelihood (-log p(trees)): 8.801385879516602 or 8.801385879516602 or 8.801385879516602
('pirate', 'church', 'eyes', 'registration', 'battle', 'copyright')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2337, 'copyright') has the max prob in order to be the next word. The prob is: 9.46738146012649e-05
1005400 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0214])
The negative loglikelihood (-log p(copyright)): 9.2650728225708 or 9.2650728225708 or 9.2650728225708
('senator', 'john', 'mccain', 'warned', 'tuesday', 'visit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (303, 'visit') has the max prob in order to be the next word. The prob is: 0.0014675998827442527
1005500 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0329])
The negative loglikelihood (-log p(visit)): 6.524127006530762 or 6.524127006530762 or 6.524127006530762
('rebels', 'seizure', 'air', 'base', 'sign', 'continue')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (626, 'continue') has the max prob in order to be the next word. The prob is: 0.00031801118166185915
1005600 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0400])
The negative loglikelihood (-log p(continue)): 8.053423881530762 or 8.053423881530762 or 8.053423881530762
('chinese', 'cybertheft', 'devastated', 'american', 'superconductor', 'business')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (580, 'business') has the max prob in order to be the next word. The prob is: 0.0011843570973724127
1005700 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0361])
The negative loglikelihood (-log p(business)): 6.738554954528809 or 6.738554954528809 or 6.738554954528809
('government', 'plan', 'remove', 'tens', 'thousands', 'bedouins')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12473, 'bedouins') has the max prob in order to be the next word. The prob is: 2.4403148017881904e-06
1005800 The nn predicts (tensor([120]), 'thousands') the next word with max prob: tensor([0.0953])
The negative loglikelihood (-log p(bedouins)): 12.923383712768555 or 12.923383712768555 or 12.923383712768555
('uk', 'embassy', 'staff', 'pulled', 'firing', 'line')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (935, 'line') has the max prob in order to be the next word. The prob is: 0.000919966259971261
1005900 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0133])
The negative loglikelihood (-log p(line)): 6.99117374420166 or 6.99117374420166 or 6.99117374420166
('security', 'forces', 'foiled', 'plot', 'al', 'qaeda')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (659, 'qaeda') has the max prob in order to be the next word. The prob is: 0.8136120438575745
1006000 The nn predicts (tensor([659]), 'qaeda') the next word with max prob: tensor([0.8136])
The negative loglikelihood (-log p(qaeda)): 0.20627163350582123 or 0.20627163350582123 or 0.20627163350582123
('demanded', 'countries', 'imposed', 'sanctions', 'crippling', 'economy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (285, 'economy') has the max prob in order to be the next word. The prob is: 0.002085629152134061
1006100 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(economy)): 6.172684669494629 or 6.172684669494629 or 6.172684669494629
('gases', 'putting', 'canadian', 'waters', 'risk', 'says')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2, 'says') has the max prob in order to be the next word. The prob is: 0.007607560604810715
1006200 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0098])
The negative loglikelihood (-log p(says)): 4.878612518310547 or 4.878612518310547 or 4.878612518310547
('wednesday', 'describes', 'threat', 'posed', 'made', 'loosely')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (19954, 'loosely') has the max prob in order to be the next word. The prob is: 5.049264927947661e-06
1006300 The nn predicts (tensor([32]), 'nuclear') the next word with max prob: tensor([0.0212])
The negative loglikelihood (-log p(loosely)): 12.196268081665039 or 12.196268081665039 or 12.196268081665039
('qaeda', 'plot', 'revealed', 'yemen', 'says', 'stopped')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1844, 'stopped') has the max prob in order to be the next word. The prob is: 0.00014759026817046106
1006400 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0250])
The negative loglikelihood (-log p(stopped)): 8.821070671081543 or 8.821070671081543 or 8.821070671081543
('teens', 'playing', 'soccer', 'among', 'killed', 'bomb')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (131, 'bomb') has the max prob in order to be the next word. The prob is: 0.0020337330643087626
1006500 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0460])
The negative loglikelihood (-log p(bomb)): 6.197882175445557 or 6.197882175445557 or 6.197882175445557
('switzerland', 'hunting', 'species', 'mosquito', 'capable', 'spreading')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2126, 'spreading') has the max prob in order to be the next word. The prob is: 0.0001276594412047416
1006600 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0064])
The negative loglikelihood (-log p(spreading)): 8.966144561767578 or 8.966144561767578 or 8.966144561767578
('chavannes', 'mayor', 'french', 'village', 'threatens', 'suicide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (179, 'suicide') has the max prob in order to be the next word. The prob is: 0.0036623836494982243
1006700 The nn predicts (tensor([128]), 'kill') the next word with max prob: tensor([0.0253])
The negative loglikelihood (-log p(suicide)): 5.609641075134277 or 5.609641075134277 or 5.609641075134277
('president', 'barack', 'obama', 'says', 'providing', 'additional')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3165, 'additional') has the max prob in order to be the next word. The prob is: 0.00012971716932952404
1006800 The nn predicts (tensor([492]), 'asylum') the next word with max prob: tensor([0.0326])
The negative loglikelihood (-log p(additional)): 8.950154304504395 or 8.950154304504395 or 8.950154304504395
('addressing', 'switched', 'digit', 'issues', 'affecting', 'hundreds')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (255, 'hundreds') has the max prob in order to be the next word. The prob is: 0.001581057091243565
1006900 The nn predicts (tensor([95]), 'global') the next word with max prob: tensor([0.0199])
The negative loglikelihood (-log p(hundreds)): 6.449661731719971 or 6.449661731719971 or 6.449661731719971
('sukhbir', 'takes', 'issue', 'removing', 'turban', 'dsgmc')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (64478, 'dsgmc') has the max prob in order to be the next word. The prob is: 4.486140369408531e-06
1007000 The nn predicts (tensor([363]), 'airport') the next word with max prob: tensor([0.0062])
The negative loglikelihood (-log p(dsgmc)): 12.314517974853516 or 12.314517974853516 or 12.314517974853516
('desert', 'supposed', 'catapult', 'mongolia', 'toward', 'rapid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3664, 'rapid') has the max prob in order to be the next word. The prob is: 0.000103999103885144
1007100 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0258])
The negative loglikelihood (-log p(rapid)): 9.171128273010254 or 9.171128273010254 or 9.171128273010254
('wikileaks', 'party', 'candidates', 'could', 'upset', 'balance')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3560, 'balance') has the max prob in order to be the next word. The prob is: 9.630717249820009e-05
1007200 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(balance)): 9.247967720031738 or 9.247967720031738 or 9.247967720031738
('syria', 'government', 'denies', 'rebel', 'claims', 'rockets')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1255, 'rockets') has the max prob in order to be the next word. The prob is: 0.0002668480738066137
1007300 The nn predicts (tensor([30]), 'syrian') the next word with max prob: tensor([0.0220])
The negative loglikelihood (-log p(rockets)): 8.22883129119873 or 8.22883129119873 or 8.22883129119873
('tunnelling', 'project', 'chief', 'archaeologist', 'says', 'bodies')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (691, 'bodies') has the max prob in order to be the next word. The prob is: 8.510162297170609e-05
1007400 The nn predicts (tensor([10]), 'israel') the next word with max prob: tensor([0.0136])
The negative loglikelihood (-log p(bodies)): 9.371664047241211 or 9.371664047241211 or 9.371664047241211
('two', 'years', 'slowing', 'growth', 'although', 'imminent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2750, 'imminent') has the max prob in order to be the next word. The prob is: 0.00012281628733035177
1007500 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0159])
The negative loglikelihood (-log p(imminent)): 9.004820823669434 or 9.004820823669434 or 9.004820823669434
('searching', 'without', 'warrants', 'contents', 'americans', 'communications')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2626, 'communications') has the max prob in order to be the next word. The prob is: 0.00019493798026815057
1007600 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0168])
The negative loglikelihood (-log p(communications)): 8.542829513549805 or 8.542829513549805 or 8.542829513549805
('rights', 'groups', 'human', 'rights', 'abuses', 'olympic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (864, 'olympic') has the max prob in order to be the next word. The prob is: 3.0456898457487114e-05
1007700 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.0839])
The negative loglikelihood (-log p(olympic)): 10.399198532104492 or 10.399197578430176 or 10.399197578430176
('says', 'specialist', 'pakistani', 'troops', 'involved', 'ambush')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3582, 'ambush') has the max prob in order to be the next word. The prob is: 0.00043708435259759426
1007800 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0599])
The negative loglikelihood (-log p(ambush)): 7.735384464263916 or 7.735384464263916 or 7.735384464263916
('researcher', 'invents', 'iron', 'fish', 'decrease', 'anemia')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (23482, 'anemia') has the max prob in order to be the next word. The prob is: 2.1537787233683048e-06
1007900 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0297])
The negative loglikelihood (-log p(anemia)): 13.048286437988281 or 13.048286437988281 or 13.048286437988281
('facebook', 'page', 'calls', 'mother', 'murdered', 'black')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (496, 'black') has the max prob in order to be the next word. The prob is: 0.0006718611693941057
1008000 The nn predicts (tensor([720]), 'islam') the next word with max prob: tensor([0.0226])
The negative loglikelihood (-log p(black)): 7.305459022521973 or 7.305459022521973 or 7.305459022521973
('president', 'bashar', 'al', 'assad', 'apparently', 'unharmed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12346, 'unharmed') has the max prob in order to be the next word. The prob is: 1.0825594472407829e-05
1008100 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0233])
The negative loglikelihood (-log p(unharmed)): 11.433597564697266 or 11.433597564697266 or 11.433597564697266
('offer', 'drop', 'assad', 'rich', 'arms', 'deal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (64, 'deal') has the max prob in order to be the next word. The prob is: 0.017839085310697556
1008200 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0280])
The negative loglikelihood (-log p(deal)): 4.026363372802734 or 4.026363372802734 or 4.026363372802734
('new', 'train', 'route', 'london', 'center', 'proving')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7298, 'proving') has the max prob in order to be the next word. The prob is: 7.938629096315708e-06
1008300 The nn predicts (tensor([363]), 'airport') the next word with max prob: tensor([0.0283])
The negative loglikelihood (-log p(proving)): 11.743769645690918 or 11.743769645690918 or 11.743769645690918
('burned', 'stolen', 'picasso', 'famous', 'artwork', 'cover')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1535, 'cover') has the max prob in order to be the next word. The prob is: 0.0004427373642101884
1008400 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0240])
The negative loglikelihood (-log p(cover)): 7.722533702850342 or 7.722533702850342 or 7.722533702850342
('workers', 'union', 'wields', 'oil', 'company', 'pemex')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10406, 'pemex') has the max prob in order to be the next word. The prob is: 1.5690646250732243e-05
1008500 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0221])
The negative loglikelihood (-log p(pemex)): 11.062445640563965 or 11.062445640563965 or 11.062445640563965
('nazi', 'germany', 'regarding', 'coming', 'winter', 'olympics')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (646, 'olympics') has the max prob in order to be the next word. The prob is: 0.005805585067719221
1008600 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0379])
The negative loglikelihood (-log p(olympics)): 5.148934841156006 or 5.148934841156006 or 5.148934841156006
('mother', 'ends', 'life', 'rape', 'kids', 'fight')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (152, 'fight') has the max prob in order to be the next word. The prob is: 0.0011632624082267284
1008700 The nn predicts (tensor([27]), 'india') the next word with max prob: tensor([0.0250])
The negative loglikelihood (-log p(fight)): 6.756526947021484 or 6.756526947021484 or 6.756526947021484
('signalled', 'house', 'prices', 'hit', 'record', 'high')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (235, 'high') has the max prob in order to be the next word. The prob is: 0.04939613118767738
1008800 The nn predicts (tensor([235]), 'high') the next word with max prob: tensor([0.0494])
The negative loglikelihood (-log p(high)): 3.007883071899414 or 3.007883071899414 or 3.007883071899414
('police', 'custody', 'since', 'single', 'police', 'officer')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (769, 'officer') has the max prob in order to be the next word. The prob is: 0.004719797056168318
1008900 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0444])
The negative loglikelihood (-log p(officer)): 5.355989456176758 or 5.355989456176758 or 5.355989456176758
('barack', 'obama', 'crimes', 'humanity', 'visiting', 'sweden')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (618, 'sweden') has the max prob in order to be the next word. The prob is: 9.069973020814359e-05
1009000 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0402])
The negative loglikelihood (-log p(sweden)): 9.30795669555664 or 9.307955741882324 or 9.307955741882324
('beirut', 'two', 'pilots', 'working', 'turkish', 'airlines')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (859, 'airlines') has the max prob in order to be the next word. The prob is: 0.00019062322098761797
1009100 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0575])
The negative loglikelihood (-log p(airlines)): 8.56521224975586 or 8.565211296081543 or 8.565211296081543
('decades', 'ago', 'persuaded', 'leave', 'officials', 'say')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (53, 'say') has the max prob in order to be the next word. The prob is: 0.18669134378433228
1009200 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.1867])
The negative loglikelihood (-log p(say)): 1.6782985925674438 or 1.6782985925674438 or 1.6782985925674438
('river', 'lake', 'water', 'clean', 'enough', 'household')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5793, 'household') has the max prob in order to be the next word. The prob is: 5.0687471230048686e-05
1009300 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0465])
The negative loglikelihood (-log p(household)): 9.88983154296875 or 9.88983154296875 or 9.88983154296875
('part', 'cache', 'documents', 'associated', 'press', 'found')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (59, 'found') has the max prob in order to be the next word. The prob is: 0.0009169505210593343
1009400 The nn predicts (tensor([51]), 'report') the next word with max prob: tensor([0.0157])
The negative loglikelihood (-log p(found)): 6.994457244873047 or 6.994457244873047 or 6.994457244873047
('bins', 'london', 'track', 'places', 'work', 'past')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (560, 'past') has the max prob in order to be the next word. The prob is: 0.002247569151222706
1009500 The nn predicts (tensor([298]), 'work') the next word with max prob: tensor([0.0074])
The negative loglikelihood (-log p(past)): 6.097906112670898 or 6.097906112670898 or 6.097906112670898
('use', 'marijuana', 'seen', 'slight', 'uptick', 'especially')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4326, 'especially') has the max prob in order to be the next word. The prob is: 0.0002117269905284047
1009600 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0177])
The negative loglikelihood (-log p(especially)): 8.460212707519531 or 8.460212707519531 or 8.460212707519531
('smuggling', 'chinese', 'migrants', 'europe', 'us', 'police')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8, 'police') has the max prob in order to be the next word. The prob is: 0.00037195521872490644
1009700 The nn predicts (tensor([343]), 'intelligence') the next word with max prob: tensor([0.0170])
The negative loglikelihood (-log p(police)): 7.896737098693848 or 7.896737098693848 or 7.896737098693848
('five', 'men', 'parents', 'release', 'pictures', 'injuries')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2821, 'injuries') has the max prob in order to be the next word. The prob is: 0.00021890275820624083
1009800 The nn predicts (tensor([31]), 'two') the next word with max prob: tensor([0.0155])
The negative loglikelihood (-log p(injuries)): 8.42688274383545 or 8.42688274383545 or 8.42688274383545
('told', 'beware', 'fish', 'experts', 'warned', 'swedish')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (797, 'swedish') has the max prob in order to be the next word. The prob is: 7.767633360344917e-05
1009900 The nn predicts (tensor([3]), 'us') the next word with max prob: tensor([0.0213])
The negative loglikelihood (-log p(swedish)): 9.462960243225098 or 9.462960243225098 or 9.462960243225098
('researchers', 'currently', 'exploring', 'centuries', 'old', 'tomb')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2693, 'tomb') has the max prob in order to be the next word. The prob is: 9.427460463484749e-05
1010000 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.0570])
The negative loglikelihood (-log p(tomb)): 9.269298553466797 or 9.269298553466797 or 9.269298553466797
('nyc', 'herakles', 'farms', 'says', 'project', 'involves')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8399, 'involves') has the max prob in order to be the next word. The prob is: 3.603901859605685e-05
1010100 The nn predicts (tensor([50]), 'could') the next word with max prob: tensor([0.0099])
The negative loglikelihood (-log p(involves)): 10.230908393859863 or 10.230908393859863 or 10.230908393859863
('around', 'killed', 'wounded', 'string', 'bombings', 'iraq')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (44, 'iraq') has the max prob in order to be the next word. The prob is: 0.019946610555052757
1010200 The nn predicts (tensor([60]), 'attacks') the next word with max prob: tensor([0.0327])
The negative loglikelihood (-log p(iraq)): 3.914695978164673 or 3.914695978164673 or 3.914695978164673
('condemns', 'belfast', 'loyalists', 'violence', 'leaves', 'officers')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (610, 'officers') has the max prob in order to be the next word. The prob is: 3.404614108148962e-05
1010300 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0475])
The negative loglikelihood (-log p(officers)): 10.28779411315918 or 10.28779411315918 or 10.28779411315918
('described', 'warlords', 'taking', 'part', 'syria', 'civil')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (452, 'civil') has the max prob in order to be the next word. The prob is: 0.013346172869205475
1010400 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0831])
The negative loglikelihood (-log p(civil)): 4.316525459289551 or 4.316525459289551 or 4.316525459289551
('japan', 'invisible', 'army', 'one', 'world', 'sophisticated')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4986, 'sophisticated') has the max prob in order to be the next word. The prob is: 1.7660753655945882e-05
1010500 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.4449])
The negative loglikelihood (-log p(sophisticated)): 10.94416618347168 or 10.944165229797363 or 10.944165229797363
('walk', 'redheads', 'take', 'streets', 'edinburgh', 'stand')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1198, 'stand') has the max prob in order to be the next word. The prob is: 0.000289014948066324
1010600 The nn predicts (tensor([363]), 'airport') the next word with max prob: tensor([0.0406])
The negative loglikelihood (-log p(stand)): 8.149032592773438 or 8.149032592773438 or 8.149032592773438
('ocean', 'sardines', 'mackerel', 'squid', 'generations', 'family')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (256, 'family') has the max prob in order to be the next word. The prob is: 0.00040516347507946193
1010700 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0142])
The negative loglikelihood (-log p(family)): 7.811220169067383 or 7.811219692230225 or 7.811219692230225
('khan', 'human', 'rights', 'organisation', 'odhikar', 'held')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (292, 'held') has the max prob in order to be the next word. The prob is: 0.0004781058814842254
1010800 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0224])
The negative loglikelihood (-log p(held)): 7.645678520202637 or 7.645678520202637 or 7.645678520202637
('kurds', 'northern', 'province', 'aleppo', 'sunday', 'turning')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1860, 'turning') has the max prob in order to be the next word. The prob is: 6.161875353427604e-05
1010900 The nn predicts (tensor([150]), 'killing') the next word with max prob: tensor([0.0372])
The negative loglikelihood (-log p(turning)): 9.694544792175293 or 9.694543838500977 or 9.694543838500977
('<s>', 'greek', 'youth', 'unemployment', 'hits', 'record')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (286, 'record') has the max prob in order to be the next word. The prob is: 0.0275422390550375
1011000 The nn predicts (tensor([286]), 'record') the next word with max prob: tensor([0.0275])
The negative loglikelihood (-log p(record)): 3.5920345783233643 or 3.5920345783233643 or 3.5920345783233643
('pm', 'accused', 'cheating', 'using', 'notes', 'debate')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (808, 'debate') has the max prob in order to be the next word. The prob is: 0.00011509386240504682
1011100 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.0502])
The negative loglikelihood (-log p(debate)): 9.069762229919434 or 9.069762229919434 or 9.069762229919434
('wiesenthal', 'centre', 'calls', 'boycott', 'hitler', 'wine')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3987, 'wine') has the max prob in order to be the next word. The prob is: 4.1818158933892846e-05
1011200 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0103])
The negative loglikelihood (-log p(wine)): 10.08218002319336 or 10.08218002319336 or 10.08218002319336
('china', 'children', 'series', 'disturbing', 'revelations', 'china')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1, 'china') has the max prob in order to be the next word. The prob is: 0.06241757050156593
1011300 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0624])
The negative loglikelihood (-log p(china)): 2.7739083766937256 or 2.7739083766937256 or 2.7739083766937256
('oil', 'train', 'disaster', 'montreal', 'maine', 'atlantic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2423, 'atlantic') has the max prob in order to be the next word. The prob is: 0.00014176573313307017
1011400 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0288])
The negative loglikelihood (-log p(atlantic)): 8.861334800720215 or 8.861334800720215 or 8.861334800720215
('netherlands', 'passed', 'away', 'nearly', 'one', 'half')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (634, 'half') has the max prob in order to be the next word. The prob is: 0.022405752912163734
1011500 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.0289])
The negative loglikelihood (-log p(half)): 3.7984375953674316 or 3.7984375953674316 or 3.7984375953674316
('stopped', 'operations', 'monday', 'timing', 'could', 'hardly')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9327, 'hardly') has the max prob in order to be the next word. The prob is: 3.966168515034951e-05
1011600 The nn predicts (tensor([342]), 'leave') the next word with max prob: tensor([0.0112])
The negative loglikelihood (-log p(hardly)): 10.135125160217285 or 10.135125160217285 or 10.135125160217285
('strikes', 'pacific', 'nation', 'triggering', 'landslides', 'floods')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1181, 'floods') has the max prob in order to be the next word. The prob is: 0.002822623122483492
1011700 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0255])
The negative loglikelihood (-log p(floods)): 5.870088577270508 or 5.870088577270508 or 5.870088577270508
('eu', 'questions', 'italy', 'telecom', 'watchdog', 'broadband')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6567, 'broadband') has the max prob in order to be the next word. The prob is: 1.815146788430866e-05
1011800 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0389])
The negative loglikelihood (-log p(broadband)): 10.916759490966797 or 10.916759490966797 or 10.916759490966797
('found', 'massive', 'change', 'electoral', 'demographic', 'could')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (50, 'could') has the max prob in order to be the next word. The prob is: 0.001522056758403778
1011900 The nn predicts (tensor([772]), 'growth') the next word with max prob: tensor([0.0428])
The negative loglikelihood (-log p(could)): 6.487692832946777 or 6.487692832946777 or 6.487692832946777
('computer', 'security', 'firm', 'warned', 'dalai', 'lama')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1896, 'lama') has the max prob in order to be the next word. The prob is: 0.006919029634445906
1012000 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0203])
The negative loglikelihood (-log p(lama)): 4.973479747772217 or 4.973479747772217 or 4.973479747772217
('bono', 'capitalism', 'takes', 'people', 'poverty', 'aid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (157, 'aid') has the max prob in order to be the next word. The prob is: 0.00026520079700276256
1012100 The nn predicts (tensor([776]), 'poor') the next word with max prob: tensor([0.0115])
The negative loglikelihood (-log p(aid)): 8.235023498535156 or 8.235023498535156 or 8.235023498535156
('game', 'added', 'armoury', 'ash', 'dieback', 'fight')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (152, 'fight') has the max prob in order to be the next word. The prob is: 6.405197200365365e-05
1012200 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0217])
The negative loglikelihood (-log p(fight)): 9.655816078186035 or 9.655816078186035 or 9.655816078186035
('large', 'lake', 'formed', 'north', 'pole', 'due')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (371, 'due') has the max prob in order to be the next word. The prob is: 0.0007131414604373276
1012300 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0203])
The negative loglikelihood (-log p(due)): 7.245830535888672 or 7.245830535888672 or 7.245830535888672
('girl', 'allegedly', 'raped', 'train', 'india', 'paid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1265, 'paid') has the max prob in order to be the next word. The prob is: 0.00025480042677372694
1012400 The nn predicts (tensor([197]), 'men') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(paid)): 8.275030136108398 or 8.275030136108398 or 8.275030136108398
('coptic', 'christians', 'dread', 'backlash', 'coptic', 'pope')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (217, 'pope') has the max prob in order to be the next word. The prob is: 0.0006657991325482726
1012500 The nn predicts (tensor([166]), 'muslim') the next word with max prob: tensor([0.0245])
The negative loglikelihood (-log p(pope)): 7.314522743225098 or 7.314522743225098 or 7.314522743225098
('manila', 'bay', 'causing', 'red', 'slick', 'sq')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7615, 'sq') has the max prob in order to be the next word. The prob is: 0.00014232340618036687
1012600 The nn predicts (tensor([126]), 'sea') the next word with max prob: tensor([0.0367])
The negative loglikelihood (-log p(sq)): 8.85740852355957 or 8.85740852355957 or 8.85740852355957
('weeks', 'current', 'peace', 'talks', 'bear', 'fruit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5511, 'fruit') has the max prob in order to be the next word. The prob is: 6.761196709703654e-05
1012700 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0080])
The negative loglikelihood (-log p(fruit)): 9.601725578308105 or 9.601725578308105 or 9.601725578308105
('girls', 'caught', 'trying', 'smuggle', 'cocaine', 'peru')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1350, 'peru') has the max prob in order to be the next word. The prob is: 0.0015796558000147343
1012800 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0112])
The negative loglikelihood (-log p(peru)): 6.45054817199707 or 6.45054817199707 or 6.45054817199707
('mayor', 'israeli', 'town', 'refuses', 'build', 'school')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (297, 'school') has the max prob in order to be the next word. The prob is: 0.0011613192036747932
1012900 The nn predicts (tensor([103]), 'border') the next word with max prob: tensor([0.0109])
The negative loglikelihood (-log p(school)): 6.7581987380981445 or 6.7581987380981445 or 6.7581987380981445
('years', 'life', 'beijing', 'building', 'rooftop', 'mountain')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2465, 'mountain') has the max prob in order to be the next word. The prob is: 0.001206448650918901
1013000 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0098])
The negative loglikelihood (-log p(mountain)): 6.72007417678833 or 6.72007417678833 or 6.72007417678833
('faction', 'syria', 'hands', 'teletubbies', 'spiderman', 'dolls')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9772, 'dolls') has the max prob in order to be the next word. The prob is: 2.990695065818727e-06
1013100 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0186])
The negative loglikelihood (-log p(dolls)): 12.72000503540039 or 12.72000503540039 or 12.72000503540039
('mexico', 'fight', 'release', 'drug', 'lord', 'behind')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (384, 'behind') has the max prob in order to be the next word. The prob is: 0.00047153138439171016
1013200 The nn predicts (tensor([184]), 'drug') the next word with max prob: tensor([0.0106])
The negative loglikelihood (-log p(behind)): 7.659524917602539 or 7.659524917602539 or 7.659524917602539
('french', 'government', 'tweets', 'link', 'sexy', 'underwear')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6079, 'underwear') has the max prob in order to be the next word. The prob is: 8.145545871229842e-05
1013300 The nn predicts (tensor([338]), 'online') the next word with max prob: tensor([0.0249])
The negative loglikelihood (-log p(underwear)): 9.415453910827637 or 9.415453910827637 or 9.415453910827637
('least', 'dead', 'police', 'swoop', 'cairo', 'demos')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14351, 'demos') has the max prob in order to be the next word. The prob is: 5.832150691276183e-07
1013400 The nn predicts (tensor([778]), 'mosque') the next word with max prob: tensor([0.0795])
The negative loglikelihood (-log p(demos)): 14.35470962524414 or 14.35470962524414 or 14.35470962524414
('reward', 'africa', 'booming', 'funeral', 'business', 'coffins')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12621, 'coffins') has the max prob in order to be the next word. The prob is: 3.403858272577054e-06
1013500 The nn predicts (tensor([134]), 'africa') the next word with max prob: tensor([0.0150])
The negative loglikelihood (-log p(coffins)): 12.590600967407227 or 12.590600967407227 or 12.590600967407227
('let', 'finish', 'says', 'spain', 'famous', 'potato')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11099, 'potato') has the max prob in order to be the next word. The prob is: 4.776581135956803e-06
1013600 The nn predicts (tensor([25]), 'minister') the next word with max prob: tensor([0.0327])
The negative loglikelihood (-log p(potato)): 12.251785278320312 or 12.251785278320312 or 12.251785278320312
('haunting', 'photos', 'egypt', 'veiled', 'woman', 'trying')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (521, 'trying') has the max prob in order to be the next word. The prob is: 0.0006177606410346925
1013700 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(trying)): 7.38940954208374 or 7.38940954208374 or 7.38940954208374
('emergency', 'scores', 'killed', 'security', 'forces', 'storm')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (623, 'storm') has the max prob in order to be the next word. The prob is: 0.0002211370738223195
1013800 The nn predicts (tensor([752]), 'cairo') the next word with max prob: tensor([0.0259])
The negative loglikelihood (-log p(storm)): 8.416728019714355 or 8.416728019714355 or 8.416728019714355
('photo', 'shows', 'two', 'security', 'officials', 'splayed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (64575, 'splayed') has the max prob in order to be the next word. The prob is: 5.438212156150257e-07
1013900 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.3018])
The negative loglikelihood (-log p(splayed)): 14.42464542388916 or 14.42464542388916 or 14.42464542388916
('cat', 'dna', 'database', 'helps', 'convict', 'killer')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1145, 'killer') has the max prob in order to be the next word. The prob is: 0.0008212215616367757
1014000 The nn predicts (tensor([59]), 'found') the next word with max prob: tensor([0.0145])
The negative loglikelihood (-log p(killer)): 7.10471773147583 or 7.10471773147583 or 7.10471773147583
('leader', 'lebanon', 'hezbollah', 'says', 'members', 'party')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (118, 'party') has the max prob in order to be the next word. The prob is: 0.002250610152259469
1014100 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0656])
The negative loglikelihood (-log p(party)): 6.096553802490234 or 6.096553802490234 or 6.096553802490234
('rocinha', 'call', 'light', 'shed', 'disappearance', 'local')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (466, 'local') has the max prob in order to be the next word. The prob is: 0.0016518988413736224
1014200 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0146])
The negative loglikelihood (-log p(local)): 6.405829906463623 or 6.405829906463623 or 6.405829906463623
('regime', 'wrong', 'path', 'white', 'house', 'avoided')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7014, 'avoided') has the max prob in order to be the next word. The prob is: 1.6629122910671867e-05
1014300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0469])
The negative loglikelihood (-log p(avoided)): 11.004355430603027 or 11.004355430603027 or 11.004355430603027
('<s>', 'last', 'stand', 'bbc', 'day', 'cairo')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (752, 'cairo') has the max prob in order to be the next word. The prob is: 0.02359672263264656
1014400 The nn predicts (tensor([96]), 'protests') the next word with max prob: tensor([0.0387])
The negative loglikelihood (-log p(cairo)): 3.746647357940674 or 3.746647357940674 or 3.746647357940674
('outside', 'world', 'images', 'show', 'members', 'kawahiva')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (37455, 'kawahiva') has the max prob in order to be the next word. The prob is: 1.6870825447767857e-06
1014500 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0133])
The negative loglikelihood (-log p(kawahiva)): 13.292510032653809 or 13.292510032653809 or 13.292510032653809
('reuters', 'rebels', 'syria', 'killed', 'italian', 'jesuit')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12413, 'jesuit') has the max prob in order to be the next word. The prob is: 4.812081442651106e-06
1014600 The nn predicts (tensor([172]), 'soldiers') the next word with max prob: tensor([0.0755])
The negative loglikelihood (-log p(jesuit)): 12.244380950927734 or 12.244380950927734 or 12.244380950927734
('explains', 'boss', 'spending', 'much', 'money', 'flying')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1929, 'flying') has the max prob in order to be the next word. The prob is: 4.320282459957525e-05
1014700 The nn predicts (tensor([358]), 'money') the next word with max prob: tensor([0.0166])
The negative loglikelihood (-log p(flying)): 10.049604415893555 or 10.049604415893555 or 10.049604415893555
('defends', 'crackdown', 'protestors', 'deathtoll', 'thursday', 'violence')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (141, 'violence') has the max prob in order to be the next word. The prob is: 0.0012369256000965834
1014800 The nn predicts (tensor([752]), 'cairo') the next word with max prob: tensor([0.0247])
The negative loglikelihood (-log p(violence)): 6.695126533508301 or 6.695126533508301 or 6.695126533508301
('supporters', 'stormed', 'government', 'building', 'cairo', 'set')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (142, 'set') has the max prob in order to be the next word. The prob is: 0.0007645856821909547
1014900 The nn predicts (tensor([988]), 'supporters') the next word with max prob: tensor([0.1323])
The negative loglikelihood (-log p(set)): 7.17617654800415 or 7.17617654800415 or 7.17617654800415
('investigating', 'alleged', 'gang', 'rape', 'involving', 'troops')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (92, 'troops') has the max prob in order to be the next word. The prob is: 0.00013805480557493865
1015000 The nn predicts (tensor([296]), 'charges') the next word with max prob: tensor([0.0308])
The negative loglikelihood (-log p(troops)): 8.887860298156738 or 8.887859344482422 or 8.887859344482422
('olinguito', 'first', 'new', 'carnivore', 'discovered', 'west')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (123, 'west') has the max prob in order to be the next word. The prob is: 0.0008442161488346756
1015100 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0162])
The negative loglikelihood (-log p(west)): 7.077102184295654 or 7.077102184295654 or 7.077102184295654
('cape', 'education', 'ministry', 'says', 'closed', 'schools')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (863, 'schools') has the max prob in order to be the next word. The prob is: 0.0002893061609938741
1015200 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0216])
The negative loglikelihood (-log p(schools)): 8.148025512695312 or 8.148025512695312 or 8.148025512695312
('<s>', '<s>', '<s>', '<s>', 'murder', 'craigslist')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (17392, 'craigslist') has the max prob in order to be the next word. The prob is: 2.0090558336960385e-06
1015300 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0442])
The negative loglikelihood (-log p(craigslist)): 13.11784553527832 or 13.11784553527832 or 13.11784553527832
('nina', 'siahkali', 'moradi', 'female', 'iranian', 'councillor')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10252, 'councillor') has the max prob in order to be the next word. The prob is: 2.4215194116550265e-06
1015400 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0972])
The negative loglikelihood (-log p(councillor)): 12.93111515045166 or 12.93111515045166 or 12.93111515045166
('must', 'pay', 'million', 'restitution', 'settlement', 'securities')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9276, 'securities') has the max prob in order to be the next word. The prob is: 8.858234650688246e-06
1015500 The nn predicts (tensor([87]), 'bank') the next word with max prob: tensor([0.1283])
The negative loglikelihood (-log p(securities)): 11.634162902832031 or 11.634162902832031 or 11.634162902832031
('<s>', '<s>', '<s>', 'iran', 'censors', 'internet')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (198, 'internet') has the max prob in order to be the next word. The prob is: 0.004251529928296804
1015600 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0224])
The negative loglikelihood (-log p(internet)): 5.460476398468018 or 5.460476398468018 or 5.460476398468018
('everyone', 'turned', 'us', 'says', 'yasser', 'local')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (466, 'local') has the max prob in order to be the next word. The prob is: 0.00014783126243855804
1015700 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0073])
The negative loglikelihood (-log p(local)): 8.819438934326172 or 8.819438934326172 or 8.819438934326172
('says', 'streaming', 'music', 'broadcast', 'waves', 'away')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (482, 'away') has the max prob in order to be the next word. The prob is: 0.0001471360301366076
1015800 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(away)): 8.824152946472168 or 8.824152946472168 or 8.824152946472168
('news', 'us', 'credibility', 'tatters', 'egypt', 'crisis')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (65, 'crisis') has the max prob in order to be the next word. The prob is: 0.011329390108585358
1015900 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0327])
The negative loglikelihood (-log p(crisis)): 4.480355262756348 or 4.480355262756348 or 4.480355262756348
('russian', 'military', 'facilities', 'egyptian', 'military', 'disposal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7867, 'disposal') has the max prob in order to be the next word. The prob is: 5.990776116959751e-06
1016000 The nn predicts (tensor([448]), 'base') the next word with max prob: tensor([0.0475])
The negative loglikelihood (-log p(disposal)): 12.025289535522461 or 12.025289535522461 or 12.025289535522461
('official', 'says', 'practice', 'taking', 'executed', 'prisoners')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (750, 'prisoners') has the max prob in order to be the next word. The prob is: 0.019977573305368423
1016100 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0510])
The negative loglikelihood (-log p(prisoners)): 3.913145065307617 or 3.913145065307617 or 3.913145065307617
('conflict', 'could', 'blow', 'back', 'onto', 'iran')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9, 'iran') has the max prob in order to be the next word. The prob is: 0.0023561660200357437
1016200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0189])
The negative loglikelihood (-log p(iran)): 6.050719738006592 or 6.050719738006592 or 6.050719738006592
('lady', 'sacked', 'emigrating', 'muslim', 'country', 'youngster')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (38938, 'youngster') has the max prob in order to be the next word. The prob is: 9.583156952430727e-07
1016300 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0127])
The negative loglikelihood (-log p(youngster)): 13.858088493347168 or 13.858088493347168 or 13.858088493347168
('found', 'lbs', 'ivory', 'operating', 'decades', 'since')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (153, 'since') has the max prob in order to be the next word. The prob is: 0.003359043039381504
1016400 The nn predicts (tensor([608]), 'ago') the next word with max prob: tensor([0.0556])
The negative loglikelihood (-log p(since)): 5.696099281311035 or 5.696099281311035 or 5.696099281311035
('vent', 'anger', 'nickelback', 'refusing', 'allow', 'flood')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1687, 'flood') has the max prob in order to be the next word. The prob is: 8.831398736219853e-05
1016500 The nn predicts (tensor([529]), 'laws') the next word with max prob: tensor([0.0157])
The negative loglikelihood (-log p(flood)): 9.334611892700195 or 9.334611892700195 or 9.334611892700195
('economic', 'tide', 'germany', 'floated', 'boats', 'though')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2299, 'though') has the max prob in order to be the next word. The prob is: 0.00026960953255183995
1016600 The nn predicts (tensor([66]), 'oil') the next word with max prob: tensor([0.0112])
The negative loglikelihood (-log p(though)): 8.218535423278809 or 8.218535423278809 or 8.218535423278809
('police', 'keep', 'firing', 'bodies', 'pile', 'cairo')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (752, 'cairo') has the max prob in order to be the next word. The prob is: 0.003923130687326193
1016700 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.1319])
The negative loglikelihood (-log p(cairo)): 5.540865421295166 or 5.540865421295166 or 5.540865421295166
('sunni', 'fight', 'shiite', 'muslims', 'gathered', 'friday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (300, 'friday') has the max prob in order to be the next word. The prob is: 0.0010637969244271517
1016800 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0359])
The negative loglikelihood (-log p(friday)): 6.845911026000977 or 6.845910549163818 or 6.845910549163818
('city', 'cease', 'abstain', 'discriminative', 'actions', 'indian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (135, 'indian') has the max prob in order to be the next word. The prob is: 0.0008371361182071269
1016900 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0183])
The negative loglikelihood (-log p(indian)): 7.085524082183838 or 7.085524082183838 or 7.085524082183838
('readiness', 'transmit', 'excess', 'electricity', 'burkina', 'faso')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3448, 'faso') has the max prob in order to be the next word. The prob is: 8.647237336845137e-06
1017000 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0224])
The negative loglikelihood (-log p(faso)): 11.658270835876465 or 11.658270835876465 or 11.658270835876465
('blood', 'chaos', 'prevail', 'egypt', 'testing', 'control')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (290, 'control') has the max prob in order to be the next word. The prob is: 0.00047500149230472744
1017100 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0116])
The negative loglikelihood (-log p(control)): 7.65219259262085 or 7.65219259262085 or 7.65219259262085
('western', 'myanmar', 'protested', 'monday', 'visit', 'united')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (140, 'united') has the max prob in order to be the next word. The prob is: 0.003913165535777807
1017200 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0299])
The negative loglikelihood (-log p(united)): 5.5434088706970215 or 5.5434088706970215 or 5.5434088706970215
('<s>', '<s>', '<s>', 'swiss', 'innovation', 'sex')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (220, 'sex') has the max prob in order to be the next word. The prob is: 0.00011538731632754207
1017300 The nn predicts (tensor([368]), 'policy') the next word with max prob: tensor([0.0080])
The negative loglikelihood (-log p(sex)): 9.067215919494629 or 9.067215919494629 or 9.067215919494629
('painting', 'fingernails', 'rainbow', 'colors', 'support', 'gay')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (251, 'gay') has the max prob in order to be the next word. The prob is: 0.015448857098817825
1017400 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0326])
The negative loglikelihood (-log p(gay)): 4.170220375061035 or 4.170220375061035 or 4.170220375061035
('oligarch', 'zahoor', 'wants', 'wife', 'kamaliya', 'ukrainian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (341, 'ukrainian') has the max prob in order to be the next word. The prob is: 6.869637581985444e-05
1017500 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0214])
The negative loglikelihood (-log p(ukrainian)): 9.585814476013184 or 9.585814476013184 or 9.585814476013184
('gondola', 'ride', 'famous', 'grand', 'canal', 'venice')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5962, 'venice') has the max prob in order to be the next word. The prob is: 1.5429488485096954e-05
1017600 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0171])
The negative loglikelihood (-log p(venice)): 11.079230308532715 or 11.079230308532715 or 11.079230308532715
('murder', 'involving', 'firearms', 'reported', 'four', 'months')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (281, 'months') has the max prob in order to be the next word. The prob is: 0.03490424528717995
1017700 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0871])
The negative loglikelihood (-log p(months)): 3.355146884918213 or 3.355146884918213 or 3.355146884918213
('followers', 'sina', 'weibo', 'microblogging', 'service', 'japanese')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (265, 'japanese') has the max prob in order to be the next word. The prob is: 0.0003520724712871015
1017800 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0154])
The negative loglikelihood (-log p(japanese)): 7.95167350769043 or 7.95167350769043 or 7.95167350769043
('muslim', 'brotherhood', 'supporters', 'barricaded', 'inside', 'state')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16, 'state') has the max prob in order to be the next word. The prob is: 0.0011654429836198688
1017900 The nn predicts (tensor([778]), 'mosque') the next word with max prob: tensor([0.1255])
The negative loglikelihood (-log p(state)): 6.7546539306640625 or 6.7546539306640625 or 6.7546539306640625
('said', 'scaling', 'back', 'work', 'exploration', 'site')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (383, 'site') has the max prob in order to be the next word. The prob is: 0.0015476016560569406
1018000 The nn predicts (tensor([180]), 'gas') the next word with max prob: tensor([0.0190])
The negative loglikelihood (-log p(site)): 6.471048831939697 or 6.471048831939697 or 6.471048831939697
('media', 'done', 'since', 'attacks', 'never', 'investigated')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2905, 'investigated') has the max prob in order to be the next word. The prob is: 4.971760790795088e-05
1018100 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0146])
The negative loglikelihood (-log p(investigated)): 9.909151077270508 or 9.909151077270508 or 9.909151077270508
('bin', 'talal', 'fired', 'director', 'islamic', 'tv')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (374, 'tv') has the max prob in order to be the next word. The prob is: 0.0005862482939846814
1018200 The nn predicts (tensor([62]), 'group') the next word with max prob: tensor([0.1841])
The negative loglikelihood (-log p(tv)): 7.44176721572876 or 7.44176721572876 or 7.44176721572876
('investigate', 'jpmorgan', 'hiring', 'children', 'chinese', 'officials')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (79, 'officials') has the max prob in order to be the next word. The prob is: 0.006179827265441418
1018300 The nn predicts (tensor([239]), 'authorities') the next word with max prob: tensor([0.0449])
The negative loglikelihood (-log p(officials)): 5.086464881896973 or 5.086464881896973 or 5.086464881896973
('talks', 'pick', 'palestinians', 'demand', 'return', 'villages')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1701, 'villages') has the max prob in order to be the next word. The prob is: 0.00015820724365767092
1018400 The nn predicts (tensor([115]), 'back') the next word with max prob: tensor([0.0478])
The negative loglikelihood (-log p(villages)): 8.751605033874512 or 8.751605033874512 or 8.751605033874512
('baby', 'rahul', 'spontaneous', 'human', 'combustion', 'real')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (737, 'real') has the max prob in order to be the next word. The prob is: 0.00034902369952760637
1018500 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(real)): 7.9603705406188965 or 7.9603705406188965 or 7.9603705406188965
('built', 'large', 'presence', 'kurdistan', 'oil', 'gas')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (180, 'gas') has the max prob in order to be the next word. The prob is: 0.03222905471920967
1018600 The nn predicts (tensor([66]), 'oil') the next word with max prob: tensor([0.0473])
The negative loglikelihood (-log p(gas)): 3.434886932373047 or 3.434886932373047 or 3.434886932373047
('radio', 'station', 'zm', 'hosting', 'one', 'first')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (28, 'first') has the max prob in order to be the next word. The prob is: 0.000916408549528569
1018700 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0576])
The negative loglikelihood (-log p(first)): 6.995048522949219 or 6.9950480461120605 or 6.9950480461120605
('chemical', 'probe', 'team', 'weapons', 'inspectors', 'arrived')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2754, 'arrived') has the max prob in order to be the next word. The prob is: 0.0005258720484562218
1018800 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0474])
The negative loglikelihood (-log p(arrived)): 7.550452709197998 or 7.550452709197998 or 7.550452709197998
('tries', 'new', 'tack', 'defectors', 'nice', 'kim')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (441, 'kim') has the max prob in order to be the next word. The prob is: 0.000171957683051005
1018900 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.0388])
The negative loglikelihood (-log p(kim)): 8.668262481689453 or 8.668262481689453 or 8.668262481689453
('<s>', 'jordan', 'oil', 'imports', 'first', 'half')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (634, 'half') has the max prob in order to be the next word. The prob is: 0.001080153975635767
1019000 The nn predicts (tensor([101]), 'time') the next word with max prob: tensor([0.1801])
The negative loglikelihood (-log p(half)): 6.830651760101318 or 6.830651760101318 or 6.830651760101318
('diplomatic', 'campaign', 'urging', 'europe', 'united', 'states')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (182, 'states') has the max prob in order to be the next word. The prob is: 0.4354907274246216
1019100 The nn predicts (tensor([182]), 'states') the next word with max prob: tensor([0.4355])
The negative loglikelihood (-log p(states)): 0.8312817811965942 or 0.8312817811965942 or 0.8312817811965942
('plume', 'approximately', 'feet', 'high', 'article', 'pictures')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1167, 'pictures') has the max prob in order to be the next word. The prob is: 0.00024782592663541436
1019200 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0420])
The negative loglikelihood (-log p(pictures)): 8.302783966064453 or 8.302783966064453 or 8.302783966064453
('saudi', 'king', 'offers', 'support', 'egyptian', 'military')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18, 'military') has the max prob in order to be the next word. The prob is: 0.035315170884132385
1019300 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.1258])
The negative loglikelihood (-log p(military)): 3.343442678451538 or 3.343442678451538 or 3.343442678451538
('least', 'egyptian', 'police', 'sinai', 'news', 'reports')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (234, 'reports') has the max prob in order to be the next word. The prob is: 0.009087917394936085
1019400 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0393])
The negative loglikelihood (-log p(reports)): 4.700809478759766 or 4.700809478759766 or 4.700809478759766
('orders', 'release', 'hosni', 'mubarak', 'graft', 'case')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (177, 'case') has the max prob in order to be the next word. The prob is: 0.012272496707737446
1019500 The nn predicts (tensor([296]), 'charges') the next word with max prob: tensor([0.0660])
The negative loglikelihood (-log p(case)): 4.400394439697266 or 4.400394439697266 or 4.400394439697266
('illegal', 'due', 'conflicts', 'interest', 'gas', 'hub')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3690, 'hub') has the max prob in order to be the next word. The prob is: 0.00038005688111297786
1019600 The nn predicts (tensor([66]), 'oil') the next word with max prob: tensor([0.0244])
The negative loglikelihood (-log p(hub)): 7.875189781188965 or 7.875189781188965 or 7.875189781188965
('african', 'platinum', 'miners', 'gathered', 'friday', 'hill')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5838, 'hill') has the max prob in order to be the next word. The prob is: 1.4272387488745153e-05
1019700 The nn predicts (tensor([295]), 'monday') the next word with max prob: tensor([0.0171])
The negative loglikelihood (-log p(hill)): 11.157183647155762 or 11.157183647155762 or 11.157183647155762
('making', 'yelena', 'isinbayeva', 'youth', 'ambassador', 'failed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (697, 'failed') has the max prob in order to be the next word. The prob is: 0.00027427924214862287
1019800 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0531])
The negative loglikelihood (-log p(failed)): 8.201363563537598 or 8.201363563537598 or 8.201363563537598
('observatory', 'human', 'rights', 'said', 'monday', 'almost')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (549, 'almost') has the max prob in order to be the next word. The prob is: 0.0006498036091215909
1019900 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0197])
The negative loglikelihood (-log p(almost)): 7.338840484619141 or 7.338840484619141 or 7.338840484619141
('caught', 'sisters', 'egypt', 'protests', 'beaten', 'hell')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2498, 'hell') has the max prob in order to be the next word. The prob is: 5.227063229540363e-05
1020000 The nn predicts (tensor([42]), 'death') the next word with max prob: tensor([0.0257])
The negative loglikelihood (-log p(hell)): 9.859075546264648 or 9.859075546264648 or 9.859075546264648
('setback', 'leader', 'kim', 'pet', 'ski', 'resort')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2459, 'resort') has the max prob in order to be the next word. The prob is: 0.001956470077857375
1020100 The nn predicts (tensor([28]), 'first') the next word with max prob: tensor([0.0068])
The negative loglikelihood (-log p(resort)): 6.2366132736206055 or 6.2366132736206055 or 6.2366132736206055
('fifth', 'wingsuit', 'jumper', 'plunges', 'death', 'alps')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4641, 'alps') has the max prob in order to be the next word. The prob is: 4.711730434792116e-05
1020200 The nn predicts (tensor([438]), 'toll') the next word with max prob: tensor([0.1637])
The negative loglikelihood (-log p(alps)): 9.962870597839355 or 9.962870597839355 or 9.962870597839355
('recent', 'decades', 'sea', 'levels', 'could', 'rise')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (337, 'rise') has the max prob in order to be the next word. The prob is: 0.004863082431256771
1020300 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0179])
The negative loglikelihood (-log p(rise)): 5.326082706451416 or 5.326082706451416 or 5.326082706451416
('drones', 'initial', 'reports', 'suggest', 'drone', 'countermeasures')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (18333, 'countermeasures') has the max prob in order to be the next word. The prob is: 1.1443603398220148e-06
1020400 The nn predicts (tensor([193]), 'strikes') the next word with max prob: tensor([0.3751])
The negative loglikelihood (-log p(countermeasures)): 13.680665016174316 or 13.680665016174316 or 13.680665016174316
('police', 'release', 'statement', 'david', 'miranda', 'detained')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (536, 'detained') has the max prob in order to be the next word. The prob is: 0.001376499654725194
1020500 The nn predicts (tensor([442]), 'released') the next word with max prob: tensor([0.0175])
The negative loglikelihood (-log p(detained)): 6.588211536407471 or 6.588211536407471 or 6.588211536407471
('railways', 'rail', 'ministry', 'admits', 'mown', 'trains')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3099, 'trains') has the max prob in order to be the next word. The prob is: 7.449875556631014e-05
1020600 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0273])
The negative loglikelihood (-log p(trains)): 9.504728317260742 or 9.504728317260742 or 9.504728317260742
('kids', 'bring', 'drones', 'iranian', 'high', 'schools')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (863, 'schools') has the max prob in order to be the next word. The prob is: 0.0005110450438223779
1020700 The nn predicts (tensor([297]), 'school') the next word with max prob: tensor([0.0158])
The negative loglikelihood (-log p(schools)): 7.579052925109863 or 7.579052925109863 or 7.579052925109863
('egyptian', 'journalist', 'shot', 'dead', 'curfew', 'egyptian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (233, 'egyptian') has the max prob in order to be the next word. The prob is: 0.005934145301580429
1020800 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0540])
The negative loglikelihood (-log p(egyptian)): 5.127032279968262 or 5.127032279968262 or 5.127032279968262
('become', 'first', 'european', 'country', 'legally', 'recognise')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5536, 'recognise') has the max prob in order to be the next word. The prob is: 4.501420335145667e-05
1020900 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0205])
The negative loglikelihood (-log p(recognise)): 10.008532524108887 or 10.008532524108887 or 10.008532524108887
('egypt', 'foreign', 'secretary', 'albert', 'del', 'rosario')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (32486, 'rosario') has the max prob in order to be the next word. The prob is: 4.987898591934936e-06
1021000 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.1403])
The negative loglikelihood (-log p(rosario)): 12.20849609375 or 12.20849609375 or 12.20849609375
('within', 'hour', 'receive', 'radiation', 'dose', 'five')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (225, 'five') has the max prob in order to be the next word. The prob is: 0.00012798189709428698
1021100 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(five)): 8.963622093200684 or 8.963622093200684 or 8.963622093200684
('minister', 'claims', 'israel', 'behind', 'egyptian', 'coup')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (597, 'coup') has the max prob in order to be the next word. The prob is: 0.014909844845533371
1021200 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0997])
The negative loglikelihood (-log p(coup)): 4.205733776092529 or 4.205733776092529 or 4.205733776092529
('hitting', 'woman', 'around', 'head', 'posted', 'online')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (338, 'online') has the max prob in order to be the next word. The prob is: 0.014266684651374817
1021300 The nn predicts (tensor([100]), 'video') the next word with max prob: tensor([0.0201])
The negative loglikelihood (-log p(online)): 4.249828338623047 or 4.249828338623047 or 4.249828338623047
('activist', 'campaigning', 'law', 'ban', 'black', 'magic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7006, 'magic') has the max prob in order to be the next word. The prob is: 2.6478506697458215e-05
1021400 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0138])
The negative loglikelihood (-log p(magic)): 10.539176940917969 or 10.539176940917969 or 10.539176940917969
('official', 'certainty', 'humans', 'responsible', 'climate', 'change')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (162, 'change') has the max prob in order to be the next word. The prob is: 0.16371142864227295
1021500 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.1637])
The negative loglikelihood (-log p(change)): 1.80964994430542 or 1.80964994430542 or 1.80964994430542
('south', 'africa', 'richards', 'bay', 'port', 'large')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (813, 'large') has the max prob in order to be the next word. The prob is: 0.00020529815810732543
1021600 The nn predicts (tensor([112]), 'near') the next word with max prob: tensor([0.0168])
The negative loglikelihood (-log p(large)): 8.491046905517578 or 8.491046905517578 or 8.491046905517578
('national', 'trust', 'resignation', 'crackdown', 'morsi', 'supporters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (988, 'supporters') has the max prob in order to be the next word. The prob is: 0.028215378522872925
1021700 The nn predicts (tensor([988]), 'supporters') the next word with max prob: tensor([0.0282])
The negative loglikelihood (-log p(supporters)): 3.567888021469116 or 3.567888021469116 or 3.567888021469116
('high', 'school', 'students', 'dismissed', 'excessive', 'unethical')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13155, 'unethical') has the max prob in order to be the next word. The prob is: 7.441617526637856e-06
1021800 The nn predicts (tensor([529]), 'laws') the next word with max prob: tensor([0.0285])
The negative loglikelihood (-log p(unethical)): 11.808422088623047 or 11.808422088623047 or 11.808422088623047
('cheap', 'grain', 'month', 'million', 'poor', 'people')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14, 'people') has the max prob in order to be the next word. The prob is: 0.012953615747392178
1021900 The nn predicts (tensor([776]), 'poor') the next word with max prob: tensor([0.0346])
The negative loglikelihood (-log p(people)): 4.346380233764648 or 4.346380233764648 or 4.346380233764648
('<s>', '<s>', 'rebirth', 'istanbul', 'bosphorus', 'istanbul')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1204, 'istanbul') has the max prob in order to be the next word. The prob is: 0.0013995420886203647
1022000 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0509])
The negative loglikelihood (-log p(istanbul)): 6.571609973907471 or 6.571609973907471 or 6.571609973907471
('world', 'championships', 'paralympian', 'sophie', 'pascoe', 'received')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1561, 'received') has the max prob in order to be the next word. The prob is: 0.00022359385911840945
1022100 The nn predicts (tensor([319]), 'dies') the next word with max prob: tensor([0.0206])
The negative loglikelihood (-log p(received)): 8.405679702758789 or 8.405679702758789 or 8.405679702758789
('promoted', 'fake', 'bomb', 'detectors', 'three', 'departments')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (11320, 'departments') has the max prob in order to be the next word. The prob is: 3.954049134335946e-06
1022200 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0624])
The negative loglikelihood (-log p(departments)): 12.440770149230957 or 12.440770149230957 or 12.440770149230957
('paper', 'reporting', 'documents', 'edward', 'snowden', 'gave')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1745, 'gave') has the max prob in order to be the next word. The prob is: 0.0005575782852247357
1022300 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0242])
The negative loglikelihood (-log p(gave)): 7.491907596588135 or 7.491907596588135 or 7.491907596588135
('inmates', 'eat', 'frogs', 'mother', 'kill', 'baby')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (760, 'baby') has the max prob in order to be the next word. The prob is: 0.006361460313200951
1022400 The nn predicts (tensor([93]), 'children') the next word with max prob: tensor([0.0137])
The negative loglikelihood (-log p(baby)): 5.057497501373291 or 5.057497501373291 or 5.057497501373291
('<s>', 'china', 'takes', 'aim', 'western', 'ideas')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4473, 'ideas') has the max prob in order to be the next word. The prob is: 7.719333370914683e-05
1022500 The nn predicts (tensor([168]), 'countries') the next word with max prob: tensor([0.0336])
The negative loglikelihood (-log p(ideas)): 9.469197273254395 or 9.469197273254395 or 9.469197273254395
('violent', 'reaction', 'protests', 'spurs', 'coup', 'supporters')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (988, 'supporters') has the max prob in order to be the next word. The prob is: 0.009112518280744553
1022600 The nn predicts (tensor([597]), 'coup') the next word with max prob: tensor([0.0459])
The negative loglikelihood (-log p(supporters)): 4.698106288909912 or 4.698106288909912 or 4.698106288909912
('attack', 'near', 'damascus', 'un', 'observers', 'arrive')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2324, 'arrive') has the max prob in order to be the next word. The prob is: 0.00011498442472657189
1022700 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.1801])
The negative loglikelihood (-log p(arrive)): 9.070713996887207 or 9.070713996887207 or 9.070713996887207
('guardian', 'contacted', 'britain', 'top', 'civil', 'servant')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9580, 'servant') has the max prob in order to be the next word. The prob is: 1.1399900358810555e-05
1022800 The nn predicts (tensor([17]), 'war') the next word with max prob: tensor([0.0738])
The negative loglikelihood (-log p(servant)): 11.381905555725098 or 11.381905555725098 or 11.381905555725098
('<s>', '<s>', '<s>', 'journalism', 'terrorism', 'greenwald')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4290, 'greenwald') has the max prob in order to be the next word. The prob is: 0.00021082378225401044
1022900 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0406])
The negative loglikelihood (-log p(greenwald)): 8.46448802947998 or 8.46448802947998 or 8.46448802947998
('<s>', 'scientists', 'attempting', 'clone', 'john', 'lennon')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (16329, 'lennon') has the max prob in order to be the next word. The prob is: 3.120120936728199e-06
1023000 The nn predicts (tensor([395]), 'snowden') the next word with max prob: tensor([0.0215])
The negative loglikelihood (-log p(lennon)): 12.67763900756836 or 12.67763900756836 or 12.67763900756836
('russians', 'contact', 'authorities', 'report', 'eerie', 'behaviour')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5053, 'behaviour') has the max prob in order to be the next word. The prob is: 3.86196406907402e-05
1023100 The nn predicts (tensor([100]), 'video') the next word with max prob: tensor([0.0148])
The negative loglikelihood (-log p(behaviour)): 10.161749839782715 or 10.161749839782715 or 10.161749839782715
('mediterranean', 'men', 'latest', 'sexual', 'bloomers', 'europe')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (78, 'europe') has the max prob in order to be the next word. The prob is: 0.010430717840790749
1023200 The nn predicts (tensor([46]), 'egypt') the next word with max prob: tensor([0.0376])
The negative loglikelihood (-log p(europe)): 4.563000202178955 or 4.563000202178955 or 4.563000202178955
('suffers', 'affluent', 'citizens', 'pyongyang', 'enjoy', 'microbreweries')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (64701, 'microbreweries') has the max prob in order to be the next word. The prob is: 1.4932235217202106e-06
1023300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0065])
The negative loglikelihood (-log p(microbreweries)): 13.414573669433594 or 13.414573669433594 or 13.414573669433594
('ongoing', 'fukushima', 'nuclear', 'holocaust', 'almost', 'horrifying')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7307, 'horrifying') has the max prob in order to be the next word. The prob is: 1.089660781872226e-05
1023400 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0346])
The negative loglikelihood (-log p(horrifying)): 11.427059173583984 or 11.427059173583984 or 11.427059173583984
('kicked', 'kremlin', 'network', 'protesting', 'law', 'video')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (100, 'video') has the max prob in order to be the next word. The prob is: 0.0017527774907648563
1023500 The nn predicts (tensor([251]), 'gay') the next word with max prob: tensor([0.0144])
The negative loglikelihood (-log p(video)): 6.346553802490234 or 6.346553802490234 or 6.346553802490234
('sold', 'ships', 'researcher', 'says', 'thunder', 'bay')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1243, 'bay') has the max prob in order to be the next word. The prob is: 0.0012472873786464334
1023600 The nn predicts (tensor([349]), 'ship') the next word with max prob: tensor([0.0161])
The negative loglikelihood (-log p(bay)): 6.686784267425537 or 6.686784267425537 or 6.686784267425537
('lawyer', 'stealing', 'using', 'classified', 'documents', 'sign')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (550, 'sign') has the max prob in order to be the next word. The prob is: 5.914798748563044e-05
1023700 The nn predicts (tensor([692]), 'documents') the next word with max prob: tensor([0.0256])
The negative loglikelihood (-log p(sign)): 9.735467910766602 or 9.735467910766602 or 9.735467910766602
('including', 'hundreds', 'women', 'children', 'wiped', 'nerve')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5274, 'nerve') has the max prob in order to be the next word. The prob is: 0.00013204595597926527
1023800 The nn predicts (tensor([297]), 'school') the next word with max prob: tensor([0.0174])
The negative loglikelihood (-log p(nerve)): 8.932360649108887 or 8.932360649108887 or 8.932360649108887
('suggest', 'tickets', 'pot', 'possession', 'instead', 'criminal')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (566, 'criminal') has the max prob in order to be the next word. The prob is: 0.0007735073450021446
1023900 The nn predicts (tensor([294]), 'pay') the next word with max prob: tensor([0.0051])
The negative loglikelihood (-log p(criminal)): 7.164575576782227 or 7.164575576782227 or 7.164575576782227
('david', 'miranda', 'lawyers', 'apply', 'interim', 'injunction')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8796, 'injunction') has the max prob in order to be the next word. The prob is: 6.352366654027719e-06
1024000 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.1683])
The negative loglikelihood (-log p(injunction)): 11.966683387756348 or 11.966683387756348 or 11.966683387756348
('three', 'members', 'nfl', 'team', 'hall', 'fame')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9871, 'fame') has the max prob in order to be the next word. The prob is: 1.6212808986892924e-05
1024100 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0160])
The negative loglikelihood (-log p(fame)): 11.029708862304688 or 11.029708862304688 or 11.029708862304688
('one', 'politician', 'says', 'disappearance', 'skilled', 'professionals')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (9520, 'professionals') has the max prob in order to be the next word. The prob is: 1.9769979189732112e-05
1024200 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.0268])
The negative loglikelihood (-log p(professionals)): 10.83134651184082 or 10.831345558166504 or 10.831345558166504
('election', 'hassan', 'rouhani', 'new', 'dawn', 'women')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (75, 'women') has the max prob in order to be the next word. The prob is: 0.00010767839557956904
1024300 The nn predicts (tensor([118]), 'party') the next word with max prob: tensor([0.2156])
The negative loglikelihood (-log p(women)): 9.136362075805664 or 9.136361122131348 or 9.136361122131348
('chemical', 'weapons', 'attack', 'syria', 'activists', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.024202333763241768
1024400 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.1799])
The negative loglikelihood (-log p(said)): 3.721306324005127 or 3.721306324005127 or 3.721306324005127
('female', 'identity', 'wants', 'live', 'woman', 'named')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1275, 'named') has the max prob in order to be the next word. The prob is: 0.0003609047853387892
1024500 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0128])
The negative loglikelihood (-log p(named)): 7.926896572113037 or 7.926896572113037 or 7.926896572113037
('possibility', 'racism', 'motive', 'committee', 'chairman', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.029596267268061638
1024600 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0353])
The negative loglikelihood (-log p(said)): 3.5201070308685303 or 3.5201070308685303 or 3.5201070308685303
('fighting', 'land', 'indigenous', 'peoples', 'brazil', 'lost')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (584, 'lost') has the max prob in order to be the next word. The prob is: 0.000550747150555253
1024700 The nn predicts (tensor([16]), 'state') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(lost)): 7.504234790802002 or 7.504234790802002 or 7.504234790802002
('says', 'unable', 'conclusively', 'say', 'chemical', 'weapons')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (149, 'weapons') has the max prob in order to be the next word. The prob is: 0.4203028678894043
1024800 The nn predicts (tensor([149]), 'weapons') the next word with max prob: tensor([0.4203])
The negative loglikelihood (-log p(weapons)): 0.8667797446250916 or 0.8667796850204468 or 0.8667796850204468
('suggests', 'character', 'deeply', 'flawed', 'rash', 'dangerous')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (968, 'dangerous') has the max prob in order to be the next word. The prob is: 0.0003759869432542473
1024900 The nn predicts (tensor([60]), 'attacks') the next word with max prob: tensor([0.0307])
The negative loglikelihood (-log p(dangerous)): 7.885956287384033 or 7.885956287384033 or 7.885956287384033
('minister', 'warns', 'people', 'thinking', 'going', 'usa')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1864, 'usa') has the max prob in order to be the next word. The prob is: 9.809609764488414e-05
1025000 The nn predicts (tensor([236]), 'like') the next word with max prob: tensor([0.0420])
The negative loglikelihood (-log p(usa)): 9.229562759399414 or 9.229562759399414 or 9.229562759399414
('president', 'military', 'accused', 'treason', 'anybody', 'even')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (334, 'even') has the max prob in order to be the next word. The prob is: 0.0014492030022665858
1025100 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.0121])
The negative loglikelihood (-log p(even)): 6.536741733551025 or 6.536741733551025 or 6.536741733551025
('wants', 'abolish', 'nuclear', 'power', 'japan', 'videos')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1945, 'videos') has the max prob in order to be the next word. The prob is: 2.0214774849591777e-05
1025200 The nn predicts (tensor([32]), 'nuclear') the next word with max prob: tensor([0.2467])
The negative loglikelihood (-log p(videos)): 10.809097290039062 or 10.809096336364746 or 10.809096336364746
('following', 'ordos', 'wenzhou', 'real', 'estate', 'market')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (487, 'market') has the max prob in order to be the next word. The prob is: 0.0012641155626624823
1025300 The nn predicts (tensor([1]), 'china') the next word with max prob: tensor([0.0125])
The negative loglikelihood (-log p(market)): 6.673382759094238 or 6.673382759094238 or 6.673382759094238
('team', 'investigate', 'allegations', 'chemical', 'weapons', 'used')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (218, 'used') has the max prob in order to be the next word. The prob is: 0.041198935359716415
1025400 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.1499])
The negative loglikelihood (-log p(used)): 3.189342975616455 or 3.189342975616455 or 3.189342975616455
('investigate', 'claims', 'government', 'carried', 'chemical', 'attack')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15, 'attack') has the max prob in order to be the next word. The prob is: 0.23216035962104797
1025500 The nn predicts (tensor([149]), 'weapons') the next word with max prob: tensor([0.6504])
The negative loglikelihood (-log p(attack)): 1.460326910018921 or 1.460326910018921 or 1.460326910018921
('gdp', 'revision', 'shows', 'economy', 'expanding', 'faster')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2979, 'faster') has the max prob in order to be the next word. The prob is: 0.00011757265747291967
1025600 The nn predicts (tensor([285]), 'economy') the next word with max prob: tensor([0.0362])
The negative loglikelihood (-log p(faster)): 9.048454284667969 or 9.048454284667969 or 9.048454284667969
('israeli', 'police', 'accused', 'child', 'torture', 'middle')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (344, 'middle') has the max prob in order to be the next word. The prob is: 0.00019008322851732373
1025700 The nn predicts (tensor([296]), 'charges') the next word with max prob: tensor([0.0309])
The negative loglikelihood (-log p(middle)): 8.568048477172852 or 8.568048477172852 or 8.568048477172852
('prosecutors', 'say', 'given', 'couple', 'businessman', 'friend')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2149, 'friend') has the max prob in order to be the next word. The prob is: 0.00023256501299329102
1025800 The nn predicts (tensor([98]), 'arrested') the next word with max prob: tensor([0.0152])
The negative loglikelihood (-log p(friend)): 8.366340637207031 or 8.366340637207031 or 8.366340637207031
('germany', 'breaks', 'record', 'solar', 'power', 'generation')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2099, 'generation') has the max prob in order to be the next word. The prob is: 0.00035237183328717947
1025900 The nn predicts (tensor([347]), 'plant') the next word with max prob: tensor([0.0312])
The negative loglikelihood (-log p(generation)): 7.950823783874512 or 7.950823783874512 or 7.950823783874512
('department', 'intelligence', 'agencies', 'met', 'three', 'half')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (634, 'half') has the max prob in order to be the next word. The prob is: 0.0026727893855422735
1026000 The nn predicts (tensor([33]), 'years') the next word with max prob: tensor([0.0776])
The negative loglikelihood (-log p(half)): 5.924632549285889 or 5.924632549285889 or 5.924632549285889
('pictures', 'looters', 'shatter', 'museum', 'ancient', 'egyptian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (233, 'egyptian') has the max prob in order to be the next word. The prob is: 0.00038521745591424406
1026100 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0332])
The negative loglikelihood (-log p(egyptian)): 7.8617024421691895 or 7.8617024421691895 or 7.8617024421691895
('nfl', 'pressured', 'espn', 'withdraw', 'frontline', 'concussion')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (28965, 'concussion') has the max prob in order to be the next word. The prob is: 2.02278170036152e-06
1026200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0996])
The negative loglikelihood (-log p(concussion)): 13.111037254333496 or 13.111037254333496 or 13.111037254333496
('churches', 'schools', 'institutions', 'shops', 'torched', 'muslim')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (166, 'muslim') has the max prob in order to be the next word. The prob is: 0.010947134345769882
1026300 The nn predicts (tensor([255]), 'hundreds') the next word with max prob: tensor([0.0341])
The negative loglikelihood (-log p(muslim)): 4.51467752456665 or 4.51467752456665 or 4.51467752456665
('olympics', 'decree', 'designates', 'olympic', 'venues', 'ports')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5355, 'ports') has the max prob in order to be the next word. The prob is: 1.7166228644782677e-05
1026400 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0216])
The negative loglikelihood (-log p(ports)): 10.972566604614258 or 10.972566604614258 or 10.972566604614258
('battle', 'collect', 'debts', 'unpaid', 'since', 'country')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (35, 'country') has the max prob in order to be the next word. The prob is: 0.0039013565983623266
1026500 The nn predicts (tensor([58]), 'year') the next word with max prob: tensor([0.0272])
The negative loglikelihood (-log p(country)): 5.546431064605713 or 5.546431064605713 or 5.546431064605713
('underground', 'wall', 'ice', 'would', 'stop', 'radioactive')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1469, 'radioactive') has the max prob in order to be the next word. The prob is: 0.0017758444882929325
1026600 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0125])
The negative loglikelihood (-log p(radioactive)): 6.333479404449463 or 6.333479404449463 or 6.333479404449463
('syrian', 'army', 'soldiers', 'crossed', 'border', 'august')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2190, 'august') has the max prob in order to be the next word. The prob is: 0.00024981441674754024
1026700 The nn predicts (tensor([809]), 'damascus') the next word with max prob: tensor([0.3070])
The negative loglikelihood (-log p(august)): 8.294792175292969 or 8.294792175292969 or 8.294792175292969
('great', 'firewall', 'allow', 'guests', 'access', 'twitter')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (581, 'twitter') has the max prob in order to be the next word. The prob is: 0.0006454215035773814
1026800 The nn predicts (tensor([6]), 'russia') the next word with max prob: tensor([0.0146])
The negative loglikelihood (-log p(twitter)): 7.345606803894043 or 7.345606803894043 or 7.345606803894043
('congo', 'government', 'spokesman', 'says', 'least', 'three')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (97, 'three') has the max prob in order to be the next word. The prob is: 0.005149328149855137
1026900 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.1772])
The negative loglikelihood (-log p(three)): 5.2688889503479 or 5.2688889503479 or 5.2688889503479
('might', 'prevent', 'use', 'weapons', 'anywhere', 'world')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (7, 'world') has the max prob in order to be the next word. The prob is: 0.0220512505620718
1027000 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1589])
The negative loglikelihood (-log p(world)): 3.8143858909606934 or 3.8143858909606934 or 3.8143858909606934
('designed', 'stop', 'sex', 'offenders', 'volunteering', 'western')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (275, 'western') has the max prob in order to be the next word. The prob is: 0.0004017516621388495
1027100 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0368])
The negative loglikelihood (-log p(western)): 7.819676399230957 or 7.819676399230957 or 7.819676399230957
('obama', 'considers', 'military', 'options', 'responding', 'alleged')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (390, 'alleged') has the max prob in order to be the next word. The prob is: 0.0022069308906793594
1027200 The nn predicts (tensor([313]), 'chemical') the next word with max prob: tensor([0.2211])
The negative loglikelihood (-log p(alleged)): 6.116152286529541 or 6.116152286529541 or 6.116152286529541
('weapons', 'attack', 'said', 'killed', 'hundreds', 'people')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (14, 'people') has the max prob in order to be the next word. The prob is: 0.18643896281719208
1027300 The nn predicts (tensor([14]), 'people') the next word with max prob: tensor([0.1864])
The negative loglikelihood (-log p(people)): 1.6796513795852661 or 1.6796513795852661 or 1.6796513795852661
('papers', 'write', 'open', 'letter', 'david', 'cameron')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (451, 'cameron') has the max prob in order to be the next word. The prob is: 0.25180783867836
1027400 The nn predicts (tensor([451]), 'cameron') the next word with max prob: tensor([0.2518])
The negative loglikelihood (-log p(cameron)): 1.3790889978408813 or 1.3790889978408813 or 1.3790889978408813
('coronal', 'mass', 'ejection', 'results', 'explosive', 'video')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (100, 'video') has the max prob in order to be the next word. The prob is: 0.00454235915094614
1027500 The nn predicts (tensor([106]), 'power') the next word with max prob: tensor([0.0090])
The negative loglikelihood (-log p(video)): 5.394308567047119 or 5.394308567047119 or 5.394308567047119
('jihadist', 'group', 'boko', 'haram', 'attacks', 'nigerian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (748, 'nigerian') has the max prob in order to be the next word. The prob is: 0.0003214007301721722
1027600 The nn predicts (tensor([13]), 'killed') the next word with max prob: tensor([0.0528])
The negative loglikelihood (-log p(nigerian)): 8.042821884155273 or 8.042821884155273 or 8.042821884155273
('saturday', 'shell', 'struck', 'city', 'goma', 'killed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (13, 'killed') has the max prob in order to be the next word. The prob is: 0.00836627371609211
1027700 The nn predicts (tensor([112]), 'near') the next word with max prob: tensor([0.0474])
The negative loglikelihood (-log p(killed)): 4.7835469245910645 or 4.7835469245910645 or 4.7835469245910645
('northern', 'india', 'defying', 'ban', 'pilgrimages', 'disputed')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (989, 'disputed') has the max prob in order to be the next word. The prob is: 0.0006692853639833629
1027800 The nn predicts (tensor([130]), 'ban') the next word with max prob: tensor([0.0114])
The negative loglikelihood (-log p(disputed)): 7.309299945831299 or 7.309299945831299 or 7.309299945831299
('sudan', 'kiir', 'puts', 'generals', 'investigation', 'abuses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1848, 'abuses') has the max prob in order to be the next word. The prob is: 0.0011244448833167553
1027900 The nn predicts (tensor([313]), 'chemical') the next word with max prob: tensor([0.0983])
The negative loglikelihood (-log p(abuses)): 6.790465831756592 or 6.790465831756592 or 6.790465831756592
('week', 'around', 'damascus', 'rebuffed', 'assad', 'regime')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (422, 'regime') has the max prob in order to be the next word. The prob is: 0.06233082339167595
1028000 The nn predicts (tensor([422]), 'regime') the next word with max prob: tensor([0.0623])
The negative loglikelihood (-log p(regime)): 2.775299310684204 or 2.775299310684204 or 2.775299310684204
('states', 'military', 'intervention', 'iraq', 'russian', 'foreign')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (88, 'foreign') has the max prob in order to be the next word. The prob is: 0.02478419989347458
1028100 The nn predicts (tensor([12]), 'president') the next word with max prob: tensor([0.0807])
The negative loglikelihood (-log p(foreign)): 3.6975488662719727 or 3.6975488662719727 or 3.6975488662719727
('even', 'liberal', 'activists', 'labor', 'organizers', 'dangerous')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (968, 'dangerous') has the max prob in order to be the next word. The prob is: 0.00033172182156704366
1028200 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0581])
The negative loglikelihood (-log p(dangerous)): 8.011214256286621 or 8.011214256286621 or 8.011214256286621
('spain', 'youth', 'struggle', 'chart', 'life', 'amid')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (155, 'amid') has the max prob in order to be the next word. The prob is: 0.0005695907748304307
1028300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0099])
The negative loglikelihood (-log p(amid)): 7.470592498779297 or 7.470592498779297 or 7.470592498779297
('leaks', 'alleged', 'emails', 'american', 'officials', 'intelligence')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (343, 'intelligence') has the max prob in order to be the next word. The prob is: 0.0065148137509822845
1028400 The nn predicts (tensor([53]), 'say') the next word with max prob: tensor([0.0513])
The negative loglikelihood (-log p(intelligence)): 5.033676624298096 or 5.033676624298096 or 5.033676624298096
('secretary', 'says', 'crucial', 'evidence', 'may', 'already')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (865, 'already') has the max prob in order to be the next word. The prob is: 0.0006809440674260259
1028500 The nn predicts (tensor([218]), 'used') the next word with max prob: tensor([0.0523])
The negative loglikelihood (-log p(already)): 7.292030334472656 or 7.292030334472656 or 7.292030334472656
('britain', 'planning', 'join', 'forces', 'america', 'launch')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (367, 'launch') has the max prob in order to be the next word. The prob is: 4.2508308979449794e-05
1028600 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0446])
The negative loglikelihood (-log p(launch)): 10.065811157226562 or 10.065811157226562 or 10.065811157226562
('least', 'five', 'people', 'injuring', 'authorities', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.19916890561580658
1028700 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.1992])
The negative loglikelihood (-log p(said)): 1.6136020421981812 or 1.6136020421981812 or 1.6136020421981812
('pm', 'erdogan', 'calls', 'us', 'two', 'members')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (339, 'members') has the max prob in order to be the next word. The prob is: 0.0024761667009443045
1028800 The nn predicts (tensor([210]), 'days') the next word with max prob: tensor([0.0626])
The negative loglikelihood (-log p(members)): 6.001043796539307 or 6.001043796539307 or 6.001043796539307
('cluster', 'bombs', 'saudi', 'arabia', 'refused', 'sign')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (550, 'sign') has the max prob in order to be the next word. The prob is: 0.0007988444995135069
1028900 The nn predicts (tensor([18]), 'military') the next word with max prob: tensor([0.0196])
The negative loglikelihood (-log p(sign)): 7.1323442459106445 or 7.1323442459106445 or 7.1323442459106445
('blair', 'need', 'laws', 'stop', 'principled', 'leaking')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3141, 'leaking') has the max prob in order to be the next word. The prob is: 0.00014102543354965746
1029000 The nn predicts (tensor([162]), 'change') the next word with max prob: tensor([0.0094])
The negative loglikelihood (-log p(leaking)): 8.866570472717285 or 8.866570472717285 or 8.866570472717285
('coalition', 'syria', 'says', 'foreign', 'minister', 'even')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (334, 'even') has the max prob in order to be the next word. The prob is: 0.00037715889629907906
1029100 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.1714])
The negative loglikelihood (-log p(even)): 7.882843971252441 or 7.882843971252441 or 7.882843971252441
('work', 'dark', 'secrets', 'england', 'oldest', 'work')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (298, 'work') has the max prob in order to be the next word. The prob is: 0.0009752699406817555
1029200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0198])
The negative loglikelihood (-log p(work)): 6.932796478271484 or 6.932796478271484 or 6.932796478271484
('made', 'secret', 'deal', 'central', 'african', 'republic')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (525, 'republic') has the max prob in order to be the next word. The prob is: 0.1454794853925705
1029300 The nn predicts (tensor([525]), 'republic') the next word with max prob: tensor([0.1455])
The negative loglikelihood (-log p(republic)): 1.9277201890945435 or 1.9277201890945435 or 1.9277201890945435
('moment', 'french', 'police', 'drag', 'protestors', 'bullfighting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (10496, 'bullfighting') has the max prob in order to be the next word. The prob is: 7.796519639668986e-06
1029400 The nn predicts (tensor([1009]), 'square') the next word with max prob: tensor([0.0373])
The negative loglikelihood (-log p(bullfighting)): 11.761833190917969 or 11.761833190917969 or 11.761833190917969
('work', 'sex', 'trade', 'bring', 'measure', 'peace')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (127, 'peace') has the max prob in order to be the next word. The prob is: 0.00045020284596830606
1029500 The nn predicts (tensor([104]), 'would') the next word with max prob: tensor([0.0318])
The negative loglikelihood (-log p(peace)): 7.705812454223633 or 7.705812454223633 or 7.705812454223633
('zurich', 'launches', 'sex', 'prostitution', 'morals', 'religion')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1934, 'religion') has the max prob in order to be the next word. The prob is: 0.00021463833400048316
1029600 The nn predicts (tensor([220]), 'sex') the next word with max prob: tensor([0.0243])
The negative loglikelihood (-log p(religion)): 8.446556091308594 or 8.446556091308594 or 8.446556091308594
('girl', 'offense', 'new', 'law', 'indian', 'court')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (39, 'court') has the max prob in order to be the next word. The prob is: 0.012912223115563393
1029700 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0568])
The negative loglikelihood (-log p(court)): 4.349580764770508 or 4.349580764770508 or 4.349580764770508
('drop', 'experiment', 'dies', 'aged', 'still', 'get')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (201, 'get') has the max prob in order to be the next word. The prob is: 0.0006246636039577425
1029800 The nn predicts (tensor([203]), 'life') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(get)): 7.378297328948975 or 7.378297328948975 or 7.378297328948975
('materials', 'snowden', 'took', 'national', 'security', 'agency')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (304, 'agency') has the max prob in order to be the next word. The prob is: 0.052383411675691605
1029900 The nn predicts (tensor([54]), 'forces') the next word with max prob: tensor([0.1957])
The negative loglikelihood (-log p(agency)): 2.9491653442382812 or 2.9491653442382812 or 2.9491653442382812
('defence', 'team', 'likely', 'blame', 'muslim', 'brotherhood')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1069, 'brotherhood') has the max prob in order to be the next word. The prob is: 0.2747027277946472
1030000 The nn predicts (tensor([1069]), 'brotherhood') the next word with max prob: tensor([0.2747])
The negative loglikelihood (-log p(brotherhood)): 1.2920657396316528 or 1.2920657396316528 or 1.2920657396316528
('return', 'gays', 'normal', 'life', 'via', 'therapy')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6179, 'therapy') has the max prob in order to be the next word. The prob is: 1.6613255866104737e-05
1030100 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0316])
The negative loglikelihood (-log p(therapy)): 11.00531005859375 or 11.00531005859375 or 11.00531005859375
('widens', 'use', 'term', 'islamist', 'target', 'critics')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1681, 'critics') has the max prob in order to be the next word. The prob is: 0.00021699389617424458
1030200 The nn predicts (tensor([313]), 'chemical') the next word with max prob: tensor([0.0365])
The negative loglikelihood (-log p(critics)): 8.435641288757324 or 8.435641288757324 or 8.435641288757324
('whistleblower', 'wins', 'case', 'oil', 'firm', 'loses')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1330, 'loses') has the max prob in order to be the next word. The prob is: 6.230750295799226e-05
1030300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0273])
The negative loglikelihood (-log p(loses)): 9.683428764343262 or 9.683428764343262 or 9.683428764343262
('outcast', 'iranian', 'minister', 'carries', 'hope', 'easing')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5215, 'easing') has the max prob in order to be the next word. The prob is: 2.662662518559955e-05
1030400 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0174])
The negative loglikelihood (-log p(easing)): 10.533598899841309 or 10.533598899841309 or 10.533598899841309
('airport', 'israeli', 'tv', 'reports', 'us', 'said')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (26, 'said') has the max prob in order to be the next word. The prob is: 0.005648099817335606
1030500 The nn predicts (tensor([139]), 'strike') the next word with max prob: tensor([0.0225])
The negative loglikelihood (-log p(said)): 5.176435947418213 or 5.176435947418213 or 5.176435947418213
('thinks', 'organic', 'production', 'worth', 'serious', 'look')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1001, 'look') has the max prob in order to be the next word. The prob is: 0.00011283648200333118
1030600 The nn predicts (tensor([461]), 'evidence') the next word with max prob: tensor([0.0166])
The negative loglikelihood (-log p(look)): 9.089570999145508 or 9.089570999145508 or 9.089570999145508
('country', 'six', 'seven', 'eight', 'kids', 'long')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (604, 'long') has the max prob in order to be the next word. The prob is: 0.0005062257405370474
1030700 The nn predicts (tensor([48]), 'dead') the next word with max prob: tensor([0.0331])
The negative loglikelihood (-log p(long)): 7.588527679443359 or 7.588527679443359 or 7.588527679443359
('genetic', 'researchers', 'trying', 'find', 'indian', 'melting')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3665, 'melting') has the max prob in order to be the next word. The prob is: 7.280256249941885e-05
1030800 The nn predicts (tensor([129]), 'woman') the next word with max prob: tensor([0.0147])
The negative loglikelihood (-log p(melting)): 9.527759552001953 or 9.527759552001953 or 9.527759552001953
('investigate', 'widespread', 'abduction', 'killings', 'kosovo', 'serbs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6934, 'serbs') has the max prob in order to be the next word. The prob is: 1.1172907761647366e-05
1030900 The nn predicts (tensor([39]), 'court') the next word with max prob: tensor([0.0249])
The negative loglikelihood (-log p(serbs)): 11.402018547058105 or 11.402018547058105 or 11.402018547058105
('allies', 'us', 'weighs', 'action', 'moral', 'obscenity')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (15878, 'obscenity') has the max prob in order to be the next word. The prob is: 1.5405288422698504e-06
1031000 The nn predicts (tensor([372]), 'action') the next word with max prob: tensor([0.0564])
The negative loglikelihood (-log p(obscenity)): 13.383384704589844 or 13.383384704589844 or 13.383384704589844
('delay', 'reaching', 'site', 'alleged', 'chem', 'weapons')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (149, 'weapons') has the max prob in order to be the next word. The prob is: 0.2339956909418106
1031100 The nn predicts (tensor([149]), 'weapons') the next word with max prob: tensor([0.2340])
The negative loglikelihood (-log p(weapons)): 1.452452540397644 or 1.452452540397644 or 1.452452540397644
('according', 'sources', 'attended', 'meeting', 'envoys', 'syrian')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (30, 'syrian') has the max prob in order to be the next word. The prob is: 0.017390305176377296
1031200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1490])
The negative loglikelihood (-log p(syrian)): 4.051842212677002 or 4.051842212677002 or 4.051842212677002
('plans', 'disneyland', 'africa', 'tourism', 'minister', 'outlines')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6255, 'outlines') has the max prob in order to be the next word. The prob is: 6.825417585787363e-06
1031300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0590])
The negative loglikelihood (-log p(outlines)): 11.894857406616211 or 11.894857406616211 or 11.894857406616211
('government', 'jet', 'used', 'german', 'chancellor', 'angela')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1576, 'angela') has the max prob in order to be the next word. The prob is: 0.005739389453083277
1031400 The nn predicts (tensor([26]), 'said') the next word with max prob: tensor([0.0307])
The negative loglikelihood (-log p(angela)): 5.160402297973633 or 5.160402297973633 or 5.160402297973633
('reuters', 'syria', 'strike', 'due', 'days', 'west')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (123, 'west') has the max prob in order to be the next word. The prob is: 0.0006131341215223074
1031500 The nn predicts (tensor([139]), 'strike') the next word with max prob: tensor([0.0361])
The negative loglikelihood (-log p(west)): 7.3969268798828125 or 7.3969268798828125 or 7.3969268798828125
('russia', 'evacuates', 'citizens', 'ahead', 'military', 'strikes')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (193, 'strikes') has the max prob in order to be the next word. The prob is: 0.02100236527621746
1031600 The nn predicts (tensor([139]), 'strike') the next word with max prob: tensor([0.3275])
The negative loglikelihood (-log p(strikes)): 3.8631203174591064 or 3.8631203174591064 or 3.8631203174591064
('israel', 'readies', 'syria', 'backlash', 'everything', 'table')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3846, 'table') has the max prob in order to be the next word. The prob is: 0.00019819352019112557
1031700 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0418])
The negative loglikelihood (-log p(table)): 8.526267051696777 or 8.526267051696777 or 8.526267051696777
('last', 'night', 'easily', 'one', 'racist', 'displays')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (8174, 'displays') has the max prob in order to be the next word. The prob is: 6.278592081798706e-06
1031800 The nn predicts (tensor([8]), 'police') the next word with max prob: tensor([0.0105])
The negative loglikelihood (-log p(displays)): 11.978364944458008 or 11.978364944458008 or 11.978364944458008
('advisor', 'zbigniew', 'brzezinski', 'denounces', 'appears', 'imminent')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2750, 'imminent') has the max prob in order to be the next word. The prob is: 0.0002138816489605233
1031900 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0967])
The negative loglikelihood (-log p(imminent)): 8.450087547302246 or 8.450087547302246 or 8.450087547302246
('hacks', 'twitter', 'new', 'york', 'times', 'domains')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (12705, 'domains') has the max prob in order to be the next word. The prob is: 6.2986937336972915e-06
1032000 The nn predicts (tensor([383]), 'site') the next word with max prob: tensor([0.0176])
The negative loglikelihood (-log p(domains)): 11.975168228149414 or 11.975168228149414 or 11.975168228149414
('compatriots', 'flee', 'country', 'fearing', 'western', 'air')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (80, 'air') has the max prob in order to be the next word. The prob is: 0.0002803729148581624
1032100 The nn predicts (tensor([71]), 'city') the next word with max prob: tensor([0.0309])
The negative loglikelihood (-log p(air)): 8.179389953613281 or 8.179389953613281 or 8.179389953613281
('area', 'chemical', 'weapons', 'would', 'make', 'sense')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3112, 'sense') has the max prob in order to be the next word. The prob is: 0.00024777062935754657
1032200 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0514])
The negative loglikelihood (-log p(sense)): 8.303007125854492 or 8.303007125854492 or 8.303007125854492
('geared', 'tuesday', 'probable', 'military', 'strike', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.16702057421207428
1032300 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.1670])
The negative loglikelihood (-log p(syria)): 1.7896382808685303 or 1.7896382808685303 or 1.7896382808685303
('may', 'fake', 'probe', 'policing', 'troubled', 'new')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4, 'new') has the max prob in order to be the next word. The prob is: 0.0013669236795976758
1032400 The nn predicts (tensor([35]), 'country') the next word with max prob: tensor([0.0602])
The negative loglikelihood (-log p(new)): 6.5951924324035645 or 6.5951924324035645 or 6.5951924324035645
('erdoğan', 'accidentally', 'fostered', 'generation', 'turkish', 'liberals')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (6809, 'liberals') has the max prob in order to be the next word. The prob is: 1.3854788448952604e-05
1032500 The nn predicts (tensor([11]), 'government') the next word with max prob: tensor([0.1044])
The negative loglikelihood (-log p(liberals)): 11.186880111694336 or 11.186880111694336 or 11.186880111694336
('climate', 'change', 'worse', 'emit', 'carbon', 'dioxide')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (4822, 'dioxide') has the max prob in order to be the next word. The prob is: 0.00040645600529387593
1032600 The nn predicts (tensor([915]), 'levels') the next word with max prob: tensor([0.0202])
The negative loglikelihood (-log p(dioxide)): 7.808034896850586 or 7.808034896850586 or 7.808034896850586
('end', 'life', 'india', 'response', 'rape', 'apparently')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1849, 'apparently') has the max prob in order to be the next word. The prob is: 0.0001450117997592315
1032700 The nn predicts (tensor([139]), 'strike') the next word with max prob: tensor([0.0535])
The negative loglikelihood (-log p(apparently)): 8.838695526123047 or 8.838695526123047 or 8.838695526123047
('arab', 'league', 'rejects', 'attack', 'syriar', 'leaving')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1065, 'leaving') has the max prob in order to be the next word. The prob is: 0.000215851265238598
1032800 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.2413])
The negative loglikelihood (-log p(leaving)): 8.44092082977295 or 8.44092082977295 or 8.44092082977295
('security', 'council', 'get', 'draft', 'resolution', 'syria')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (5, 'syria') has the max prob in order to be the next word. The prob is: 0.0721905305981636
1032900 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0722])
The negative loglikelihood (-log p(syria)): 2.628446340560913 or 2.628446340560913 or 2.628446340560913
('syria', 'western', 'powers', 'move', 'closer', 'strike')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (139, 'strike') has the max prob in order to be the next word. The prob is: 0.0009342627017758787
1033000 The nn predicts (tensor([5]), 'syria') the next word with max prob: tensor([0.0780])
The negative loglikelihood (-log p(strike)): 6.975752830505371 or 6.975752830505371 or 6.975752830505371
('cocaine', 'king', 'clear', 'indication', 'govt', 'making')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (625, 'making') has the max prob in order to be the next word. The prob is: 0.0005819327780045569
1033100 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0290])
The negative loglikelihood (-log p(making)): 7.449155807495117 or 7.449155807495117 or 7.449155807495117
('europe', 'rooted', 'european', 'traditions', 'fueled', 'today')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (356, 'today') has the max prob in order to be the next word. The prob is: 0.00013853152631781995
1033200 The nn predicts (tensor([7]), 'world') the next word with max prob: tensor([0.0196])
The negative loglikelihood (-log p(today)): 8.88441276550293 or 8.88441276550293 or 8.88441276550293
('kill', 'giant', 'cats', 'get', 'giant', 'dogs')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (2019, 'dogs') has the max prob in order to be the next word. The prob is: 0.0006101728067733347
1033300 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.0188])
The negative loglikelihood (-log p(dogs)): 7.401768207550049 or 7.401768207550049 or 7.401768207550049
('subsidence', 'could', 'lead', 'increase', 'global', 'risk')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (456, 'risk') has the max prob in order to be the next word. The prob is: 0.0035164866130799055
1033400 The nn predicts (tensor([1002]), 'warming') the next word with max prob: tensor([0.0867])
The negative loglikelihood (-log p(risk)): 5.650292873382568 or 5.650292873382568 or 5.650292873382568
('strike', 'residents', 'opposition', 'sources', 'said', 'wednesday')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (362, 'wednesday') has the max prob in order to be the next word. The prob is: 0.06137148290872574
1033500 The nn predicts (tensor([324]), 'tuesday') the next word with max prob: tensor([0.2641])
The negative loglikelihood (-log p(wednesday)): 2.7908101081848145 or 2.7908101081848145 or 2.7908101081848145
('previously', 'unreported', 'instances', 'chemical', 'weapons', 'use')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (174, 'use') has the max prob in order to be the next word. The prob is: 0.03012681193649769
1033600 The nn predicts (tensor([15]), 'attack') the next word with max prob: tensor([0.1926])
The negative loglikelihood (-log p(use)): 3.5023398399353027 or 3.5023398399353027 or 3.5023398399353027
('security', 'council', 'discussion', 'ends', 'plans', 'vote')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (175, 'vote') has the max prob in order to be the next word. The prob is: 0.0019234512001276016
1033700 The nn predicts (tensor([18]), 'military') the next word with max prob: tensor([0.0455])
The negative loglikelihood (-log p(vote)): 6.253634452819824 or 6.253633975982666 or 6.253633975982666
('severity', 'leaking', 'tons', 'radioactive', 'water', 'ocean')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (1078, 'ocean') has the max prob in order to be the next word. The prob is: 0.006262087728828192
1033800 The nn predicts (tensor([208]), 'water') the next word with max prob: tensor([0.2877])
The negative loglikelihood (-log p(ocean)): 5.073241710662842 or 5.073241710662842 or 5.073241710662842
('led', 'assad', 'believe', 'could', 'get', 'away')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (482, 'away') has the max prob in order to be the next word. The prob is: 0.002113851485773921
1033900 The nn predicts (tensor([115]), 'back') the next word with max prob: tensor([0.0234])
The negative loglikelihood (-log p(away)): 6.159243583679199 or 6.159243583679199 or 6.159243583679199
('decision', 'syria', 'taking', 'action', 'voting', 'rights')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (67, 'rights') has the max prob in order to be the next word. The prob is: 0.015517113730311394
1034000 The nn predicts (tensor([4]), 'new') the next word with max prob: tensor([0.0194])
The negative loglikelihood (-log p(rights)): 4.165811538696289 or 4.165811538696289 or 4.165811538696289
('propaganda', 'encouraged', 'people', 'report', 'lgbt', 'neighbors')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (3458, 'neighbors') has the max prob in order to be the next word. The prob is: 5.255572250462137e-05
1034100 The nn predicts (tensor([67]), 'rights') the next word with max prob: tensor([0.0917])
The negative loglikelihood (-log p(neighbors)): 9.853636741638184 or 9.853636741638184 or 9.853636741638184
('paraguay', 'bus', 'drivers', 'stage', 'crucifixion', 'protest')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (91, 'protest') has the max prob in order to be the next word. The prob is: 0.055709339678287506
1034200 The nn predicts (tensor([91]), 'protest') the next word with max prob: tensor([0.0557])
The negative loglikelihood (-log p(protest)): 2.8876075744628906 or 2.8876075744628906 or 2.8876075744628906
('negotiation', 'china', 'russia', 'walk', 'un', 'security')
The shape of input context vector: (1, 500)
The shape of output prediction is: torch.Size([1, 81272])
We expect that (57, 'security') has the max prob in order to be the next word. The prob is: 0.003858668962493539
1034300 The nn predicts (tensor([2]), 'says') the next word with max prob: tensor([0.0608])
The negative loglikelihood (-log p(security)): 5.557433128356934 or 5.557433128356934 or 5.557433128356934
Traceback (most recent call last):
  File "/Users/hosseinfani/Documents/GitHub/2003BengioLM/dense_multiple_forward_pass.py", line 61, in <module>
    #print(bengiolm_w2v.W_I.weight.grad)
  File "/Users/hosseinfani/anaconda3/lib/python3.7/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/Users/hosseinfani/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
KeyboardInterrupt

Process finished with exit code 1
